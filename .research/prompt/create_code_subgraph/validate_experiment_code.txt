
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training: `uv run python -u -m src.main run={run_id} results_dir={path}`
     * Evaluation: `uv run python -m src.evaluate results_dir={path}` (independent execution)
   - Supports trial_mode=true flag for lightweight validation runs (automatically disables WandB)

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics using `wandb.log()`
   - trial_mode automatically disables WandB (sets wandb.mode=disabled)
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/run/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path}`
   - main.py DOES NOT call evaluate.py
   - evaluate.py retrieves ALL data from WandB API using `wandb.Api()` (not from local files)
   - evaluate.py exports retrieved WandB data to `{results_dir}/wandb_data/` for reproducibility
   - evaluate.py generates ALL publication-quality PDF figures and saves to `{results_dir}/images/`
   - Proper figure quality: legends, annotations, tight_layout
   - Follows naming convention: `<figure_topic>[_<condition>][_pairN].pdf`
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Trial Mode Implementation**:
   - trial_mode=true flag properly reduces computational load
   - Training: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
   - WandB automatically disabled in trial_mode (wandb.mode=disabled)
   - Purpose: Fast validation that code runs without errors

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare DistilBERT with task-specific adapters (proposed) against standard fine-tuning (comparative) across vision and language tasks to evaluate performance improvements.

# Experimental Design
- Strategy: Comparative analysis of DistilBERT performance across vision and language tasks
- Proposed Method: Fine-tuned DistilBERT with task-specific adapters
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time']

# Experiment Runs

- Run ID: proposed-DistilBERT-base-66M-CIFAR-10
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-CIFAR-10
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 1e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: proposed-DistilBERT-base-66M-alpaca-cleaned
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-alpaca-cleaned
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 5e-5
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-CIFAR-10
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-CIFAR-10
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-alpaca-cleaned
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - run: proposed-DistilBERT-base-66M-CIFAR-10  # default example, override via CLI\n  - _self_\n\nwandb:\n  entity: gengaru617-personal\n  project: 251017-test\n  mode: online  # will be overwritten to \"disabled\" automatically when trial_mode=true\n\ntrial_mode: false  # set via CLI for lightweight validation\n\nresults_dir: outputs/\n\nhydra:\n  run:\n    dir: .\n  output_subdir: null\n", "evaluate_py": "\"\"\"Independent evaluation: fetch WandB runs, export metrics and generate figures.\"\"\"\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Dict\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\n\nFIGURE_DIR = \"images\"\nDATA_DIR = \"wandb_data\"\n\n\n# ---------------------------------------------------------------------------\n# CLI helper (supports key=value style)\n# ---------------------------------------------------------------------------\n\ndef _parse_results_dir(argv) -\u003e Path:\n    \"\"\"Extract results_dir from argv supporting `results_dir=/foo/bar` style.\"\"\"\n    for arg in argv[1:]:\n        if arg.startswith(\"results_dir=\"):\n            return Path(arg.split(\"=\", 1)[1]).expanduser().absolute()\n    # fallback to positional first argument\n    if len(argv) \u003e 1:\n        return Path(argv[1]).expanduser().absolute()\n    raise ValueError(\"results_dir argument is required (e.g., results_dir=outputs/)\")\n\n\n# ---------------------------------------------------------------------------\n# WandB helpers\n# ---------------------------------------------------------------------------\n\ndef _fetch_runs(entity: str, project: str):\n    api = wandb.Api()\n    return api.runs(f\"{entity}/{project}\")\n\n\ndef _export_metrics(runs, out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    summaries: Dict[str, Dict] = {}\n    needed_cols = [\n        \"epoch\",\n        \"best_val_acc\",\n        \"best_val_f1\",\n        \"train_loss\",\n        \"val_loss\",\n        \"val_acc\",\n        \"val_f1\",\n    ]\n    for run in runs:\n        df = run.history(keys=needed_cols, pandas=True)\n        df.to_csv(out_dir / f\"run_{run.name}_metrics.csv\", index=False)\n        summaries[run.name] = {\n            \"best_val_acc\": df[\"best_val_acc\"].dropna().max() if \"best_val_acc\" in df else None,\n            \"best_val_f1\": df[\"best_val_f1\"].dropna().max() if \"best_val_f1\" in df else None,\n        }\n    with open(out_dir / \"summary.json\", \"w\") as f:\n        json.dump(summaries, f, indent=2)\n    return summaries\n\n\n# ---------------------------------------------------------------------------\n# Plotting\n# ---------------------------------------------------------------------------\n\ndef _plot_learning_curves(runs, dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    for run in runs:\n        df = run.history(keys=[\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"], pandas=True)\n        if df.empty:\n            continue\n        # Loss curve\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_loss\"], label=\"Train Loss\")\n        ax.plot(df[\"epoch\"], df[\"val_loss\"], label=\"Val Loss\")\n        ax.set_title(f\"Loss Curve - {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"loss_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n        # Accuracy curve\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_acc\"], label=\"Train Acc\")\n        ax.plot(df[\"epoch\"], df[\"val_acc\"], label=\"Val Acc\")\n        ax.set_title(f\"Accuracy Curve - {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"acc_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n\ndef _plot_comparison(summary: Dict[str, Dict], dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    df = pd.DataFrame.from_dict(summary, orient=\"index\").reset_index().rename(columns={\"index\": \"run_id\"})\n    if df[\"best_val_acc\"].isnull().all():\n        return\n    fig, ax = plt.subplots(figsize=(max(6, len(df) * 0.75), 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"best_val_acc\", ax=ax, palette=\"viridis\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_title(\"Best Validation Accuracy Across Runs\")\n    for p in ax.patches:\n        ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                    ha=\"center\", va=\"center\", xytext=(0, 5), textcoords=\"offset points\")\n    fig.tight_layout()\n    fname = dest / \"comparison_best_val_acc.pdf\"\n    fig.savefig(fname)\n    print(fname.name)\n    plt.close(fig)\n\n\n# ---------------------------------------------------------------------------\n# Improvement computation\n# ---------------------------------------------------------------------------\n\ndef _compute_improvement(summary: Dict[str, Dict]):\n    records: Dict[str, Dict[str, float]] = {}\n    for run_id, metrics in summary.items():\n        dataset_name = run_id.split(\"-\")[-1]\n        method = \"proposed\" if run_id.startswith(\"proposed\") else \"baseline\"\n        records.setdefault(dataset_name, {})[method] = metrics[\"best_val_acc\"]\n\n    improvement = {ds: {**vals, \"improvement_rate\": (vals.get(\"proposed\") - vals.get(\"baseline\")) / vals.get(\"baseline\")\n                        if vals.get(\"baseline\") else None}\n                   for ds, vals in records.items()}\n    return improvement\n\n\n# ---------------------------------------------------------------------------\n# Entrypoint\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    results_path = _parse_results_dir(sys.argv)\n\n    cfg_path = results_path / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"Config file not found at {cfg_path}\")\n\n    import yaml\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    entity = cfg[\"wandb\"][\"entity\"]\n    project = cfg[\"wandb\"][\"project\"]\n\n    runs = _fetch_runs(entity, project)\n    summary = _export_metrics(runs, results_path / DATA_DIR)\n\n    _plot_learning_curves(runs, results_path / FIGURE_DIR)\n    _plot_comparison(summary, results_path / FIGURE_DIR)\n\n    improvement = _compute_improvement(summary)\n    with open(results_path / DATA_DIR / \"improvement.json\", \"w\") as f:\n        json.dump(improvement, f, indent=2)\n", "main_py": "\"\"\"Orchestrator: receives run id \u0026 launches src.train as subprocess.\"\"\"\n\nimport subprocess\nimport sys\nfrom typing import List\nfrom pathlib import Path\n\nimport hydra\nfrom hydra.utils import get_original_cli_args\n\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):\n    # Build base command\n    original_cli: List[str] = get_original_cli_args()\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + original_cli\n\n    # Ensure wandb.mode is disabled when trial_mode=true -------------------\n    if cfg.get(\"trial_mode\", False):\n        cmd.append(\"wandb.mode=disabled\")\n\n    print(\"Executing subprocess:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "import torch\nfrom torch import nn\nfrom transformers import DistilBertModel\n\n\nclass HoulsbyAdapter(nn.Module):\n    def __init__(self, hidden_dim: int, reduction_factor: int = 16, non_linearity: str = \"relu\"):\n        super().__init__()\n        bottleneck = max(1, hidden_dim // reduction_factor)\n        self.down = nn.Linear(hidden_dim, bottleneck)\n        self.nonlin = getattr(nn, non_linearity.capitalize())() if hasattr(nn, non_linearity.capitalize()) else nn.ReLU()\n        self.up = nn.Linear(bottleneck, hidden_dim)\n\n    def forward(self, x):\n        return self.up(self.nonlin(self.down(x)))\n\n\nclass VisionPatchEmbedding(nn.Module):\n    def __init__(self, in_channels: int, patch_size: int, embed_dim: int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Linear(in_channels * patch_size * patch_size, embed_dim)\n\n    def forward(self, imgs):  # imgs: B,C,H,W\n        B, C, H, W = imgs.shape\n        p = self.patch_size\n        assert H % p == 0 and W % p == 0, \"Image dims must be divisible by patch size\"\n        patches = imgs.unfold(2, p, p).unfold(3, p, p)  # B,C,nH,nW,p,p\n        patches = patches.contiguous().view(B, C, -1, p, p).permute(0, 2, 1, 3, 4).contiguous()\n        patches = patches.view(B, patches.size(1), -1)  # B,N,C*p*p\n        return self.proj(patches)  # B,N,D\n\n\nclass DistilBertClassifier(nn.Module):\n    def __init__(self, model_name: str, num_labels: int, adapter_cfg=None, vision_patch_cfg=None):\n        super().__init__()\n        self.is_vision = vision_patch_cfg is not None\n        self.bert = DistilBertModel.from_pretrained(model_name, cache_dir=\".cache/\")\n        hidden = self.bert.config.hidden_size\n\n        if self.is_vision:\n            self.vision = VisionPatchEmbedding(3, vision_patch_cfg.patch_size, hidden)\n        else:\n            self.vision = None\n\n        if adapter_cfg and adapter_cfg.get(\"enabled\", False):\n            self.adapter = HoulsbyAdapter(hidden, adapter_cfg.reduction_factor, adapter_cfg.non_linearity)\n        else:\n            self.adapter = None\n\n        self.pre_cls = nn.Linear(hidden, hidden)\n        self.act = nn.GELU()\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(hidden, num_labels)\n\n    def forward(self, batch):\n        if self.is_vision and \"pixel_values\" in batch:\n            embeds = self.vision(batch[\"pixel_values\"])\n            mask = torch.ones(embeds.size()[:-1], dtype=torch.long, device=embeds.device)\n            out = self.bert(inputs_embeds=embeds, attention_mask=mask)\n        else:\n            out = self.bert(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n        pooled = out.last_hidden_state[:, 0]\n        if self.adapter is not None:\n            pooled = pooled + self.adapter(pooled)\n        x = self.pre_cls(pooled)\n        x = self.act(x)\n        x = self.dropout(x)\n        return self.cls(x)\n\n\ndef build_model(cfg, tokenizer=None):\n    vision_patch_cfg = cfg.model.vision_patch if cfg.model.get(\"input_representation\", \"text\") == \"image_sequence\" else None\n    adapter_cfg = cfg.model.task_adapters if cfg.method == \"proposed\" else None\n    return DistilBertClassifier(cfg.model.name, cfg.model.num_labels, adapter_cfg, vision_patch_cfg)\n", "preprocess_py": "\"\"\"Dataset preprocessing utilities.\"\"\"\nfrom typing import Tuple, List\nimport random\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import datasets as tv_datasets, transforms\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n\n# ---------------------------------------------------------------------------\n# CIFAR-10 helpers\n# ---------------------------------------------------------------------------\n\nclass _CIFARWrapper(torch.utils.data.Dataset):\n    def __init__(self, base, tfm):\n        self.base = base\n        self.tfm = tfm\n\n    def __len__(self):\n        return len(self.base)\n\n    def __getitem__(self, idx):\n        img, label = self.base[idx]\n        img = self.tfm(img)\n        return {\"pixel_values\": img, \"labels\": torch.tensor(label, dtype=torch.long)}\n\n\ndef _split(dataset: torch.utils.data.Dataset, splits: Tuple[float, float, float]):\n    n_total = len(dataset)\n    lengths = [int(p * n_total) for p in splits]\n    lengths[-1] = n_total - sum(lengths[:-1])\n    return random_split(dataset, lengths, generator=torch.Generator().manual_seed(42))\n\n\ndef _build_cifar10(cfg, trial_mode: bool):\n    size = cfg.dataset.preprocessing.resize\n    aug: List[str] = cfg.dataset.preprocessing.augmentations\n    tfm_train = [transforms.RandomCrop(size, padding=4) if \"random_crop\" in aug else transforms.Resize(size),\n                 transforms.RandomHorizontalFlip() if \"horizontal_flip\" in aug else transforms.Lambda(lambda x: x),\n                 transforms.ToTensor()]\n    tfm_eval = [transforms.Resize(size), transforms.ToTensor()]\n\n    train_ds = tv_datasets.CIFAR10(root=\".cache/\", train=True, download=True)\n    test_ds = tv_datasets.CIFAR10(root=\".cache/\", train=False, download=True)\n    full = ConcatDataset([train_ds, test_ds])\n    train_set, val_set, test_set = _split(full, (cfg.dataset.split.train, cfg.dataset.split.val, cfg.dataset.split.test))\n\n    train_set = _CIFARWrapper(train_set, transforms.Compose(tfm_train))\n    val_set = _CIFARWrapper(val_set, transforms.Compose(tfm_eval))\n    test_set = _CIFARWrapper(test_set, transforms.Compose(tfm_eval))\n\n    bs = 2 if trial_mode else cfg.training.batch_size\n    dl_kwargs = dict(batch_size=bs, num_workers=2, pin_memory=True)\n    return (DataLoader(train_set, shuffle=True, **dl_kwargs),\n            DataLoader(val_set, shuffle=False, **dl_kwargs),\n            DataLoader(test_set, shuffle=False, **dl_kwargs),\n            None)\n\n\n# ---------------------------------------------------------------------------\n# Alpaca-cleaned helpers\n# ---------------------------------------------------------------------------\n\ndef _label_from_instruction(instr: str) -\u003e int:\n    lowered = instr.lower()\n    keywords = [\"translate\", \"translation\", \"translating\", \"translator\"]\n    return int(any(k in lowered for k in keywords))\n\n\ndef _encode_text(batch, tokenizer, max_len):\n    instructions = batch[\"instruction\"]\n    inputs = batch.get(\"input\", [\"\"] * len(instructions))\n    texts = [f\"Instruction: {ins} Input: {inp}\" for ins, inp in zip(instructions, inputs)]\n    enc = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n    enc[\"labels\"] = [_label_from_instruction(ins) for ins in instructions]\n    return enc\n\n\ndef _build_alpaca(cfg, trial_mode: bool):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    raw = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", cache_dir=\".cache/\")\n\n    encoded = raw.map(lambda b: _encode_text(b, tokenizer, cfg.dataset.max_length), batched=True,\n                      remove_columns=raw.column_names)\n    encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    n = len(encoded)\n    train_size = int(cfg.dataset.split.train * n)\n    val_size = int(cfg.dataset.split.val * n)\n    test_size = n - train_size - val_size\n    train_ds, val_ds, test_ds = random_split(encoded, [train_size, val_size, test_size],\n                                             generator=torch.Generator().manual_seed(42))\n\n    bs = 2 if trial_mode else cfg.training.batch_size\n    dl_kwargs = dict(batch_size=bs, num_workers=2, pin_memory=True)\n    return (DataLoader(train_ds, shuffle=True, **dl_kwargs),\n            DataLoader(val_ds, shuffle=False, **dl_kwargs),\n            DataLoader(test_ds, shuffle=False, **dl_kwargs),\n            tokenizer)\n\n\n# ---------------------------------------------------------------------------\n# Public factory\n# ---------------------------------------------------------------------------\n\ndef build_dataloaders(cfg, trial_mode: bool = False):\n    name = cfg.dataset.name.lower()\n    if name == \"cifar10\":\n        return _build_cifar10(cfg, trial_mode)\n    elif name == \"alpaca-cleaned\":\n        return _build_alpaca(cfg, trial_mode)\n    else:\n        raise ValueError(f\"Unsupported dataset {name}\")\n", "pyproject_toml": "[project]\nname = \"distilbert-adapter-exp\"\nversion = \"0.1.0\"\ndescription = \"DistilBERT adapter experiments with Hydra \u0026 WandB\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntransformers = \"\u003e=4.35.0\"\ndatasets = \"\u003e=2.14.0\"\ntorchvision = \"\u003e=0.15.0\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.15.0\"\nscikit-learn = \"^1.3.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\noptuna = \"^3.3.0\"\npandas = \"^2.1.0\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "import os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Optional\n\nimport hydra\nimport optuna\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import OmegaConf\n\nimport torch\nfrom torch import nn\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import get_linear_schedule_with_warmup\nimport wandb\n\n# Local imports (executed with -m from repo root)\nfrom src.preprocess import build_dataloaders\nfrom src.model import build_model\n\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef _set_seed(seed: int = 42):\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _save_cfg(cfg, results_dir: Path):\n    results_dir.mkdir(parents=True, exist_ok=True)\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        OmegaConf.save(config=cfg, f=str(cfg_path))\n\n\n# ---------------------------------------------------------------------------\n# Core train / eval\n# ---------------------------------------------------------------------------\n\ndef _train_eval(cfg, trial: Optional[optuna.Trial] = None, verbose: bool = True) -\u003e Dict[str, Any]:\n    \"\"\"Train one full run (or shortened trial) and return best metrics.\"\"\"\n\n    trial_mode: bool = cfg.get(\"trial_mode\", False)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ---------------------------------------------------------------------\n    # Data \u0026 model\n    # ---------------------------------------------------------------------\n    train_loader, val_loader, _, tokenizer = build_dataloaders(cfg, trial_mode=trial_mode)\n    model = build_model(cfg, tokenizer).to(device)\n\n    # Optimiser \u0026 scheduler ------------------------------------------------\n    opt_map = {\"adamw\": torch.optim.AdamW, \"sgd\": torch.optim.SGD}\n    optim_cls = opt_map.get(cfg.training.optimizer.lower())\n    if optim_cls is None:\n        raise ValueError(f\"Unsupported optimizer {cfg.training.optimizer}\")\n\n    optimizer = optim_cls(model.parameters(), lr=cfg.training.learning_rate, weight_decay=cfg.training.weight_decay)\n    total_steps = len(train_loader) * cfg.training.epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.training.warmup_steps,\n        num_training_steps=total_steps,\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    # WandB ---------------------------------------------------------------\n    use_wandb = (trial is None) and (cfg.wandb.mode != \"disabled\") and (not trial_mode)\n    if use_wandb:\n        run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n        )\n        if verbose:\n            print(f\"WandB URL: {wandb.run.get_url()}\")\n    else:\n        run = None\n\n    # Epoch loop ----------------------------------------------------------\n    epochs = 1 if trial_mode else cfg.training.epochs if trial is None else cfg.optuna.get(\"max_epochs\", 3)\n    global_step = 0\n    best_val_acc, best_val_f1 = 0.0, 0.0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_losses: List[float] = []\n        epoch_true, epoch_pred = [], []\n\n        for step, batch in enumerate(train_loader):\n            if trial_mode and step \u003e 1:\n                break\n            labels = batch.pop(\"labels\").to(device)\n            for k in batch:\n                batch[k] = batch[k].to(device)\n            outputs = model(batch)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n            epoch_losses.append(loss.item())\n            epoch_pred.extend(outputs.argmax(dim=-1).detach().cpu().tolist())\n            epoch_true.extend(labels.detach().cpu().tolist())\n            global_step += 1\n\n        train_loss = float(sum(epoch_losses) / max(1, len(epoch_losses)))\n        train_acc = accuracy_score(epoch_true, epoch_pred)\n        train_f1 = f1_score(epoch_true, epoch_pred, average=\"weighted\")\n\n        # ------------------------ Validation -----------------------------\n        model.eval()\n        val_losses: List[float] = []\n        val_true, val_pred = [], []\n        start_inf = time.time()\n        with torch.no_grad():\n            for step, batch in enumerate(val_loader):\n                if trial_mode and step \u003e 1:\n                    break\n                labels = batch.pop(\"labels\").to(device)\n                for k in batch:\n                    batch[k] = batch[k].to(device)\n                outputs = model(batch)\n                val_losses.append(criterion(outputs, labels).item())\n                val_pred.extend(outputs.argmax(dim=-1).detach().cpu().tolist())\n                val_true.extend(labels.detach().cpu().tolist())\n        inference_time = (time.time() - start_inf) / max(1, len(val_loader))\n        val_loss = float(sum(val_losses) / max(1, len(val_losses)))\n        val_acc = accuracy_score(val_true, val_pred)\n        val_f1 = f1_score(val_true, val_pred, average=\"weighted\")\n\n        best_val_acc = max(best_val_acc, val_acc)\n        best_val_f1 = max(best_val_f1, val_f1)\n\n        log_dict = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"train_acc\": train_acc,\n            \"val_acc\": val_acc,\n            \"train_f1\": train_f1,\n            \"val_f1\": val_f1,\n            \"best_val_acc\": best_val_acc,\n            \"best_val_f1\": best_val_f1,\n            \"inference_time\": inference_time,\n        }\n        if use_wandb:\n            wandb.log(log_dict, step=epoch)\n        if verbose:\n            print(f\"Epoch {epoch}: {log_dict}\")\n\n    if run is not None:\n        wandb.finish()\n\n    return {\"best_val_acc\": best_val_acc, \"best_val_f1\": best_val_f1}\n\n\n# ---------------------------------------------------------------------------\n# Optuna helpers\n# ---------------------------------------------------------------------------\n\ndef _suggest_and_apply(cfg, trial: optuna.Trial, param_map: Dict[str, List[str]]):\n    \"\"\"Apply Optuna suggestions to cfg in-place according to defined search space.\"\"\"\n    for param_name, space in cfg.optuna.search_space.items():\n        if space.type == \"loguniform\":\n            val = trial.suggest_float(param_name, space.low, space.high, log=True)\n        elif space.type == \"uniform\":\n            val = trial.suggest_float(param_name, space.low, space.high)\n        elif space.type == \"categorical\":\n            val = trial.suggest_categorical(param_name, space.choices)\n        elif space.type == \"int\":\n            val = trial.suggest_int(param_name, space.low, space.high, step=space.get(\"step\", 1))\n        else:\n            raise ValueError(f\"Unsupported search space type: {space.type}\")\n\n        # propagate value into cfg following param_map\n        ptr = cfg\n        for k in param_map[param_name][:-1]:\n            ptr = ptr[k]\n        ptr[param_map[param_name][-1]] = val\n\n\n# ---------------------------------------------------------------------------\n# Entrypoint\n# ---------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):  # noqa: C901\n    _set_seed()\n\n    # Ensure wandb.mode is disabled for trial_mode -------------------------\n    if cfg.get(\"trial_mode\", False):\n        cfg.wandb.mode = \"disabled\"\n\n    results_dir = Path(cfg.results_dir)\n    _save_cfg(cfg, results_dir)\n\n    # ------------------ Hyper-parameter optimisation ---------------------\n    if (cfg.optuna.n_trials \u003e 0) and (not cfg.get(\"trial_mode\", False)):\n        param_map = {\n            \"learning_rate\": [\"training\", \"learning_rate\"],\n            \"batch_size\": [\"training\", \"batch_size\"],\n            \"weight_decay\": [\"training\", \"weight_decay\"],\n            \"adapter_reduction_factor\": [\"model\", \"task_adapters\", \"reduction_factor\"],\n        }\n        study_path = results_dir / \"optuna_study.db\"\n        study = optuna.create_study(\n            direction=cfg.optuna.direction,\n            storage=f\"sqlite:///{study_path}\",\n            study_name=cfg.run.run_id,\n            load_if_exists=True,\n        )\n\n        def objective(trial: optuna.Trial):\n            trial_cfg = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n            _suggest_and_apply(trial_cfg, trial, param_map)\n            trial_cfg.wandb.mode = \"disabled\"  # always disable inside trials\n            trial_cfg.optuna.max_epochs = cfg.optuna.get(\"max_epochs\", 3)\n            metrics = _train_eval(trial_cfg, trial=trial, verbose=False)\n            return metrics[\"best_val_acc\"]\n\n        study.optimize(objective, n_trials=cfg.optuna.n_trials, gc_after_trial=True)\n        print(f\"Optuna best params: {study.best_params}\")\n        # apply best params to cfg for final training\n        for k, v in study.best_params.items():\n            if k == \"adapter_reduction_factor\":\n                cfg.model.task_adapters.reduction_factor = v\n            elif k in {\"learning_rate\", \"batch_size\", \"weight_decay\"}:\n                cfg.training[k] = v\n\n    # ---------------------- Final full training --------------------------\n    _train_eval(cfg, trial=None, verbose=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "Configuration interface is inconsistent with the implementation:\n1. config/config.yaml defines a `run` config-group (defaults: - run:<id>). Therefore all experiment parameters (method, model, dataset, training …) are available under cfg.run.* .\n2. train.py, preprocess.py, model.py and other modules access these parameters at the **top level** (e.g. cfg.method, cfg.dataset, cfg.training). At runtime those keys do not exist and AttributeErrors will be raised immediately (build_dataloaders, build_model, _train_eval, etc.).\n   • Example: build_model uses `adapter_cfg = cfg.model.task_adapters`, but cfg.model is inside cfg.run.\n3. Because of this mismatch none of the training / preprocessing code can execute.\n\nFix: either move all experiment fields to the root level in the run yaml files, or (preferred) change every access in the code to `cfg.run.method`, `cfg.run.model`, `cfg.run.dataset`, `cfg.run.training`, etc. and propagate the run sub-config where needed.\n\nUntil this is corrected the code cannot be executed, so it is *not* production-ready."
}
