
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training: `uv run python -u -m src.main run={run_id} results_dir={path}`
     * Evaluation: `uv run python -m src.evaluate results_dir={path}` (independent execution)
   - Supports trial_mode=true flag for lightweight validation runs (automatically disables WandB)

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics using `wandb.log()`
   - trial_mode automatically disables WandB (sets wandb.mode=disabled)
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/run/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path}`
   - main.py DOES NOT call evaluate.py
   - evaluate.py retrieves ALL data from WandB API using `wandb.Api()` (not from local files)
   - evaluate.py exports retrieved WandB data to `{results_dir}/wandb_data/` for reproducibility
   - evaluate.py generates ALL publication-quality PDF figures and saves to `{results_dir}/images/`
   - Proper figure quality: legends, annotations, tight_layout
   - Follows naming convention: `<figure_topic>[_<condition>][_pairN].pdf`
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Trial Mode Implementation**:
   - trial_mode=true flag properly reduces computational load
   - Training: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
   - WandB automatically disabled in trial_mode (wandb.mode=disabled)
   - Purpose: Fast validation that code runs without errors

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare DistilBERT with task-specific adapters (proposed) against standard fine-tuning (comparative) across vision and language tasks to evaluate performance improvements.

# Experimental Design
- Strategy: Comparative analysis of DistilBERT performance across vision and language tasks
- Proposed Method: Fine-tuned DistilBERT with task-specific adapters
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time']

# Experiment Runs

- Run ID: proposed-DistilBERT-base-66M-CIFAR-10
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-CIFAR-10
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 1e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: proposed-DistilBERT-base-66M-alpaca-cleaned
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-alpaca-cleaned
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 5e-5
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-CIFAR-10
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-CIFAR-10
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-alpaca-cleaned
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - run: proposed-DistilBERT-base-66M-CIFAR-10\n  - _self_\n\n# Global settings\nwandb:\n  entity: gengaru617-personal\n  project: 251017-test\n  mode: online  # will be overridden to \"disabled\" in trial_mode\n\n# Trial mode flag (overwritten via CLI)\ntrial_mode: false\n\n# Results directory (passed via CLI)\nresults_dir: outputs/\n\nhydra:\n  run:\n    dir: .\n  output_subdir: null", "evaluate_py": "import argparse\nimport os\nimport json\nfrom pathlib import Path\nfrom typing import Dict\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\n\n\nFIGURE_DIR = \"images\"\nDATA_DIR = \"wandb_data\"\n\n\ndef fetch_runs(entity: str, project: str):\n    api = wandb.Api()\n    return api.runs(f\"{entity}/{project}\")\n\n\ndef export_metrics(runs, out_dir: Path):\n    summaries = {}\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    for run in runs:\n        df = run.history(keys=[\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\", \"train_f1\", \"val_f1\"], pandas=True)\n        csv_path = out_dir / f\"run_{run.name}_metrics.csv\"\n        df.to_csv(csv_path, index=False)\n        summaries[run.name] = {\n            \"best_val_acc\": df[\"val_acc\"].max(),\n            \"best_val_f1\": df[\"val_f1\"].max(),\n            \"final_val_acc\": df[\"val_acc\"].iloc[-1],\n            \"final_val_f1\": df[\"val_f1\"].iloc[-1],\n        }\n    with open(out_dir / \"summary.json\", \"w\") as f:\n        json.dump(summaries, f, indent=2)\n    return summaries\n\n\ndef plot_learning_curves(runs, dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    for run in runs:\n        df = run.history(keys=[\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"], pandas=True)\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_loss\"], label=\"Train Loss\")\n        ax.plot(df[\"epoch\"], df[\"val_loss\"], label=\"Val Loss\")\n        ax.set_title(f\"Loss Curve - {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"loss_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_acc\"], label=\"Train Acc\")\n        ax.plot(df[\"epoch\"], df[\"val_acc\"], label=\"Val Acc\")\n        ax.set_title(f\"Accuracy Curve - {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"acc_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n\n\n\ndef plot_comparison(summary: Dict[str, Dict], dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    df = pd.DataFrame.from_dict(summary, orient=\"index\")\n    fig, ax = plt.subplots(figsize=(8, 4))\n    sns.barplot(data=df.reset_index(), x=\"index\", y=\"best_val_acc\", ax=ax)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_title(\"Best Validation Accuracy Across Runs\")\n    for p in ax.patches:\n        ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha=\u0027center\u0027, va=\u0027center\u0027, xytext=(0, 5), textcoords=\u0027offset points\u0027)\n    fig.tight_layout()\n    fname = dest / \"comparison_best_val_acc.pdf\"\n    fig.savefig(fname)\n    print(fname.name)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate WandB experiments and generate figures.\")\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory containing experiment metadata\")\n    args = parser.parse_args()\n\n    results_path = Path(args.results_dir)\n    config_path = results_path / \"config.yaml\"\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Expected config.yaml at {config_path}\")\n\n    import yaml\n    with open(config_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    entity = cfg[\"wandb\"][\"entity\"]\n    project = cfg[\"wandb\"][\"project\"]\n\n    runs = fetch_runs(entity, project)\n\n    data_dir = results_path / DATA_DIR\n    summary = export_metrics(runs, data_dir)\n\n    fig_dir = results_path / FIGURE_DIR\n    plot_learning_curves(runs, fig_dir)\n    plot_comparison(summary, fig_dir)\n", "main_py": "import subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import OmegaConf\n\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):\n    # Build command for subprocess call to train.py\n    cmd: List[str] = [sys.executable, \"-u\", \"-m\", \"src.train\", f\"run={cfg.run.run_id}\"]\n\n    if cfg.get(\"results_dir\"):\n        cmd.append(f\"results_dir={cfg.results_dir}\")\n    if cfg.get(\"trial_mode\", False):\n        cmd.extend([\"trial_mode=true\", \"wandb.mode=disabled\", \"optuna.n_trials=0\"])\n\n    # Forward additional overrides from command line not present in defaults\n    extra = hydra.utils.get_args()\n    cmd.extend(extra)\n\n    print(\"Executing sub-process:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "import torch\nfrom torch import nn\nfrom transformers import DistilBertModel, AutoTokenizer\n\n\nclass HoulsbyAdapter(nn.Module):\n    def __init__(self, hidden_dim: int, reduction_factor: int = 16, non_linearity: str = \"relu\"):\n        super().__init__()\n        bottleneck_dim = max(1, hidden_dim // reduction_factor)\n        self.down = nn.Linear(hidden_dim, bottleneck_dim)\n        self.act = getattr(nn, non_linearity.capitalize())() if hasattr(nn, non_linearity.capitalize()) else nn.ReLU()\n        self.up = nn.Linear(bottleneck_dim, hidden_dim)\n\n    def forward(self, x):\n        return self.up(self.act(self.down(x)))\n\n\nclass VisionPatchEmbedding(nn.Module):\n    def __init__(self, img_channels: int = 3, patch_size: int = 4, embed_dim: int = 768):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Linear(img_channels * patch_size * patch_size, embed_dim)\n\n    def forward(self, images):\n        B, C, H, W = images.shape\n        p = self.patch_size\n        assert H % p == 0 and W % p == 0, \"Image dimensions must be divisible by patch size\"\n        patches = images.unfold(2, p, p).unfold(3, p, p)  # B,C,H\u0027,W\u0027,p,p\n        patches = patches.contiguous().view(B, C, -1, p, p)\n        patches = patches.permute(0, 2, 1, 3, 4).contiguous()  # B, N, C, p, p\n        patches = patches.view(B, patches.size(1), -1)  # B,N,C*p*p\n        embeddings = self.proj(patches)\n        return embeddings  # B, N, D\n\n\nclass DistilBertClassifier(nn.Module):\n    def __init__(self, model_name: str, num_labels: int, adapter_cfg=None, vision_patch_cfg=None):\n        super().__init__()\n        self.is_vision = vision_patch_cfg is not None\n        self.bert = DistilBertModel.from_pretrained(model_name, cache_dir=\".cache/\")\n        hidden_dim = self.bert.config.hidden_size\n        if adapter_cfg and adapter_cfg.get(\"enabled\", False):\n            self.adapter = HoulsbyAdapter(hidden_dim, reduction_factor=adapter_cfg.reduction_factor, non_linearity=adapter_cfg.non_linearity)\n        else:\n            self.adapter = None\n        self.pre_classifier = nn.Linear(hidden_dim, hidden_dim)\n        self.activation = nn.GELU()\n        self.dropout = nn.Dropout(0.2)\n        self.classifier = nn.Linear(hidden_dim, num_labels)\n        if self.is_vision:\n            self.vision_embedding = VisionPatchEmbedding(patch_size=vision_patch_cfg.patch_size, embed_dim=hidden_dim)\n\n    def forward(self, batch):\n        if self.is_vision and \"pixel_values\" in batch:\n            embeddings = self.vision_embedding(batch[\"pixel_values\"])  # B,N,D\n            attention_mask = torch.ones(embeddings.size()[:-1], dtype=torch.long, device=embeddings.device)\n            outputs = self.bert(inputs_embeds=embeddings, attention_mask=attention_mask)\n            pooled = outputs.last_hidden_state[:, 0]  # First token\n        else:  # Text path\n            outputs = self.bert(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n            pooled = outputs.last_hidden_state[:, 0]\n        if self.adapter is not None:\n            pooled = pooled + self.adapter(pooled)\n        x = self.pre_classifier(pooled)\n        x = self.activation(x)\n        x = self.dropout(x)\n        logits = self.classifier(x)\n        return logits\n\n\ndef build_model(cfg, tokenizer=None):\n    adapter_cfg = cfg.model.task_adapters if cfg.method == \"proposed\" else None\n    vision_patch_cfg = cfg.model.vision_patch if cfg.model.get(\"input_representation\", \"text\") == \"image_sequence\" else None\n    model = DistilBertClassifier(cfg.model.name, cfg.model.num_labels, adapter_cfg=adapter_cfg, vision_patch_cfg=vision_patch_cfg)\n    return model\n", "preprocess_py": "from typing import Tuple, Optional\nimport math\nimport random\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, Subset\nfrom torchvision import datasets as tv_datasets, transforms\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset, Dataset as HFDataset\n\n\nclass CIFAR10DatasetWrapper(torch.utils.data.Dataset):\n    \"\"\"Returns (pixel_values, label) suitable for our model.\"\"\"\n\n    def __init__(self, base_dataset, transform):\n        self.base_dataset = base_dataset\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.base_dataset)\n\n    def __getitem__(self, idx):\n        img, label = self.base_dataset[idx]\n        img = self.transform(img)\n        return {\"pixel_values\": img, \"labels\": torch.tensor(label, dtype=torch.long)}\n\n\ndef _split_dataset(dataset, splits: Tuple[float, float, float]):\n    n_total = len(dataset)\n    n_train = int(splits[0] * n_total)\n    n_val = int(splits[1] * n_total)\n    n_test = n_total - n_train - n_val\n    return random_split(dataset, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(42))\n\n\ndef _build_cifar10(cfg, trial_mode=False):\n    patch_size = cfg.model.vision_patch.patch_size\n    resize_size = cfg.dataset.preprocessing.resize\n    batch_size = 2 if trial_mode else cfg.training.batch_size\n\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(resize_size, padding=4) if \"random_crop\" in cfg.dataset.preprocessing.augmentations else transforms.Resize(resize_size),\n        transforms.RandomHorizontalFlip() if \"horizontal_flip\" in cfg.dataset.preprocessing.augmentations else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ])\n    transform_eval = transforms.Compose([\n        transforms.Resize(resize_size),\n        transforms.ToTensor(),\n    ])\n\n    base_train = tv_datasets.CIFAR10(root=\".cache/\", train=True, download=True)\n    base_test = tv_datasets.CIFAR10(root=\".cache/\", train=False, download=True)\n    full_dataset = torch.utils.data.ConcatDataset([base_train, base_test])\n\n    train_set, val_set, test_set = _split_dataset(full_dataset, (cfg.dataset.split.train, cfg.dataset.split.val, cfg.dataset.split.test))\n\n    # Wrap with transforms\n    train_set = CIFAR10DatasetWrapper(train_set, transform_train)\n    val_set = CIFAR10DatasetWrapper(val_set, transform_eval)\n    test_set = CIFAR10DatasetWrapper(test_set, transform_eval)\n\n    loader_kwargs = dict(batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    train_loader = DataLoader(train_set, **loader_kwargs)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    return train_loader, val_loader, test_loader, None\n\n\ndef _encode_text(examples, tokenizer, max_length):\n    texts = [f\"Instruction: {ins} Input: {inp}\" for ins, inp in zip(examples[\"instruction\"], examples.get(\"input\", [\"\"] * len(examples)))]\n    encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_length)\n    encodings[\"labels\"] = [random.randint(0, 1) for _ in range(len(texts))]  # Dummy labels if not provided\n    return encodings\n\n\ndef _build_alpaca(cfg, trial_mode=False):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    max_len = cfg.dataset.max_length\n    raw_dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", cache_dir=\".cache/\")\n\n    # Encode\n    encoded = raw_dataset.map(lambda ex: _encode_text(ex, tokenizer, max_len), batched=True, remove_columns=raw_dataset.column_names)\n    encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    train_size = int(cfg.dataset.split.train * len(encoded))\n    val_size = int(cfg.dataset.split.val * len(encoded))\n    test_size = len(encoded) - train_size - val_size\n    train_dataset, val_dataset, test_dataset = random_split(encoded, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n\n    batch_size = 2 if trial_mode else cfg.training.batch_size\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, val_loader, test_loader, tokenizer\n\n\ndef build_dataloaders(cfg, trial_mode=False):\n    name = cfg.dataset.name.lower()\n    if name == \"cifar10\":\n        return _build_cifar10(cfg, trial_mode)\n    elif name == \"alpaca-cleaned\":\n        return _build_alpaca(cfg, trial_mode)\n    else:\n        raise ValueError(f\"Unsupported dataset {name}\")\n", "pyproject_toml": "[project]\nname = \"distilbert-adapter-exp\"\nversion = \"0.1.0\"\ndescription = \"Experiment code for DistilBERT adapters across tasks\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntransformers = \"\u003e=4.35.0\"\ndatasets = \"\u003e=2.14.0\"\ntorchvision = \"\u003e=0.15.0\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.15.0\"\nscikit-learn = \"^1.3.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\noptuna = \"^3.3.0\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "import os\nimport sys\nimport time\nimport subprocess\nfrom typing import Dict, Any\n\nimport hydra\nfrom omegaconf import OmegaConf\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score\nimport wandb\n\n# Local imports\nfrom .preprocess import build_dataloaders\nfrom .model import build_model\n\n\ndef _set_seed(seed: int = 42):\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\n@hydra.main(config_path=\"../config\")\ndef train(cfg):\n    cfg = OmegaConf.to_container(cfg, resolve=True)\n    # Convert back to OmegaConf for attribute access convenience\n    cfg = OmegaConf.create(cfg)\n\n    trial_mode = cfg.get(\"trial_mode\", False)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _set_seed()\n\n    # Data -------------------------------------------------------------------\n    train_loader, val_loader, test_loader, tokenizer = build_dataloaders(cfg, trial_mode=trial_mode)\n\n    # Model ------------------------------------------------------------------\n    model = build_model(cfg, tokenizer)\n    model.to(device)\n\n    # Optimizer --------------------------------------------------------------\n    optim_cls = {\n        \"adamw\": torch.optim.AdamW,\n        \"sgd\": torch.optim.SGD,\n    }[cfg.training.optimizer.lower()]\n\n    optimizer = optim_cls(\n        model.parameters(),\n        lr=cfg.training.learning_rate,\n        weight_decay=cfg.training.weight_decay,\n    )\n\n    criterion = nn.CrossEntropyLoss()\n\n    # WandB ------------------------------------------------------------------\n    wandb_mode = cfg.wandb.mode if not trial_mode else \"disabled\"\n    if wandb_mode != \"disabled\":\n        wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n            mode=wandb_mode,\n        )\n        print(f\"WandB URL: {wandb.run.get_url()}\")\n\n    # Training ---------------------------------------------------------------\n    epochs = 1 if trial_mode else cfg.training.epochs\n    global_step = 0\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses = []\n        true_labels, pred_labels = [], []\n        for step, batch in enumerate(train_loader):\n            if trial_mode and step \u003e 1:\n                break\n            labels = batch.pop(\"labels\").to(device)\n            for k in batch:\n                batch[k] = batch[k].to(device)\n            outputs = model(batch)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            optimizer.step()\n            optimizer.zero_grad()\n\n            train_losses.append(loss.item())\n            preds = outputs.argmax(dim=-1).detach().cpu().tolist()\n            pred_labels.extend(preds)\n            true_labels.extend(labels.detach().cpu().tolist())\n            global_step += 1\n\n        train_acc = accuracy_score(true_labels, pred_labels)\n        train_f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n        train_loss = sum(train_losses) / len(train_losses)\n\n        # Validation ---------------------------------------------------------\n        model.eval()\n        val_losses = []\n        val_true, val_pred = [], []\n        start_inf = time.time()\n        with torch.no_grad():\n            for step, batch in enumerate(val_loader):\n                if trial_mode and step \u003e 1:\n                    break\n                labels = batch.pop(\"labels\").to(device)\n                for k in batch:\n                    batch[k] = batch[k].to(device)\n                outputs = model(batch)\n                loss = criterion(outputs, labels)\n                val_losses.append(loss.item())\n                preds = outputs.argmax(dim=-1).detach().cpu().tolist()\n                val_pred.extend(preds)\n                val_true.extend(labels.detach().cpu().tolist())\n        inference_time = (time.time() - start_inf) / max(1, len(val_loader))\n        val_acc = accuracy_score(val_true, val_pred)\n        val_f1 = f1_score(val_true, val_pred, average=\"weighted\")\n        val_loss = sum(val_losses) / len(val_losses)\n\n        log_dict = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"train_acc\": train_acc,\n            \"val_acc\": val_acc,\n            \"train_f1\": train_f1,\n            \"val_f1\": val_f1,\n            \"inference_time\": inference_time,\n        }\n        if wandb_mode != \"disabled\":\n            wandb.log(log_dict, step=epoch)\n        print(f\"Epoch {epoch}: {log_dict}\")\n\n    if wandb_mode != \"disabled\":\n        wandb.finish()\n\n\nif __name__ == \"__main__\":\n    train()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "The code fails several core validation criteria:\n\n1. Incomplete / incorrect data pipeline\n   • For the \"alpaca-cleaned\" run the preprocessing routine assigns RANDOM labels (see _encode_text in preprocess.py). This makes the task meaningless and is not publication-worthy.\n   • Consequently accuracy / F1 that are logged are unrelated to the dataset, violating the “complete, production-ready” requirement.\n\n2. Missing Optuna integration\n   • An optuna section is defined in every run config, but train.py never creates / optimises a study.  optuna.n_trials overrides passed from CLI are ignored.  This breaks the promise of automated HPO in the experiment design.\n\n3. Evaluation script cannot run independently\n   • evaluate.py expects {results_dir}/config.yaml but train.py never saves such a file, so the script crashes when executed as specified (uv run python -m src.evaluate results_dir=…).\n\n4. Hydra/CLI inconsistencies\n   • main.py forwards extra CLI overrides using hydra.utils.get_args() (which inside a hydra run returns an empty list), therefore users cannot pass arbitrary overrides to the subprocess as required.\n\nBecause of these issues the experiment suite is not ready for immediate execution and publication-quality research."
}
