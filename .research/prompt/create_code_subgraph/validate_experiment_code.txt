
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training: `uv run python -u -m src.main run={run_id} results_dir={path}`
     * Evaluation: `uv run python -m src.evaluate results_dir={path}` (independent execution)
   - Supports trial_mode=true flag for lightweight validation runs (automatically disables WandB)

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics using `wandb.log()`
   - trial_mode automatically disables WandB (sets wandb.mode=disabled)
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/run/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path}`
   - main.py DOES NOT call evaluate.py
   - evaluate.py retrieves ALL data from WandB API using `wandb.Api()` (not from local files)
   - evaluate.py exports retrieved WandB data to `{results_dir}/wandb_data/` for reproducibility
   - evaluate.py generates ALL publication-quality PDF figures and saves to `{results_dir}/images/`
   - Proper figure quality: legends, annotations, tight_layout
   - Follows naming convention: `<figure_topic>[_<condition>][_pairN].pdf`
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Trial Mode Implementation**:
   - trial_mode=true flag properly reduces computational load
   - Training: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
   - WandB automatically disabled in trial_mode (wandb.mode=disabled)
   - Purpose: Fast validation that code runs without errors

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare DistilBERT with task-specific adapters (proposed) against standard fine-tuning (comparative) across vision and language tasks to evaluate performance improvements.

# Experimental Design
- Strategy: Comparative analysis of DistilBERT performance across vision and language tasks
- Proposed Method: Fine-tuned DistilBERT with task-specific adapters
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time']

# Experiment Runs

- Run ID: proposed-DistilBERT-base-66M-CIFAR-10
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-CIFAR-10
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 1e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: proposed-DistilBERT-base-66M-alpaca-cleaned
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-alpaca-cleaned
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 5e-5
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-CIFAR-10
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-CIFAR-10
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-alpaca-cleaned
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - run: proposed-DistilBERT-base-66M-CIFAR-10  # default run, override via CLI\n  - _self_\n\nwandb:\n  entity: gengaru617-personal\n  project: 251017-test\n  mode: online  # will be overridden to \"disabled\" automatically in trial_mode\n\ntrial_mode: false  # enable via CLI for CI validation\n\nresults_dir: outputs/\n\nhydra:\n  run:\n    dir: .\n  output_subdir: null\n", "evaluate_py": "\"\"\"Independent evaluation \u0026 figure generation for the collected WandB runs.\n\nFixes: ensure all metrics referenced in plotting exist. When a run does not\ncontain ``train_acc`` or other expected columns they are skipped gracefully.\n\"\"\"\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\n\nFIGURE_DIR = \"images\"\nDATA_DIR = \"wandb_data\"\n\n# ---------------------------------------------------------------------------\n# CLI helper\n# ---------------------------------------------------------------------------\n\ndef _parse_results_dir(argv) -\u003e Path:\n    \"\"\"Extract results_dir either from key=value or first positional argument.\"\"\"\n    for arg in argv[1:]:\n        if arg.startswith(\"results_dir=\"):\n            return Path(arg.split(\"=\", 1)[1]).expanduser().absolute()\n    if len(argv) \u003e 1 and not argv[1].startswith(\"-\"):\n        return Path(argv[1]).expanduser().absolute()\n    raise ValueError(\"results_dir argument is required (e.g., results_dir=outputs/)\")\n\n\n# ---------------------------------------------------------------------------\n# WandB helpers\n# ---------------------------------------------------------------------------\n\ndef _fetch_runs(entity: str, project: str):\n    api = wandb.Api()\n    return api.runs(f\"{entity}/{project}\")\n\n\ndef _export_metrics(runs, out_dir: Path):\n    \"\"\"Save per-run history CSV \u0026 summary JSON. Returns summary dict.\"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    summaries: Dict[str, Dict] = {}\n\n    needed_cols: List[str] = [\n        \"epoch\",\n        \"best_val_acc\",\n        \"best_val_f1\",\n        \"train_loss\",\n        \"val_loss\",\n        \"train_acc\",\n        \"val_acc\",\n        \"train_f1\",\n        \"val_f1\",\n        \"inference_time\",\n    ]\n    for run in runs:\n        df = run.history(keys=needed_cols, pandas=True)\n        df.to_csv(out_dir / f\"run_{run.name}_metrics.csv\", index=False)\n\n        # Summary per run ---------------------------------------------------\n        summaries[run.name] = {\n            \"best_val_acc\": float(df[\"best_val_acc\"].dropna().max()) if \"best_val_acc\" in df else None,\n            \"best_val_f1\": float(df[\"best_val_f1\"].dropna().max()) if \"best_val_f1\" in df else None,\n        }\n    with open(out_dir / \"summary.json\", \"w\") as f:\n        json.dump(summaries, f, indent=2)\n    return summaries\n\n\n# ---------------------------------------------------------------------------\n# Plotting helpers\n# ---------------------------------------------------------------------------\n\ndef _plot_learning_curves(runs, dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    for run in runs:\n        df = run.history(keys=[\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"], pandas=True)\n        if df.empty or \"epoch\" not in df:\n            continue\n\n        # Loss curve -------------------------------------------------------\n        fig, ax = plt.subplots(figsize=(6, 4))\n        if \"train_loss\" in df and not df[\"train_loss\"].isnull().all():\n            ax.plot(df[\"epoch\"], df[\"train_loss\"], label=\"Train Loss\")\n        if \"val_loss\" in df and not df[\"val_loss\"].isnull().all():\n            ax.plot(df[\"epoch\"], df[\"val_loss\"], label=\"Val Loss\")\n        ax.set_title(f\"Loss Curve \u2013 {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"loss_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n        # Accuracy curve ---------------------------------------------------\n        if (\"train_acc\" in df and not df[\"train_acc\"].isnull().all()) or (\n            \"val_acc\" in df and not df[\"val_acc\"].isnull().all()\n        ):\n            fig, ax = plt.subplots(figsize=(6, 4))\n            if \"train_acc\" in df and not df[\"train_acc\"].isnull().all():\n                ax.plot(df[\"epoch\"], df[\"train_acc\"], label=\"Train Acc\")\n            if \"val_acc\" in df and not df[\"val_acc\"].isnull().all():\n                ax.plot(df[\"epoch\"], df[\"val_acc\"], label=\"Val Acc\")\n            ax.set_title(f\"Accuracy Curve \u2013 {run.name}\")\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"Accuracy\")\n            ax.legend()\n            fig.tight_layout()\n            fname = dest / f\"acc_curve_{run.name}.pdf\"\n            fig.savefig(fname)\n            print(fname.name)\n            plt.close(fig)\n\n\ndef _plot_comparison(summary: Dict[str, Dict], dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    df = pd.DataFrame.from_dict(summary, orient=\"index\").reset_index().rename(columns={\"index\": \"run_id\"})\n    if df[\"best_val_acc\"].isnull().all():\n        return\n    fig, ax = plt.subplots(figsize=(max(6, len(df) * 0.8), 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"best_val_acc\", ax=ax, palette=\"viridis\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_title(\"Best Validation Accuracy Across Runs\")\n    for p in ax.patches:\n        ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                    ha=\"center\", va=\"center\", xytext=(0, 5), textcoords=\"offset points\")\n    fig.tight_layout()\n    fname = dest / \"comparison_best_val_acc.pdf\"\n    fig.savefig(fname)\n    print(fname.name)\n    plt.close(fig)\n\n\n# ---------------------------------------------------------------------------\n# Improvement computation\n# ---------------------------------------------------------------------------\n\ndef _compute_improvement(summary: Dict[str, Dict]):\n    records: Dict[str, Dict[str, float]] = {}\n    for run_id, metrics in summary.items():\n        # heuristic: dataset name is last token after last \u0027-\u0027\n        dataset_name = run_id.split(\"-\")[-1]\n        method = \"proposed\" if run_id.startswith(\"proposed\") else \"baseline\"\n        records.setdefault(dataset_name, {})[method] = metrics[\"best_val_acc\"]\n\n    improvement = {\n        ds: {\n            **vals,\n            \"improvement_rate\": ((vals.get(\"proposed\") - vals.get(\"baseline\")) / vals.get(\"baseline\")) if vals.get(\"baseline\") else None,\n        }\n        for ds, vals in records.items()\n    }\n    return improvement\n\n\n# ---------------------------------------------------------------------------\n# Entrypoint\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    results_dir = _parse_results_dir(sys.argv)\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"Config file not found at {cfg_path}\")\n\n    import yaml\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    entity = cfg[\"wandb\"][\"entity\"]\n    project = cfg[\"wandb\"][\"project\"]\n\n    runs = _fetch_runs(entity, project)\n    summary = _export_metrics(runs, results_dir / DATA_DIR)\n\n    _plot_learning_curves(runs, results_dir / FIGURE_DIR)\n    _plot_comparison(summary, results_dir / FIGURE_DIR)\n\n    improvement = _compute_improvement(summary)\n    with open(results_dir / DATA_DIR / \"improvement.json\", \"w\") as f:\n        json.dump(improvement, f, indent=2)\n\n    print(\"Figures and data exported successfully.\")\n", "main_py": "\"\"\"Experiment orchestrator: launches src.train as a subprocess with inherited Hydra overrides.\"\"\"\n\nimport subprocess\nimport sys\nfrom typing import List\n\nimport hydra\nfrom hydra.utils import get_original_cli_args\n\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):\n    \"\"\"Hydra entrypoint that forwards all CLI overrides to the training subprocess.\"\"\"\n    original_cli: List[str] = get_original_cli_args()\n\n    # Build subprocess command ------------------------------------------------\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + original_cli\n\n    # Force WandB disabled when trial_mode=true --------------------------------\n    if cfg.get(\"trial_mode\", False):\n        cmd.append(\"wandb.mode=disabled\")\n\n    print(\"Executing subprocess:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"Model architectures.\"\"\"\n\nimport torch\nfrom torch import nn\nfrom transformers import DistilBertModel\n\n# ---------------------------------------------------------------------------\n# Auxiliary modules\n# ---------------------------------------------------------------------------\n\nclass HoulsbyAdapter(nn.Module):\n    \"\"\"Implementation of Houlsby adapter (bottleneck).\"\"\"\n\n    def __init__(self, hidden_dim: int, reduction_factor: int = 16, non_linearity: str = \"relu\"):\n        super().__init__()\n        bottleneck = max(1, hidden_dim // reduction_factor)\n        self.down = nn.Linear(hidden_dim, bottleneck)\n        self.nonlin = getattr(nn, non_linearity.capitalize())() if hasattr(nn, non_linearity.capitalize()) else nn.ReLU()\n        self.up = nn.Linear(bottleneck, hidden_dim)\n\n    def forward(self, x):\n        return self.up(self.nonlin(self.down(x)))\n\n\nclass VisionPatchEmbedding(nn.Module):\n    \"\"\"Small ViT-style patch embedding projecting image patches to hidden dim.\"\"\"\n\n    def __init__(self, in_channels: int, patch_size: int, embed_dim: int):\n        super().__init__()\n        self.patch_size = int(patch_size)\n        self.proj = nn.Linear(in_channels * self.patch_size * self.patch_size, embed_dim)\n\n    def forward(self, imgs):  # imgs: B,C,H,W\n        B, C, H, W = imgs.shape\n        p = self.patch_size\n        assert H % p == 0 and W % p == 0, \"Image dimensions must be divisible by patch size\"\n        patches = imgs.unfold(2, p, p).unfold(3, p, p)  # B,C,nH,nW,p,p\n        patches = patches.contiguous().view(B, C, -1, p, p)\n        patches = patches.permute(0, 2, 1, 3, 4).contiguous()  # B,N,C,p,p\n        patches = patches.view(B, patches.size(1), -1)  # B,N,C*p*p\n        return self.proj(patches)  # B, N, D\n\n# ---------------------------------------------------------------------------\n# DistilBERT classifier (text or vision)\n# ---------------------------------------------------------------------------\n\nclass DistilBertClassifier(nn.Module):\n    \"\"\"Adapter-ready DistilBERT classifier supporting vision patch inputs.\"\"\"\n\n    def __init__(self, model_name: str, num_labels: int, adapter_cfg=None, vision_patch_cfg=None):\n        super().__init__()\n        self.is_vision = vision_patch_cfg is not None\n        self.bert = DistilBertModel.from_pretrained(model_name, cache_dir=\".cache/\")\n        hidden = self.bert.config.hidden_size\n\n        if self.is_vision:\n            self.vision = VisionPatchEmbedding(3, vision_patch_cfg.patch_size, hidden)\n        else:\n            self.vision = None\n\n        if adapter_cfg and adapter_cfg.get(\"enabled\", False):\n            self.adapter = HoulsbyAdapter(\n                hidden_dim=hidden,\n                reduction_factor=int(adapter_cfg.reduction_factor),\n                non_linearity=str(adapter_cfg.non_linearity),\n            )\n        else:\n            self.adapter = None\n\n        self.pre_cls = nn.Linear(hidden, hidden)\n        self.act = nn.GELU()\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(hidden, num_labels)\n\n    def forward(self, batch):\n        if self.is_vision and \"pixel_values\" in batch:\n            embeds = self.vision(batch[\"pixel_values\"])  # B,N,D\n            mask = torch.ones(embeds.shape[:2], dtype=torch.long, device=embeds.device)\n            outputs = self.bert(inputs_embeds=embeds, attention_mask=mask)\n        else:\n            outputs = self.bert(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n            )\n\n        pooled = outputs.last_hidden_state[:, 0]  # CLS-token equivalent\n        if self.adapter is not None:\n            pooled = pooled + self.adapter(pooled)\n        x = self.pre_cls(pooled)\n        x = self.act(x)\n        x = self.dropout(x)\n        logits = self.cls(x)\n        return logits\n\n# ---------------------------------------------------------------------------\n# Factory\n# ---------------------------------------------------------------------------\n\ndef build_model(cfg, tokenizer=None):\n    \"\"\"Factory that builds the appropriate model according to cfg.\"\"\"\n    vision_patch_cfg = (\n        cfg.model.vision_patch if str(cfg.model.get(\"input_representation\", \"text\")) == \"image_sequence\" else None\n    )\n    method_tag = str(cfg.method).lower()\n    adapter_cfg = cfg.model.task_adapters if method_tag.startswith(\"proposed\") else None\n    return DistilBertClassifier(\n        model_name=cfg.model.name,\n        num_labels=int(cfg.model.num_labels),\n        adapter_cfg=adapter_cfg,\n        vision_patch_cfg=vision_patch_cfg,\n    )\n", "preprocess_py": "\"\"\"Dataset preprocessing and DataLoader construction.\"\"\"\nfrom typing import Tuple, List\nimport random\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import datasets as tv_datasets, transforms\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n# ----------------------------------------------------------------------------\n# CIFAR-10 helpers\n# ----------------------------------------------------------------------------\n\nclass _CIFARWrapper(torch.utils.data.Dataset):\n    \"\"\"Wrap torchvision dataset to return dicts compatible with model forward.\"\"\"\n\n    def __init__(self, base, tfm):\n        self.base = base\n        self.tfm = tfm\n\n    def __len__(self):\n        return len(self.base)\n\n    def __getitem__(self, idx):\n        img, label = self.base[idx]\n        img = self.tfm(img)\n        return {\"pixel_values\": img, \"labels\": torch.tensor(label, dtype=torch.long)}\n\n\ndef _split(dataset: torch.utils.data.Dataset, splits: Tuple[float, float, float]):\n    n_total = len(dataset)\n    lengths = [int(p * n_total) for p in splits]\n    lengths[-1] = n_total - sum(lengths[:-1])  # ensure all data used\n    return random_split(dataset, lengths, generator=torch.Generator().manual_seed(42))\n\n\ndef _build_cifar10(cfg, trial_mode: bool):\n    size = int(cfg.dataset.preprocessing.resize)\n    aug: List[str] = list(cfg.dataset.preprocessing.augmentations)\n\n    tfm_train = [\n        transforms.RandomCrop(size, padding=4) if \"random_crop\" in aug else transforms.Resize(size),\n        transforms.RandomHorizontalFlip() if \"horizontal_flip\" in aug else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ]\n    tfm_eval = [transforms.Resize(size), transforms.ToTensor()]\n\n    train_ds = tv_datasets.CIFAR10(root=\".cache/\", train=True, download=True)\n    test_ds = tv_datasets.CIFAR10(root=\".cache/\", train=False, download=True)\n    full_ds = ConcatDataset([train_ds, test_ds])\n    train_set, val_set, test_set = _split(full_ds, (\n        cfg.dataset.split.train,\n        cfg.dataset.split.val,\n        cfg.dataset.split.test,\n    ))\n\n    train_set = _CIFARWrapper(train_set, transforms.Compose(tfm_train))\n    val_set = _CIFARWrapper(val_set, transforms.Compose(tfm_eval))\n    test_set = _CIFARWrapper(test_set, transforms.Compose(tfm_eval))\n\n    bs = 2 if trial_mode else int(cfg.training.batch_size)\n    dl_kwargs = dict(batch_size=bs, num_workers=2, pin_memory=True)\n    return (\n        DataLoader(train_set, shuffle=True, **dl_kwargs),\n        DataLoader(val_set, shuffle=False, **dl_kwargs),\n        DataLoader(test_set, shuffle=False, **dl_kwargs),\n        None,\n    )\n\n# ----------------------------------------------------------------------------\n# Alpaca-cleaned helpers (binary classification: translation vs others)\n# ----------------------------------------------------------------------------\n\ndef _label_from_instruction(instr: str) -\u003e int:\n    lowered = instr.lower()\n    keywords = [\"translate\", \"translation\", \"translating\", \"translator\"]\n    return int(any(k in lowered for k in keywords))\n\n\ndef _encode_text(batch, tokenizer, max_len):\n    instructions = batch[\"instruction\"]\n    inputs = batch.get(\"input\", [\"\" for _ in range(len(instructions))])\n    texts = [f\"Instruction: {ins} Input: {inp}\" for ins, inp in zip(instructions, inputs)]\n    enc = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n    enc[\"labels\"] = [_label_from_instruction(ins) for ins in instructions]\n    return enc\n\n\ndef _build_alpaca(cfg, trial_mode: bool):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    raw = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", cache_dir=\".cache/\")\n\n    encoded = raw.map(\n        lambda b: _encode_text(b, tokenizer, cfg.dataset.max_length),\n        batched=True,\n        remove_columns=raw.column_names,\n    )\n    encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    total = len(encoded)\n    train_size = int(cfg.dataset.split.train * total)\n    val_size = int(cfg.dataset.split.val * total)\n    test_size = total - train_size - val_size\n    train_ds, val_ds, test_ds = random_split(\n        encoded,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42),\n    )\n\n    bs = 2 if trial_mode else int(cfg.training.batch_size)\n    dl_kwargs = dict(batch_size=bs, num_workers=2, pin_memory=True)\n    return (\n        DataLoader(train_ds, shuffle=True, **dl_kwargs),\n        DataLoader(val_ds, shuffle=False, **dl_kwargs),\n        DataLoader(test_ds, shuffle=False, **dl_kwargs),\n        tokenizer,\n    )\n\n# ----------------------------------------------------------------------------\n# Public factory\n# ----------------------------------------------------------------------------\n\ndef build_dataloaders(cfg, trial_mode: bool = False):\n    name = str(cfg.dataset.name).lower()\n    if name == \"cifar10\":\n        return _build_cifar10(cfg, trial_mode)\n    elif name == \"alpaca-cleaned\":\n        return _build_alpaca(cfg, trial_mode)\n    else:\n        raise ValueError(f\"Unsupported dataset \u0027{cfg.dataset.name}\u0027\")\n", "pyproject_toml": "[project]\nname = \"distilbert-adapter-exp\"\nversion = \"0.1.0\"\ndescription = \"DistilBERT adapter experiments with Hydra \u0026 WandB\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntransformers = \"\u003e=4.35.0\"\ndatasets = \"\u003e=2.14.0\"\ntorchvision = \"\u003e=0.15.0\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.15.0\"\nscikit-learn = \"^1.3.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\noptuna = \"^3.3.0\"\npandas = \"^2.1.0\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "import os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Optional, Sequence\n\nimport hydra\nimport optuna\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import OmegaConf\n\nimport torch\nfrom torch import nn\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import get_linear_schedule_with_warmup\nimport wandb\n\n# ---------------------------------------------------------------------------\n# Local imports (executed with -m from repository root)\n# ---------------------------------------------------------------------------\nfrom src.preprocess import build_dataloaders\nfrom src.model import build_model\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef _set_seed(seed: int = 42):\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _save_cfg(cfg, results_dir: Path):\n    \"\"\"Persist the (flattened) Hydra config for reproduction/evaluation.\"\"\"\n    results_dir.mkdir(parents=True, exist_ok=True)\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        OmegaConf.save(config=cfg, f=str(cfg_path))\n\n\n# ---------------------------------------------------------------------------\n# Core training / evaluation routine\n# ---------------------------------------------------------------------------\n\ndef _train_eval(cfg, trial: Optional[optuna.Trial] = None, verbose: bool = True) -\u003e Dict[str, Any]:\n    \"\"\"Complete training \u0026 validation loop. Returns best metrics recorded.\"\"\"\n\n    trial_mode: bool = bool(cfg.get(\"trial_mode\", False))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------- Data \u0026 Model -----------------------------------\n    train_loader, val_loader, _, tokenizer = build_dataloaders(cfg, trial_mode=trial_mode)\n    model = build_model(cfg, tokenizer).to(device)\n\n    # --------------------- Optimizer \u0026 LR schedule ------------------------\n    opt_map = {\"adamw\": torch.optim.AdamW, \"sgd\": torch.optim.SGD}\n    optim_cls = opt_map.get(str(cfg.training.optimizer).lower())\n    if optim_cls is None:\n        raise ValueError(f\"Unsupported optimizer {cfg.training.optimizer}\")\n\n    optimizer = optim_cls(model.parameters(), lr=cfg.training.learning_rate, weight_decay=cfg.training.weight_decay)\n    total_steps = len(train_loader) * (1 if trial_mode else cfg.training.epochs)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.training.warmup_steps,\n        num_training_steps=max(total_steps, 1),\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    # --------------------- WandB ------------------------------------------\n    use_wandb = (trial is None) and (cfg.wandb.mode != \"disabled\") and (not trial_mode)\n    if use_wandb:\n        run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run_id,\n            resume=\"allow\",\n            config=OmegaConf.to_container(cfg, resolve=True),\n            mode=cfg.wandb.mode,\n        )\n        if verbose:\n            print(f\"WandB URL: {wandb.run.get_url()}\")\n    else:\n        run = None\n\n    # --------------------- Training loop ----------------------------------\n    epochs = 1 if trial_mode else (cfg.optuna.get(\"max_epochs\", cfg.training.epochs) if trial is not None else cfg.training.epochs)\n    best_val_acc, best_val_f1 = 0.0, 0.0\n\n    global_step = 0\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_losses: List[float] = []\n        epoch_true: List[int] = []\n        epoch_pred: List[int] = []\n\n        for step, batch in enumerate(train_loader):\n            if trial_mode and step \u003e 1:\n                break\n            labels = batch.pop(\"labels\").to(device)\n            for k in batch:\n                batch[k] = batch[k].to(device)\n            outputs = model(batch)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n            epoch_losses.append(loss.item())\n            epoch_pred.extend(outputs.argmax(dim=-1).detach().cpu().tolist())\n            epoch_true.extend(labels.detach().cpu().tolist())\n            global_step += 1\n\n        train_loss = float(sum(epoch_losses) / max(1, len(epoch_losses)))\n        train_acc = accuracy_score(epoch_true, epoch_pred)\n        train_f1 = f1_score(epoch_true, epoch_pred, average=\"weighted\")\n\n        # --------------------- Validation ---------------------------------\n        model.eval()\n        val_losses: List[float] = []\n        val_true: List[int] = []\n        val_pred: List[int] = []\n        start_inf = time.time()\n        with torch.no_grad():\n            for step, batch in enumerate(val_loader):\n                if trial_mode and step \u003e 1:\n                    break\n                labels = batch.pop(\"labels\").to(device)\n                for k in batch:\n                    batch[k] = batch[k].to(device)\n                outputs = model(batch)\n                val_losses.append(criterion(outputs, labels).item())\n                val_pred.extend(outputs.argmax(dim=-1).detach().cpu().tolist())\n                val_true.extend(labels.detach().cpu().tolist())\n        inference_time = (time.time() - start_inf) / max(1, len(val_loader))\n        val_loss = float(sum(val_losses) / max(1, len(val_losses)))\n        val_acc = accuracy_score(val_true, val_pred)\n        val_f1 = f1_score(val_true, val_pred, average=\"weighted\")\n\n        best_val_acc = max(best_val_acc, val_acc)\n        best_val_f1 = max(best_val_f1, val_f1)\n\n        log_dict = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"train_acc\": train_acc,\n            \"val_acc\": val_acc,\n            \"train_f1\": train_f1,\n            \"val_f1\": val_f1,\n            \"best_val_acc\": best_val_acc,\n            \"best_val_f1\": best_val_f1,\n            \"inference_time\": inference_time,\n        }\n        if use_wandb:\n            wandb.log(log_dict, step=epoch)\n        if verbose:\n            print(f\"Epoch {epoch}: {log_dict}\")\n\n    if run is not None:\n        wandb.finish()\n\n    return {\"best_val_acc\": best_val_acc, \"best_val_f1\": best_val_f1}\n\n\n# ---------------------------------------------------------------------------\n# Optuna helpers\n# ---------------------------------------------------------------------------\n\ndef _apply_optuna_suggestions(cfg, trial: optuna.Trial, param_map: Dict[str, Sequence[str]]):\n    \"\"\"Mutate `cfg` in-place with values suggested by Optuna.\"\"\"\n    for param_name, space in cfg.optuna.search_space.items():\n        stype = str(space.type)\n        if stype == \"loguniform\":\n            val = trial.suggest_float(param_name, float(space.low), float(space.high), log=True)\n        elif stype == \"uniform\":\n            val = trial.suggest_float(param_name, float(space.low), float(space.high))\n        elif stype == \"categorical\":\n            val = trial.suggest_categorical(param_name, list(space.choices))\n        elif stype == \"int\":\n            val = trial.suggest_int(param_name, int(space.low), int(space.high), step=int(space.get(\"step\", 1)))\n        else:\n            raise ValueError(f\"Unsupported search space type: {space.type}\")\n\n        # propagate into cfg along mapped path (create intermediates if missing)\n        ptr = cfg\n        for key in param_map[param_name][:-1]:\n            if key not in ptr:\n                ptr[key] = {}\n            ptr = ptr[key]\n        ptr[param_map[param_name][-1]] = val\n\n\n# ---------------------------------------------------------------------------\n# Hydra entrypoint ----------------------------------------------------------\n# ---------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):  # noqa: C901\n    \"\"\"Hydra-managed entrypoint. Handles Optuna \u0026 final training.\"\"\"\n\n    _set_seed()\n\n    # ---------------------------------------------------------------------\n    # Flatten configuration: merge run-specific subtree into root namespace\n    # ---------------------------------------------------------------------\n    flat_cfg = OmegaConf.merge(cfg, cfg.run)  # later keys (run) overwrite root duplicates\n    if cfg.get(\"trial_mode\", False):\n        flat_cfg.trial_mode = True\n        flat_cfg.wandb.mode = \"disabled\"\n    flat_cfg.results_dir = cfg.results_dir\n\n    # results directory ----------------------------------------------------\n    results_dir = Path(flat_cfg.results_dir).expanduser().absolute()\n    _save_cfg(flat_cfg, results_dir)\n\n    # --------------------- Hyper-parameter search -------------------------\n    if int(flat_cfg.optuna.n_trials) \u003e 0 and not flat_cfg.get(\"trial_mode\", False):\n        param_map = {\n            \"learning_rate\": [\"training\", \"learning_rate\"],\n            \"batch_size\": [\"training\", \"batch_size\"],\n            \"weight_decay\": [\"training\", \"weight_decay\"],\n            \"adapter_reduction_factor\": [\"model\", \"task_adapters\", \"reduction_factor\"],\n        }\n\n        study_path = results_dir / \"optuna_study.db\"\n        study = optuna.create_study(\n            study_name=str(flat_cfg.run_id),\n            direction=str(flat_cfg.optuna.direction),\n            storage=f\"sqlite:///{study_path}\",\n            load_if_exists=True,\n        )\n\n        def objective(trial: optuna.Trial):\n            trial_cfg = OmegaConf.create(OmegaConf.to_container(flat_cfg, resolve=True))\n            _apply_optuna_suggestions(trial_cfg, trial, param_map)\n            trial_cfg.wandb.mode = \"disabled\"  # disable WandB inside optimisation trials\n            trial_cfg.optuna.max_epochs = flat_cfg.optuna.get(\"max_epochs\", 3)\n            metrics = _train_eval(trial_cfg, trial=trial, verbose=False)\n            return metrics[\"best_val_acc\"]\n\n        study.optimize(objective, n_trials=int(flat_cfg.optuna.n_trials), gc_after_trial=True)\n        print(f\"Optuna best params: {study.best_params}\")\n\n        # Update flattened cfg with best params for final training ----------\n        for key, val in study.best_params.items():\n            if key == \"adapter_reduction_factor\":\n                flat_cfg.model.task_adapters.reduction_factor = val\n            elif key in {\"learning_rate\", \"batch_size\", \"weight_decay\"}:\n                flat_cfg.training[key] = val\n\n    # --------------------- Final full training ---------------------------\n    _train_eval(flat_cfg, trial=None, verbose=True)\n\n\nif __name__ == \"__main__\":\n    main()"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": true,
    "code_issue": ""
}
