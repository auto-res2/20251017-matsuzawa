
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training: `uv run python -u -m src.main run={run_id} results_dir={path}`
     * Evaluation: `uv run python -m src.evaluate results_dir={path}` (independent execution)
   - Supports trial_mode=true flag for lightweight validation runs (automatically disables WandB)

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics using `wandb.log()`
   - trial_mode automatically disables WandB (sets wandb.mode=disabled)
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/run/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path}`
   - main.py DOES NOT call evaluate.py
   - evaluate.py retrieves ALL data from WandB API using `wandb.Api()` (not from local files)
   - evaluate.py exports retrieved WandB data to `{results_dir}/wandb_data/` for reproducibility
   - evaluate.py generates ALL publication-quality PDF figures and saves to `{results_dir}/images/`
   - Proper figure quality: legends, annotations, tight_layout
   - Follows naming convention: `<figure_topic>[_<condition>][_pairN].pdf`
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Trial Mode Implementation**:
   - trial_mode=true flag properly reduces computational load
   - Training: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
   - WandB automatically disabled in trial_mode (wandb.mode=disabled)
   - Purpose: Fast validation that code runs without errors

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare DistilBERT with task-specific adapters (proposed) against standard fine-tuning (comparative) across vision and language tasks to evaluate performance improvements.

# Experimental Design
- Strategy: Comparative analysis of DistilBERT performance across vision and language tasks
- Proposed Method: Fine-tuned DistilBERT with task-specific adapters
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time']

# Experiment Runs

- Run ID: proposed-DistilBERT-base-66M-CIFAR-10
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-CIFAR-10
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 1e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: proposed-DistilBERT-base-66M-alpaca-cleaned
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-alpaca-cleaned
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 5e-5
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-CIFAR-10
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-CIFAR-10
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-alpaca-cleaned
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - run: proposed-DistilBERT-base-66M-CIFAR-10\n  - _self_\n\nwandb:\n  entity: gengaru617-personal\n  project: 251017-test\n  mode: online  # overridden to \"disabled\" when trial_mode=true\n\n# Trial mode flag (set via CLI)\ntrial_mode: false\n\n# Path to store results and artefacts\nresults_dir: outputs/\n\nhydra:\n  run:\n    dir: .\n  output_subdir: null\n", "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\n\nFIGURE_DIR = \"images\"\nDATA_DIR = \"wandb_data\"\n\n\n# ---------------------------------------------------------------------------\n# WandB helpers\n# ---------------------------------------------------------------------------\n\ndef _fetch_runs(entity: str, project: str):\n    api = wandb.Api()\n    return api.runs(f\"{entity}/{project}\")\n\n\ndef _export_metrics(runs, out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    summaries: Dict[str, Dict] = {}\n\n    needed_cols = [\n        \"epoch\",\n        \"best_val_acc\",\n        \"best_val_f1\",\n        \"train_loss\",\n        \"val_loss\",\n        \"val_acc\",\n        \"val_f1\",\n    ]\n\n    for run in runs:\n        df = run.history(keys=needed_cols, pandas=True)\n        df.to_csv(out_dir / f\"run_{run.name}_metrics.csv\", index=False)\n        summaries[run.name] = {\n            \"best_val_acc\": df[\"best_val_acc\"].dropna().max() if \"best_val_acc\" in df else None,\n            \"best_val_f1\": df[\"best_val_f1\"].dropna().max() if \"best_val_f1\" in df else None,\n        }\n    with open(out_dir / \"summary.json\", \"w\") as f:\n        json.dump(summaries, f, indent=2)\n    return summaries\n\n\n# ---------------------------------------------------------------------------\n# Plotting\n# ---------------------------------------------------------------------------\n\ndef _plot_learning_curves(runs, dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    for run in runs:\n        df = run.history(keys=[\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"], pandas=True)\n        if df.empty:\n            continue\n        # Loss curve\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_loss\"], label=\"Train Loss\")\n        ax.plot(df[\"epoch\"], df[\"val_loss\"], label=\"Val Loss\")\n        ax.set_title(f\"Loss Curve - {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"loss_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n        # Accuracy curve\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_acc\"], label=\"Train Acc\")\n        ax.plot(df[\"epoch\"], df[\"val_acc\"], label=\"Val Acc\")\n        ax.set_title(f\"Accuracy Curve - {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"acc_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n\ndef _plot_comparison(summary: Dict[str, Dict], dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    df = pd.DataFrame.from_dict(summary, orient=\"index\")\n    df = df.reset_index().rename(columns={\"index\": \"run_id\"})\n\n    if df[\"best_val_acc\"].isnull().all():\n        return  # Nothing to plot\n\n    fig, ax = plt.subplots(figsize=(max(6, len(df) * 0.75), 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"best_val_acc\", ax=ax, palette=\"viridis\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_title(\"Best Validation Accuracy Across Runs\")\n    for p in ax.patches:\n        ax.annotate(\n            f\"{p.get_height():.2f}\",\n            (p.get_x() + p.get_width() / 2.0, p.get_height()),\n            ha=\"center\",\n            va=\"center\",\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )\n    fig.tight_layout()\n    fname = dest / \"comparison_best_val_acc.pdf\"\n    fig.savefig(fname)\n    print(fname.name)\n    plt.close(fig)\n\n\n# ---------------------------------------------------------------------------\n# Improvement metrics (proposed vs baseline)\n# ---------------------------------------------------------------------------\n\ndef _compute_improvement(summary: Dict[str, Dict]):\n    \"\"\"Computes improvement rate (proposed - baseline) / baseline per dataset.\n    Assumes run_id pattern contains dataset name and method (proposed / comparative).\"\"\"\n    records: Dict[str, Dict[str, float]] = {}\n    for run_id, metrics in summary.items():\n        dataset_name = run_id.split(\"-\")[-1]\n        method = \"proposed\" if run_id.startswith(\"proposed\") else \"baseline\"\n        records.setdefault(dataset_name, {})[method] = metrics[\"best_val_acc\"]\n\n    improvement = {\n        ds: {\n            \"baseline\": vals.get(\"baseline\"),\n            \"proposed\": vals.get(\"proposed\"),\n            \"improvement_rate\": (vals.get(\"proposed\") - vals.get(\"baseline\")) / vals.get(\"baseline\") if vals.get(\"baseline\") else None,\n        }\n        for ds, vals in records.items()\n    }\n    return improvement\n\n\n# ---------------------------------------------------------------------------\n# entrypoint\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate WandB experiments and generate figures.\")\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory containing experiment metadata\")\n    args = parser.parse_args()\n\n    results_path = Path(args.results_dir).absolute()\n    cfg_path = results_path / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"Config file not found at {cfg_path}\")\n\n    import yaml\n\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    entity = cfg[\"wandb\"][\"entity\"]\n    project = cfg[\"wandb\"][\"project\"]\n\n    runs = _fetch_runs(entity, project)\n    data_dir = results_path / DATA_DIR\n    summary = _export_metrics(runs, data_dir)\n\n    fig_dir = results_path / FIGURE_DIR\n    _plot_learning_curves(runs, fig_dir)\n    _plot_comparison(summary, fig_dir)\n\n    improvement = _compute_improvement(summary)\n    with open(results_path / DATA_DIR / \"improvement.json\", \"w\") as f:\n        json.dump(improvement, f, indent=2)\n", "main_py": "import subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom hydra.utils import get_original_cli_args\n\n\n@hydra.main(config_path=\"../config\")\ndef main(_cfg):\n    \"\"\"Main orchestrator \u2013 simply forwards all CLI overrides to src.train in a subprocess.\"\"\"\n    # Build command for subprocess; reuse exactly the CLI overrides provided originally.\n    original_cli: List[str] = get_original_cli_args()\n    # Remove the module specification (first element is run=..., etc.)\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + original_cli\n\n    print(\"Executing subprocess:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "import torch\nfrom torch import nn\nfrom transformers import DistilBertModel, AutoTokenizer\n\n\n# ---------------------------------------------------------------------------\n# Blocks\n# ---------------------------------------------------------------------------\n\nclass HoulsbyAdapter(nn.Module):\n    \"\"\"Implementation of the Houlsby adapter block.\"\"\"\n\n    def __init__(self, hidden_dim: int, reduction_factor: int = 16, non_linearity: str = \"relu\"):\n        super().__init__()\n        bottleneck_dim = max(1, hidden_dim // reduction_factor)\n        self.down = nn.Linear(hidden_dim, bottleneck_dim)\n        self.nonlin = getattr(nn, non_linearity.capitalize())() if hasattr(nn, non_linearity.capitalize()) else nn.ReLU()\n        self.up = nn.Linear(bottleneck_dim, hidden_dim)\n\n    def forward(self, x):  # x: (B, D)\n        return self.up(self.nonlin(self.down(x)))\n\n\nclass VisionPatchEmbedding(nn.Module):\n    \"\"\"Tiny patch embedding layer turning an image into a sequence of patch vectors.\"\"\"\n\n    def __init__(self, in_channels: int, patch_size: int, embed_dim: int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Linear(in_channels * patch_size * patch_size, embed_dim)\n\n    def forward(self, images):  # images: (B, C, H, W)\n        B, C, H, W = images.shape\n        p = self.patch_size\n        assert H % p == 0 and W % p == 0, \"Image size must be divisible by patch size\"\n        # unfold =\u003e B, C, n_patches_h, n_patches_w, p, p\n        patches = images.unfold(2, p, p).unfold(3, p, p)\n        patches = patches.contiguous().view(B, C, -1, p, p)\n        patches = patches.permute(0, 2, 1, 3, 4).contiguous()  # B, N, C, p, p\n        patches = patches.view(B, patches.size(1), -1)  # B, N, C*p*p\n        return self.proj(patches)  # B, N, D\n\n\n# ---------------------------------------------------------------------------\n# DistilBERT classifier (multi-modal)\n# ---------------------------------------------------------------------------\n\nclass DistilBertClassifier(nn.Module):\n    def __init__(\n        self,\n        model_name: str,\n        num_labels: int,\n        adapter_cfg=None,\n        vision_patch_cfg=None,\n    ):\n        super().__init__()\n        self.is_vision = vision_patch_cfg is not None\n        self.bert = DistilBertModel.from_pretrained(model_name, cache_dir=\".cache/\")\n        self.hidden_dim = self.bert.config.hidden_size\n\n        # Vision embedding when images are used\n        if self.is_vision:\n            self.vision_embedding = VisionPatchEmbedding(\n                in_channels=3,\n                patch_size=vision_patch_cfg.patch_size,\n                embed_dim=self.hidden_dim,\n            )\n        else:\n            self.vision_embedding = None\n\n        # Optional adapter\n        if adapter_cfg and adapter_cfg.get(\"enabled\", False):\n            self.adapter = HoulsbyAdapter(\n                hidden_dim=self.hidden_dim,\n                reduction_factor=adapter_cfg.reduction_factor,\n                non_linearity=adapter_cfg.non_linearity,\n            )\n        else:\n            self.adapter = None\n\n        # Classification head\n        self.pre_classifier = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.activation = nn.GELU()\n        self.dropout = nn.Dropout(0.2)\n        self.classifier = nn.Linear(self.hidden_dim, num_labels)\n\n    def forward(self, batch):\n        if self.is_vision and \"pixel_values\" in batch:\n            embeddings = self.vision_embedding(batch[\"pixel_values\"])  # (B, N, D)\n            attention_mask = torch.ones(embeddings.size()[:-1], dtype=torch.long, device=embeddings.device)\n            out = self.bert(inputs_embeds=embeddings, attention_mask=attention_mask)\n        else:\n            out = self.bert(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n            )\n        pooled = out.last_hidden_state[:, 0]  # first token / CLS alike\n\n        # Adapter residual\n        if self.adapter is not None:\n            pooled = pooled + self.adapter(pooled)\n\n        x = self.pre_classifier(pooled)\n        x = self.activation(x)\n        x = self.dropout(x)\n        logits = self.classifier(x)\n        return logits\n\n\n# ---------------------------------------------------------------------------\n# Builder\n# ---------------------------------------------------------------------------\n\ndef build_model(cfg, tokenizer=None):\n    vision_patch_cfg = (\n        cfg.model.vision_patch if cfg.model.get(\"input_representation\", \"text\") == \"image_sequence\" else None\n    )\n    adapter_cfg = cfg.model.task_adapters if cfg.method == \"proposed\" else None\n    model = DistilBertClassifier(\n        model_name=cfg.model.name,\n        num_labels=cfg.model.num_labels,\n        adapter_cfg=adapter_cfg,\n        vision_patch_cfg=vision_patch_cfg,\n    )\n    return model\n", "preprocess_py": "\"\"\"Complete preprocessing utilities for all datasets used in the study.\"\"\"\n\nfrom typing import Tuple, Optional, List\nimport random\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import datasets as tv_datasets, transforms\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n\n# ---------------------------------------------------------------------------\n# CIFAR-10 (vision) helpers\n# ---------------------------------------------------------------------------\n\n\nclass _CIFARWrapper(torch.utils.data.Dataset):\n    \"\"\"Wraps torchvision CIFAR instances returning dicts the model expects.\"\"\"\n\n    def __init__(self, base, tfm):\n        self.base = base\n        self.tfm = tfm\n\n    def __len__(self):\n        return len(self.base)\n\n    def __getitem__(self, idx):\n        img, label = self.base[idx]\n        img = self.tfm(img)\n        return {\n            \"pixel_values\": img,  # Tensor C\u00d7H\u00d7W\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\ndef _split(dataset: torch.utils.data.Dataset, splits: Tuple[float, float, float]):\n    n_total = len(dataset)\n    lengths = [int(p * n_total) for p in splits]\n    lengths[-1] = n_total - sum(lengths[:-1])  # adjust rounding\n    return random_split(dataset, lengths, generator=torch.Generator().manual_seed(42))\n\n\ndef _build_cifar10(cfg, trial_mode: bool):\n    resize_size = cfg.dataset.preprocessing.resize\n    augmentations: List[str] = cfg.dataset.preprocessing.augmentations\n\n    tfm_train = [\n        transforms.RandomCrop(resize_size, padding=4) if \"random_crop\" in augmentations else transforms.Resize(resize_size),\n        transforms.RandomHorizontalFlip() if \"horizontal_flip\" in augmentations else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ]\n    tfm_eval = [transforms.Resize(resize_size), transforms.ToTensor()]\n\n    train_dataset = tv_datasets.CIFAR10(root=\".cache/\", train=True, download=True)\n    test_dataset = tv_datasets.CIFAR10(root=\".cache/\", train=False, download=True)\n    full = ConcatDataset([train_dataset, test_dataset])\n    train_set, val_set, test_set = _split(full, (\n        cfg.dataset.split.train,\n        cfg.dataset.split.val,\n        cfg.dataset.split.test,\n    ))\n\n    train_set = _CIFARWrapper(train_set, transforms.Compose(tfm_train))\n    val_set = _CIFARWrapper(val_set, transforms.Compose(tfm_eval))\n    test_set = _CIFARWrapper(test_set, transforms.Compose(tfm_eval))\n\n    batch_size = 2 if trial_mode else cfg.training.batch_size\n    dl_kwargs = dict(batch_size=batch_size, num_workers=2, pin_memory=True)\n\n    return (\n        DataLoader(train_set, shuffle=True, **dl_kwargs),\n        DataLoader(val_set, shuffle=False, **dl_kwargs),\n        DataLoader(test_set, shuffle=False, **dl_kwargs),\n        None,  # tokenizer not used\n    )\n\n\n# ---------------------------------------------------------------------------\n# Alpaca-cleaned (text) helpers\n# ---------------------------------------------------------------------------\n\n\ndef _label_from_instruction(instr: str) -\u003e int:\n    \"\"\"Binary label: 1 if instruction requests translation else 0.\"\"\"\n    lowered = instr.lower()\n    keywords = [\"translate\", \"translation\", \"translating\", \"translator\"]\n    return int(any(k in lowered for k in keywords))\n\n\ndef _encode_text(batch, tokenizer, max_len):\n    instructions = batch[\"instruction\"]\n    inputs = batch.get(\"input\", [\"\"] * len(instructions))\n    texts = [f\"Instruction: {ins} Input: {inp}\" for ins, inp in zip(instructions, inputs)]\n    encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n    encodings[\"labels\"] = [_label_from_instruction(ins) for ins in instructions]\n    return encodings\n\n\ndef _build_alpaca(cfg, trial_mode: bool):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    raw_ds = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", cache_dir=\".cache/\")\n\n    encoded = raw_ds.map(\n        lambda batch: _encode_text(batch, tokenizer, cfg.dataset.max_length),\n        batched=True,\n        remove_columns=raw_ds.column_names,\n    )\n    encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    train_size = int(cfg.dataset.split.train * len(encoded))\n    val_size = int(cfg.dataset.split.val * len(encoded))\n    test_size = len(encoded) - train_size - val_size\n    train_ds, val_ds, test_ds = random_split(\n        encoded,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42),\n    )\n\n    batch_size = 2 if trial_mode else cfg.training.batch_size\n    dl_kwargs = dict(batch_size=batch_size, num_workers=2, pin_memory=True)\n\n    return (\n        DataLoader(train_ds, shuffle=True, **dl_kwargs),\n        DataLoader(val_ds, shuffle=False, **dl_kwargs),\n        DataLoader(test_ds, shuffle=False, **dl_kwargs),\n        tokenizer,\n    )\n\n\n# ---------------------------------------------------------------------------\n# Public factory\n# ---------------------------------------------------------------------------\n\ndef build_dataloaders(cfg, trial_mode: bool = False):\n    name = cfg.dataset.name.lower()\n    if name == \"cifar10\":\n        return _build_cifar10(cfg, trial_mode)\n    elif name == \"alpaca-cleaned\":\n        return _build_alpaca(cfg, trial_mode)\n    else:\n        raise ValueError(f\"Unsupported dataset {name}\")\n", "pyproject_toml": "[project]\nname = \"distilbert-adapter-exp\"\nversion = \"0.1.0\"\ndescription = \"Experiment code for DistilBERT adapters across tasks with Hydra, WandB and Optuna\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntransformers = \"\u003e=4.35.0\"\ndatasets = \"\u003e=2.14.0\"\ntorchvision = \"\u003e=0.15.0\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.15.0\"\nscikit-learn = \"^1.3.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\noptuna = \"^3.3.0\"\npandas = \"^2.1.0\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "import os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Tuple, Optional\n\nimport hydra\nimport optuna\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import OmegaConf\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import get_linear_schedule_with_warmup\nimport wandb\n\n# Local imports (absolute because executed with -m from repo root)\nfrom src.preprocess import build_dataloaders\nfrom src.model import build_model\n\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef _set_seed(seed: int = 42):\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _save_cfg(cfg, results_dir: Path):\n    results_dir.mkdir(parents=True, exist_ok=True)\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        OmegaConf.save(config=cfg, f=str(cfg_path))\n\n\n# ---------------------------------------------------------------------------\n# Core training / evaluation routine (single pass)\n# ---------------------------------------------------------------------------\n\ndef _train_eval(cfg, trial: Optional[optuna.Trial] = None, verbose: bool = True) -\u003e Dict[str, Any]:\n    \"\"\"Train for cfg.training.epochs (or cfg.training.optuna_epochs if supplied when called from Optuna).\n\n    When called inside Optuna, trial is not None; in that case WandB is disabled, and\n    we can optionally shorten training via cfg.optuna.max_epochs.\n    Returns a dict with at least keys \"val_acc\" and \"val_f1\".\n    \"\"\"\n    trial_mode: bool = cfg.get(\"trial_mode\", False)\n\n    # ---------------------------------------------------------------------\n    # Dataloaders / Model\n    # ---------------------------------------------------------------------\n    train_loader, val_loader, _, tokenizer = build_dataloaders(cfg, trial_mode=trial_mode)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = build_model(cfg, tokenizer)\n    model.to(device)\n\n    # ---------------------------------------------------------------------\n    # Optimiser + scheduler\n    # ---------------------------------------------------------------------\n    optim_cls = {\n        \"adamw\": torch.optim.AdamW,\n        \"sgd\": torch.optim.SGD,\n    }.get(cfg.training.optimizer.lower())\n    if optim_cls is None:\n        raise ValueError(f\"Unsupported optimizer {cfg.training.optimizer}\")\n\n    optimizer = optim_cls(\n        model.parameters(),\n        lr=cfg.training.learning_rate,\n        weight_decay=cfg.training.weight_decay,\n    )\n    total_steps = len(train_loader) * cfg.training.epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.training.warmup_steps,\n        num_training_steps=total_steps,\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    # ---------------------------------------------------------------------\n    # WandB (only outside Optuna trials and not in trial_mode when disabled)\n    # ---------------------------------------------------------------------\n    use_wandb = (trial is None) and (cfg.wandb.mode != \"disabled\") and (not trial_mode)\n    if use_wandb:\n        run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n        )\n        if verbose:\n            print(f\"WandB URL: {wandb.run.get_url()}\")\n    else:\n        run = None\n\n    # ---------------------------------------------------------------------\n    # Training loop\n    # ---------------------------------------------------------------------\n    epochs = (\n        1 if trial_mode else cfg.training.epochs\n        if trial is None else cfg.optuna.get(\"max_epochs\", 3)\n    )\n    global_step = 0\n    best_val_acc = 0.0\n    best_val_f1 = 0.0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_losses: List[float] = []\n        epoch_true, epoch_pred = [], []\n\n        for step, batch in enumerate(train_loader):\n            # In trial_mode keep it lightweight\n            if trial_mode and step \u003e 1:\n                break\n\n            labels = batch.pop(\"labels\").to(device)\n            for k in batch:\n                batch[k] = batch[k].to(device)\n            outputs = model(batch)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n            epoch_losses.append(loss.item())\n            pred_labels = outputs.argmax(dim=-1).detach().cpu().tolist()\n            epoch_pred.extend(pred_labels)\n            epoch_true.extend(labels.detach().cpu().tolist())\n            global_step += 1\n\n        train_loss = float(sum(epoch_losses) / max(1, len(epoch_losses)))\n        train_acc = accuracy_score(epoch_true, epoch_pred)\n        train_f1 = f1_score(epoch_true, epoch_pred, average=\"weighted\")\n\n        # -------------------------- Validation ---------------------------\n        model.eval()\n        val_losses: List[float] = []\n        val_true, val_pred = [], []\n        start_inf = time.time()\n        with torch.no_grad():\n            for step, batch in enumerate(val_loader):\n                if trial_mode and step \u003e 1:\n                    break\n                labels = batch.pop(\"labels\").to(device)\n                for k in batch:\n                    batch[k] = batch[k].to(device)\n                outputs = model(batch)\n                loss = criterion(outputs, labels)\n                val_losses.append(loss.item())\n                preds = outputs.argmax(dim=-1).detach().cpu().tolist()\n                val_pred.extend(preds)\n                val_true.extend(labels.detach().cpu().tolist())\n        inference_time = (time.time() - start_inf) / max(1, len(val_loader))\n        val_loss = float(sum(val_losses) / max(1, len(val_losses)))\n        val_acc = accuracy_score(val_true, val_pred)\n        val_f1 = f1_score(val_true, val_pred, average=\"weighted\")\n\n        best_val_acc = max(best_val_acc, val_acc)\n        best_val_f1 = max(best_val_f1, val_f1)\n\n        log_dict = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"train_acc\": train_acc,\n            \"val_acc\": val_acc,\n            \"train_f1\": train_f1,\n            \"val_f1\": val_f1,\n            \"best_val_acc\": best_val_acc,\n            \"best_val_f1\": best_val_f1,\n            \"inference_time\": inference_time,\n        }\n        if use_wandb:\n            wandb.log(log_dict, step=epoch)\n        if verbose:\n            print(f\"Epoch {epoch}: {log_dict}\")\n\n    if run is not None:\n        wandb.finish()\n\n    return {\n        \"best_val_acc\": best_val_acc,\n        \"best_val_f1\": best_val_f1,\n    }\n\n\n# ---------------------------------------------------------------------------\n# Optuna integration\n# ---------------------------------------------------------------------------\n\ndef _suggest_and_apply(cfg, trial: optuna.Trial, param_map: Dict[str, List[str]]):\n    \"\"\"Apply Optuna suggestions to cfg in-place according to search_space and param_map.\"\"\"\n    for param_name, space in cfg.optuna.search_space.items():\n        # Determine value\n        if space.type == \"loguniform\":\n            val = trial.suggest_float(param_name, space.low, space.high, log=True)\n        elif space.type == \"uniform\":\n            val = trial.suggest_float(param_name, space.low, space.high, log=False)\n        elif space.type == \"categorical\":\n            val = trial.suggest_categorical(param_name, space.choices)\n        elif space.type == \"int\":\n            val = trial.suggest_int(param_name, space.low, space.high, step=space.get(\"step\", 1))\n        else:\n            raise ValueError(f\"Unsupported search space type: {space.type}\")\n\n        # Apply value into cfg following param_map\n        if param_name not in param_map:\n            raise KeyError(f\"Param {param_name} not present in param_map for config update\")\n        ptr = cfg\n        keys = param_map[param_name]\n        for k in keys[:-1]:\n            ptr = ptr[k]\n        ptr[keys[-1]] = val\n\n\n# ---------------------------------------------------------------------------\n# main entrypoint\n# ---------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):  # noqa: C901  (function is intentionally long for clarity)\n    _set_seed()\n\n    trial_mode: bool = cfg.get(\"trial_mode\", False)\n    results_dir = Path(cfg.results_dir)\n    _save_cfg(cfg, results_dir)\n\n    # If Optuna requested and not disabled via n_trials = 0 or trial_mode, run optimisation.\n    if (cfg.optuna.n_trials \u003e 0) and (not trial_mode):\n        param_map = {\n            \"learning_rate\": [\"training\", \"learning_rate\"],\n            \"batch_size\": [\"training\", \"batch_size\"],\n            \"weight_decay\": [\"training\", \"weight_decay\"],\n            \"adapter_reduction_factor\": [\"model\", \"task_adapters\", \"reduction_factor\"],\n        }\n\n        study_path = results_dir / \"optuna_study.db\"\n        storage_str = f\"sqlite:///{study_path}\"\n        study = optuna.create_study(\n            direction=cfg.optuna.direction,\n            storage=storage_str,\n            study_name=cfg.run.run_id,\n            load_if_exists=True,\n        )\n\n        def objective(trial: optuna.Trial):\n            # Work on a deep copy of cfg to avoid polluting it across trials\n            trial_cfg = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n            _suggest_and_apply(trial_cfg, trial, param_map)\n            # Disable WandB inside objective to keep it lightweight\n            trial_cfg.wandb.mode = \"disabled\"\n            # Reduce epochs if user specified optuna.max_epochs\n            trial_cfg.optuna.max_epochs = cfg.optuna.get(\"max_epochs\", 3)\n            metrics = _train_eval(trial_cfg, trial=trial, verbose=False)\n            # We optimise best_val_acc by default\n            return metrics[\"best_val_acc\"]\n\n        study.optimize(objective, n_trials=cfg.optuna.n_trials, gc_after_trial=True)\n        best_params = study.best_params\n        # Apply best params to cfg for final full training\n        for k, v in best_params.items():\n            if k == \"learning_rate\":\n                cfg.training.learning_rate = v\n            elif k == \"batch_size\":\n                cfg.training.batch_size = v\n            elif k == \"weight_decay\":\n                cfg.training.weight_decay = v\n            elif k == \"adapter_reduction_factor\":\n                cfg.model.task_adapters.reduction_factor = v\n        # Persist study for inspection\n        study_path.write_text(str(study.best_trial))\n        print(f\"Optuna best params: {best_params}\")\n\n    # ---------------- Final full training \u0026 logging -----------------------\n    _train_eval(cfg, trial=None, verbose=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "The package is close, but the following mandatory requirements are not satisfied:\n\n1. Evaluation-script CLI mismatch (Core criterion 2 & 8).\n   • Specified interface:  `uv run python -m src.evaluate results_dir={path}`  (key-value arg).\n   • Provided evaluate.py expects a positional argument (e.g. `python -m src.evaluate outputs/`).  The required call will pass the literal string `results_dir=/...` and the script will fail.  Evaluate.py must accept the `results_dir` key-value override exactly as defined.\n\n2. Trial-mode WandB disabling (Core criterion 6 & 9).\n   • Requirement: trial_mode=true must *set* `wandb.mode = \"disabled\"` automatically.\n   • Current implementation only suppresses wandb.init via a boolean flag, the config value itself is left unchanged.  The code must explicitly overwrite `cfg.wandb.mode` (or equivalent) so that downstream logic that relies on the field can safely assume it is set to \"disabled\".\n\nBecause these mandatory interface and configuration requirements are not fully met, the code is **not ready for immediate execution** in the described experiments."
}
