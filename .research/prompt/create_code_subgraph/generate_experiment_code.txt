
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path}

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} trial_mode=true
```
- `run`: Experiment run_id (matching a run_id from config/run/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `trial_mode=true` (optional): Lightweight execution for validation (epochs=1, batches limited to 1-2, disable Optuna n_trials=0, **WandB disabled**)

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path}
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * Log ALL metrics to WandB: `wandb.log({"train_loss": 0.5, "val_acc": 0.85, "epoch": 1, ...})`
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path}`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse `results_dir` from command line arguments
  * Load WandB config from `{results_dir}/config.yaml`
  * Retrieve all experimental data from WandB API:
    ```python
    api = wandb.Api()
    runs = api.runs(f"{entity}/{project}")
    for run in runs:
        metrics_df = run.history()  # pandas DataFrame with all logged metrics
    ```
  * Export retrieved data to:
    - `{results_dir}/wandb_data/run_{run_id}_metrics.csv` (per-run metrics)
    - `{results_dir}/wandb_data/summary.json` (aggregated comparison)
  * Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
  * Generate ALL publication-quality PDF figures and save to `{results_dir}/images/`:
    - Learning curves (loss, accuracy over epochs)
    - Cross-run comparison charts (bar charts, box plots)
    - Performance metrics tables
  * Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
  * Follow naming convention: `<figure_topic>[_<condition>][_pairN].pdf`
  * Print generated figure names to stdout

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- Pass all Hydra overrides to train.py subprocess (e.g., `wandb.mode=disabled`, `trial_mode=true`)
- In trial_mode, automatically set `wandb.mode=disabled`

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 251017-test
    mode: online  # Automatically set to "disabled" in trial_mode
  ```

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Trial Mode Behavior**: trial_mode=true automatically disables WandB (sets wandb.mode=disabled)
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: src/evaluate.py is incomplete/inconsistent – _export_metrics() does NOT request or save the key `train_acc`, yet _plot_learning_curves() unconditionally accesses `df['train_acc']` (and `df['train_acc']` will be missing), causing a KeyError and making the evaluation script crash.  Because evaluation must run independently and generate all figures, this bug violates requirement #8 (Evaluation Script Independence).  Fix: include 'train_acc' when fetching WandB history or guard against its absence (and likewise ensure all columns referenced in plotting functions are collected). No other blocking issues were found, but this error prevents immediate execution.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "defaults:\n  - run: proposed-DistilBERT-base-66M-CIFAR-10  # default run, override via CLI\n  - _self_\n\nwandb:\n  entity: gengaru617-personal\n  project: 251017-test\n  mode: online  # will be overridden to \"disabled\" automatically in trial_mode\n\ntrial_mode: false  # enable via CLI for CI validation\n\nresults_dir: outputs/\n\nhydra:\n  run:\n    dir: .\n  output_subdir: null\n", "evaluate_py": "\"\"\"Independent evaluation \u0026 figure generation for the collected WandB runs.\"\"\"\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Dict\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\n\nFIGURE_DIR = \"images\"\nDATA_DIR = \"wandb_data\"\n\n# ---------------------------------------------------------------------------\n# CLI helper\n# ---------------------------------------------------------------------------\n\ndef _parse_results_dir(argv) -\u003e Path:\n    \"\"\"Extract results_dir either from key=value or first positional argument.\"\"\"\n    for arg in argv[1:]:\n        if arg.startswith(\"results_dir=\"):\n            return Path(arg.split(\"=\", 1)[1]).expanduser().absolute()\n    if len(argv) \u003e 1:\n        return Path(argv[1]).expanduser().absolute()\n    raise ValueError(\"results_dir argument is required (e.g., results_dir=outputs/)\")\n\n\n# ---------------------------------------------------------------------------\n# WandB helpers\n# ---------------------------------------------------------------------------\n\ndef _fetch_runs(entity: str, project: str):\n    api = wandb.Api()\n    return api.runs(f\"{entity}/{project}\")\n\n\ndef _export_metrics(runs, out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    summaries: Dict[str, Dict] = {}\n    needed_cols = [\n        \"epoch\",\n        \"best_val_acc\",\n        \"best_val_f1\",\n        \"train_loss\",\n        \"val_loss\",\n        \"val_acc\",\n        \"val_f1\",\n    ]\n    for run in runs:\n        df = run.history(keys=needed_cols, pandas=True)\n        df.to_csv(out_dir / f\"run_{run.name}_metrics.csv\", index=False)\n        summaries[run.name] = {\n            \"best_val_acc\": df[\"best_val_acc\"].dropna().max() if \"best_val_acc\" in df else None,\n            \"best_val_f1\": df[\"best_val_f1\"].dropna().max() if \"best_val_f1\" in df else None,\n        }\n    with open(out_dir / \"summary.json\", \"w\") as f:\n        json.dump(summaries, f, indent=2)\n    return summaries\n\n\n# ---------------------------------------------------------------------------\n# Plotting helpers\n# ---------------------------------------------------------------------------\n\ndef _plot_learning_curves(runs, dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    for run in runs:\n        df = run.history(keys=[\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"], pandas=True)\n        if df.empty:\n            continue\n\n        # Loss curve -------------------------------------------------------\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_loss\"], label=\"Train Loss\")\n        ax.plot(df[\"epoch\"], df[\"val_loss\"], label=\"Val Loss\")\n        ax.set_title(f\"Loss Curve \u2013 {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"loss_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n        # Accuracy curve ---------------------------------------------------\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_acc\"], label=\"Train Acc\")\n        ax.plot(df[\"epoch\"], df[\"val_acc\"], label=\"Val Acc\")\n        ax.set_title(f\"Accuracy Curve \u2013 {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"acc_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n\ndef _plot_comparison(summary: Dict[str, Dict], dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    df = pd.DataFrame.from_dict(summary, orient=\"index\").reset_index().rename(columns={\"index\": \"run_id\"})\n    if df[\"best_val_acc\"].isnull().all():\n        return\n    fig, ax = plt.subplots(figsize=(max(6, len(df) * 0.75), 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"best_val_acc\", ax=ax, palette=\"viridis\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_title(\"Best Validation Accuracy Across Runs\")\n    for p in ax.patches:\n        ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                    ha=\"center\", va=\"center\", xytext=(0, 5), textcoords=\"offset points\")\n    fig.tight_layout()\n    fname = dest / \"comparison_best_val_acc.pdf\"\n    fig.savefig(fname)\n    print(fname.name)\n    plt.close(fig)\n\n\n# ---------------------------------------------------------------------------\n# Improvement computation\n# ---------------------------------------------------------------------------\n\ndef _compute_improvement(summary: Dict[str, Dict]):\n    records: Dict[str, Dict[str, float]] = {}\n    for run_id, metrics in summary.items():\n        # heuristic: dataset name is last token after last \u0027-\u0027\n        dataset_name = run_id.split(\"-\")[-1]\n        method = \"proposed\" if run_id.startswith(\"proposed\") else \"baseline\"\n        records.setdefault(dataset_name, {})[method] = metrics[\"best_val_acc\"]\n\n    improvement = {\n        ds: {\n            **vals,\n            \"improvement_rate\": ((vals.get(\"proposed\") - vals.get(\"baseline\")) / vals.get(\"baseline\"))\n            if vals.get(\"baseline\") else None,\n        }\n        for ds, vals in records.items()\n    }\n    return improvement\n\n\n# ---------------------------------------------------------------------------\n# Entrypoint\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    results_dir = _parse_results_dir(sys.argv)\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"Config file not found at {cfg_path}\")\n\n    import yaml\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    entity = cfg[\"wandb\"][\"entity\"]\n    project = cfg[\"wandb\"][\"project\"]\n\n    runs = _fetch_runs(entity, project)\n    summary = _export_metrics(runs, results_dir / DATA_DIR)\n\n    _plot_learning_curves(runs, results_dir / FIGURE_DIR)\n    _plot_comparison(summary, results_dir / FIGURE_DIR)\n\n    improvement = _compute_improvement(summary)\n    with open(results_dir / DATA_DIR / \"improvement.json\", \"w\") as f:\n        json.dump(improvement, f, indent=2)\n", "main_py": "\"\"\"Experiment orchestrator: launches src.train as a subprocess with inherited Hydra overrides.\"\"\"\n\nimport subprocess\nimport sys\nfrom typing import List\n\nimport hydra\nfrom hydra.utils import get_original_cli_args\n\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):\n    \"\"\"Hydra entrypoint that forwards all CLI overrides to the training subprocess.\"\"\"\n    original_cli: List[str] = get_original_cli_args()\n\n    # Build subprocess command ------------------------------------------------\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + original_cli\n\n    # Force WandB disabled when trial_mode=true --------------------------------\n    if cfg.get(\"trial_mode\", False):\n        cmd.append(\"wandb.mode=disabled\")\n\n    print(\"Executing subprocess:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"Model architectures.\"\"\"\n\nimport torch\nfrom torch import nn\nfrom transformers import DistilBertModel\n\n# ---------------------------------------------------------------------------\n# Auxiliary modules\n# ---------------------------------------------------------------------------\n\nclass HoulsbyAdapter(nn.Module):\n    \"\"\"Implementation of Houlsby adapter (bottleneck).\"\"\"\n\n    def __init__(self, hidden_dim: int, reduction_factor: int = 16, non_linearity: str = \"relu\"):\n        super().__init__()\n        bottleneck = max(1, hidden_dim // reduction_factor)\n        self.down = nn.Linear(hidden_dim, bottleneck)\n        self.nonlin = getattr(nn, non_linearity.capitalize())() if hasattr(nn, non_linearity.capitalize()) else nn.ReLU()\n        self.up = nn.Linear(bottleneck, hidden_dim)\n\n    def forward(self, x):\n        return self.up(self.nonlin(self.down(x)))\n\n\nclass VisionPatchEmbedding(nn.Module):\n    \"\"\"Very small ViT-style patch embedding projecting image patches to hidden dim.\"\"\"\n\n    def __init__(self, in_channels: int, patch_size: int, embed_dim: int):\n        super().__init__()\n        self.patch_size = int(patch_size)\n        self.proj = nn.Linear(in_channels * self.patch_size * self.patch_size, embed_dim)\n\n    def forward(self, imgs):  # imgs: B,C,H,W\n        B, C, H, W = imgs.shape\n        p = self.patch_size\n        assert H % p == 0 and W % p == 0, \"Image dimensions must be divisible by patch size\"\n        patches = imgs.unfold(2, p, p).unfold(3, p, p)  # B,C,nH,nW,p,p\n        patches = patches.contiguous().view(B, C, -1, p, p)\n        patches = patches.permute(0, 2, 1, 3, 4).contiguous()  # B,N,C,p,p\n        patches = patches.view(B, patches.size(1), -1)  # B,N,C*p*p\n        return self.proj(patches)  # B, N, D\n\n# ---------------------------------------------------------------------------\n# DistilBERT classifier (text or vision)\n# ---------------------------------------------------------------------------\n\nclass DistilBertClassifier(nn.Module):\n    \"\"\"Adapter-ready DistilBERT classifier supporting vision patch inputs.\"\"\"\n\n    def __init__(self, model_name: str, num_labels: int, adapter_cfg=None, vision_patch_cfg=None):\n        super().__init__()\n        self.is_vision = vision_patch_cfg is not None\n        self.bert = DistilBertModel.from_pretrained(model_name, cache_dir=\".cache/\")\n        hidden = self.bert.config.hidden_size\n\n        if self.is_vision:\n            self.vision = VisionPatchEmbedding(3, vision_patch_cfg.patch_size, hidden)\n        else:\n            self.vision = None\n\n        if adapter_cfg and adapter_cfg.get(\"enabled\", False):\n            self.adapter = HoulsbyAdapter(\n                hidden_dim=hidden,\n                reduction_factor=int(adapter_cfg.reduction_factor),\n                non_linearity=str(adapter_cfg.non_linearity),\n            )\n        else:\n            self.adapter = None\n\n        self.pre_cls = nn.Linear(hidden, hidden)\n        self.act = nn.GELU()\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(hidden, num_labels)\n\n    def forward(self, batch):\n        if self.is_vision and \"pixel_values\" in batch:\n            embeds = self.vision(batch[\"pixel_values\"])  # B,N,D\n            mask = torch.ones(embeds.shape[:2], dtype=torch.long, device=embeds.device)\n            outputs = self.bert(inputs_embeds=embeds, attention_mask=mask)\n        else:\n            outputs = self.bert(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n            )\n\n        pooled = outputs.last_hidden_state[:, 0]  # CLS-token equivalent\n        if self.adapter is not None:\n            pooled = pooled + self.adapter(pooled)\n        x = self.pre_cls(pooled)\n        x = self.act(x)\n        x = self.dropout(x)\n        logits = self.cls(x)\n        return logits\n\n# ---------------------------------------------------------------------------\n# Factory\n# ---------------------------------------------------------------------------\n\ndef build_model(cfg, tokenizer=None):\n    \"\"\"Factory that builds the appropriate model according to cfg.\"\"\"\n    vision_patch_cfg = (\n        cfg.model.vision_patch if str(cfg.model.get(\"input_representation\", \"text\")) == \"image_sequence\" else None\n    )\n    method_tag = str(cfg.method).lower()\n    adapter_cfg = cfg.model.task_adapters if method_tag.startswith(\"proposed\") else None\n    return DistilBertClassifier(\n        model_name=cfg.model.name,\n        num_labels=int(cfg.model.num_labels),\n        adapter_cfg=adapter_cfg,\n        vision_patch_cfg=vision_patch_cfg,\n    )\n", "preprocess_py": "\"\"\"Dataset preprocessing and DataLoader construction.\"\"\"\nfrom typing import Tuple, List\nimport random\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import datasets as tv_datasets, transforms\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n# ----------------------------------------------------------------------------\n# CIFAR-10 helpers\n# ----------------------------------------------------------------------------\n\nclass _CIFARWrapper(torch.utils.data.Dataset):\n    \"\"\"Wrap torchvision dataset to return dicts compatible with model forward.\"\"\"\n\n    def __init__(self, base, tfm):\n        self.base = base\n        self.tfm = tfm\n\n    def __len__(self):\n        return len(self.base)\n\n    def __getitem__(self, idx):\n        img, label = self.base[idx]\n        img = self.tfm(img)\n        return {\"pixel_values\": img, \"labels\": torch.tensor(label, dtype=torch.long)}\n\n\ndef _split(dataset: torch.utils.data.Dataset, splits: Tuple[float, float, float]):\n    n_total = len(dataset)\n    lengths = [int(p * n_total) for p in splits]\n    lengths[-1] = n_total - sum(lengths[:-1])  # ensure all data used\n    return random_split(dataset, lengths, generator=torch.Generator().manual_seed(42))\n\n\ndef _build_cifar10(cfg, trial_mode: bool):\n    size = int(cfg.dataset.preprocessing.resize)\n    aug: List[str] = list(cfg.dataset.preprocessing.augmentations)\n\n    tfm_train = [\n        transforms.RandomCrop(size, padding=4) if \"random_crop\" in aug else transforms.Resize(size),\n        transforms.RandomHorizontalFlip() if \"horizontal_flip\" in aug else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ]\n    tfm_eval = [transforms.Resize(size), transforms.ToTensor()]\n\n    train_ds = tv_datasets.CIFAR10(root=\".cache/\", train=True, download=True)\n    test_ds = tv_datasets.CIFAR10(root=\".cache/\", train=False, download=True)\n    full_ds = ConcatDataset([train_ds, test_ds])\n    train_set, val_set, test_set = _split(full_ds, (\n        cfg.dataset.split.train,\n        cfg.dataset.split.val,\n        cfg.dataset.split.test,\n    ))\n\n    train_set = _CIFARWrapper(train_set, transforms.Compose(tfm_train))\n    val_set = _CIFARWrapper(val_set, transforms.Compose(tfm_eval))\n    test_set = _CIFARWrapper(test_set, transforms.Compose(tfm_eval))\n\n    bs = 2 if trial_mode else int(cfg.training.batch_size)\n    dl_kwargs = dict(batch_size=bs, num_workers=2, pin_memory=True)\n    return (\n        DataLoader(train_set, shuffle=True, **dl_kwargs),\n        DataLoader(val_set, shuffle=False, **dl_kwargs),\n        DataLoader(test_set, shuffle=False, **dl_kwargs),\n        None,\n    )\n\n# ----------------------------------------------------------------------------\n# Alpaca-cleaned helpers (binary classification: translation vs others)\n# ----------------------------------------------------------------------------\n\ndef _label_from_instruction(instr: str) -\u003e int:\n    lowered = instr.lower()\n    keywords = [\"translate\", \"translation\", \"translating\", \"translator\"]\n    return int(any(k in lowered for k in keywords))\n\n\ndef _encode_text(batch, tokenizer, max_len):\n    instructions = batch[\"instruction\"]\n    inputs = batch.get(\"input\", [\"\" for _ in range(len(instructions))])\n    texts = [f\"Instruction: {ins} Input: {inp}\" for ins, inp in zip(instructions, inputs)]\n    enc = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n    enc[\"labels\"] = [_label_from_instruction(ins) for ins in instructions]\n    return enc\n\n\ndef _build_alpaca(cfg, trial_mode: bool):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    raw = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", cache_dir=\".cache/\")\n\n    encoded = raw.map(\n        lambda b: _encode_text(b, tokenizer, cfg.dataset.max_length),\n        batched=True,\n        remove_columns=raw.column_names,\n    )\n    encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    total = len(encoded)\n    train_size = int(cfg.dataset.split.train * total)\n    val_size = int(cfg.dataset.split.val * total)\n    test_size = total - train_size - val_size\n    train_ds, val_ds, test_ds = random_split(\n        encoded,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42),\n    )\n\n    bs = 2 if trial_mode else int(cfg.training.batch_size)\n    dl_kwargs = dict(batch_size=bs, num_workers=2, pin_memory=True)\n    return (\n        DataLoader(train_ds, shuffle=True, **dl_kwargs),\n        DataLoader(val_ds, shuffle=False, **dl_kwargs),\n        DataLoader(test_ds, shuffle=False, **dl_kwargs),\n        tokenizer,\n    )\n\n# ----------------------------------------------------------------------------\n# Public factory\n# ----------------------------------------------------------------------------\n\ndef build_dataloaders(cfg, trial_mode: bool = False):\n    name = str(cfg.dataset.name).lower()\n    if name == \"cifar10\":\n        return _build_cifar10(cfg, trial_mode)\n    elif name == \"alpaca-cleaned\":\n        return _build_alpaca(cfg, trial_mode)\n    else:\n        raise ValueError(f\"Unsupported dataset \u0027{cfg.dataset.name}\u0027\")\n", "pyproject_toml": "[project]\nname = \"distilbert-adapter-exp\"\nversion = \"0.1.0\"\ndescription = \"DistilBERT adapter experiments with Hydra \u0026 WandB\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntransformers = \"\u003e=4.35.0\"\ndatasets = \"\u003e=2.14.0\"\ntorchvision = \"\u003e=0.15.0\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.15.0\"\nscikit-learn = \"^1.3.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\noptuna = \"^3.3.0\"\npandas = \"^2.1.0\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "import os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Optional, Sequence\n\nimport hydra\nimport optuna\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import OmegaConf\n\nimport torch\nfrom torch import nn\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import get_linear_schedule_with_warmup\nimport wandb\n\n# ---------------------------------------------------------------------------\n# Local imports (executed with -m from repository root)\n# ---------------------------------------------------------------------------\nfrom src.preprocess import build_dataloaders\nfrom src.model import build_model\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef _set_seed(seed: int = 42):\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _save_cfg(cfg, results_dir: Path):\n    \"\"\"Persist the (flattened) Hydra config for reproduction/evaluation.\"\"\"\n    results_dir.mkdir(parents=True, exist_ok=True)\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        OmegaConf.save(config=cfg, f=str(cfg_path))\n\n\n# ---------------------------------------------------------------------------\n# Core training / evaluation routine\n# ---------------------------------------------------------------------------\n\ndef _train_eval(cfg, trial: Optional[optuna.Trial] = None, verbose: bool = True) -\u003e Dict[str, Any]:\n    \"\"\"Complete training \u0026 validation loop. Returns best metrics recorded.\"\"\"\n\n    trial_mode: bool = bool(cfg.get(\"trial_mode\", False))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------- Data \u0026 Model -----------------------------------\n    train_loader, val_loader, _, tokenizer = build_dataloaders(cfg, trial_mode=trial_mode)\n    model = build_model(cfg, tokenizer).to(device)\n\n    # --------------------- Optimizer \u0026 LR schedule ------------------------\n    opt_map = {\"adamw\": torch.optim.AdamW, \"sgd\": torch.optim.SGD}\n    optim_cls = opt_map.get(cfg.training.optimizer.lower())\n    if optim_cls is None:\n        raise ValueError(f\"Unsupported optimizer {cfg.training.optimizer}\")\n\n    optimizer = optim_cls(model.parameters(), lr=cfg.training.learning_rate, weight_decay=cfg.training.weight_decay)\n    total_steps = len(train_loader) * (1 if trial_mode else cfg.training.epochs)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.training.warmup_steps,\n        num_training_steps=max(total_steps, 1),\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    # --------------------- WandB ------------------------------------------\n    use_wandb = (trial is None) and (cfg.wandb.mode != \"disabled\") and (not trial_mode)\n    if use_wandb:\n        run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run_id,\n            resume=\"allow\",\n            config=OmegaConf.to_container(cfg, resolve=True),\n            mode=cfg.wandb.mode,\n        )\n        if verbose:\n            print(f\"WandB URL: {wandb.run.get_url()}\")\n    else:\n        run = None\n\n    # --------------------- Training loop ----------------------------------\n    epochs = 1 if trial_mode else cfg.training.epochs if trial is None else cfg.optuna.get(\"max_epochs\", 3)\n    best_val_acc, best_val_f1 = 0.0, 0.0\n\n    global_step = 0\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_losses: List[float] = []\n        epoch_true: List[int] = []\n        epoch_pred: List[int] = []\n\n        for step, batch in enumerate(train_loader):\n            if trial_mode and step \u003e 1:\n                break\n            labels = batch.pop(\"labels\").to(device)\n            for k in batch:\n                batch[k] = batch[k].to(device)\n            outputs = model(batch)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n            epoch_losses.append(loss.item())\n            epoch_pred.extend(outputs.argmax(dim=-1).detach().cpu().tolist())\n            epoch_true.extend(labels.detach().cpu().tolist())\n            global_step += 1\n\n        train_loss = float(sum(epoch_losses) / max(1, len(epoch_losses)))\n        train_acc = accuracy_score(epoch_true, epoch_pred)\n        train_f1 = f1_score(epoch_true, epoch_pred, average=\"weighted\")\n\n        # --------------------- Validation ---------------------------------\n        model.eval()\n        val_losses: List[float] = []\n        val_true: List[int] = []\n        val_pred: List[int] = []\n        start_inf = time.time()\n        with torch.no_grad():\n            for step, batch in enumerate(val_loader):\n                if trial_mode and step \u003e 1:\n                    break\n                labels = batch.pop(\"labels\").to(device)\n                for k in batch:\n                    batch[k] = batch[k].to(device)\n                outputs = model(batch)\n                val_losses.append(criterion(outputs, labels).item())\n                val_pred.extend(outputs.argmax(dim=-1).detach().cpu().tolist())\n                val_true.extend(labels.detach().cpu().tolist())\n        inference_time = (time.time() - start_inf) / max(1, len(val_loader))\n        val_loss = float(sum(val_losses) / max(1, len(val_losses)))\n        val_acc = accuracy_score(val_true, val_pred)\n        val_f1 = f1_score(val_true, val_pred, average=\"weighted\")\n\n        best_val_acc = max(best_val_acc, val_acc)\n        best_val_f1 = max(best_val_f1, val_f1)\n\n        log_dict = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"train_acc\": train_acc,\n            \"val_acc\": val_acc,\n            \"train_f1\": train_f1,\n            \"val_f1\": val_f1,\n            \"best_val_acc\": best_val_acc,\n            \"best_val_f1\": best_val_f1,\n            \"inference_time\": inference_time,\n        }\n        if use_wandb:\n            wandb.log(log_dict, step=epoch)\n        if verbose:\n            print(f\"Epoch {epoch}: {log_dict}\")\n\n    if run is not None:\n        wandb.finish()\n\n    return {\"best_val_acc\": best_val_acc, \"best_val_f1\": best_val_f1}\n\n\n# ---------------------------------------------------------------------------\n# Optuna helpers\n# ---------------------------------------------------------------------------\n\ndef _apply_optuna_suggestions(cfg, trial: optuna.Trial, param_map: Dict[str, Sequence[str]]):\n    \"\"\"Mutate `cfg` in-place with values suggested by Optuna.\"\"\"\n    for param_name, space in cfg.optuna.search_space.items():\n        stype = str(space.type)\n        if stype == \"loguniform\":\n            val = trial.suggest_float(param_name, float(space.low), float(space.high), log=True)\n        elif stype == \"uniform\":\n            val = trial.suggest_float(param_name, float(space.low), float(space.high))\n        elif stype == \"categorical\":\n            val = trial.suggest_categorical(param_name, list(space.choices))\n        elif stype == \"int\":\n            val = trial.suggest_int(param_name, int(space.low), int(space.high), step=int(space.get(\"step\", 1)))\n        else:\n            raise ValueError(f\"Unsupported search space type: {space.type}\")\n\n        # propagate into cfg along mapped path\n        ptr = cfg\n        for key in param_map[param_name][:-1]:\n            ptr = ptr[key]\n        ptr[param_map[param_name][-1]] = val\n\n\n# ---------------------------------------------------------------------------\n# Hydra entrypoint ----------------------------------------------------------\n# ---------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):  # noqa: C901\n    \"\"\"Hydra-managed entrypoint. Handles Optuna \u0026 final training.\"\"\"\n\n    _set_seed()\n\n    # ---------------------------------------------------------------------\n    # Flatten configuration: merge run-specific subtree into root namespace\n    # ---------------------------------------------------------------------\n    flat_cfg = OmegaConf.merge(cfg, cfg.run)  # later keys (run) overwrite root duplicates\n    if cfg.get(\"trial_mode\", False):\n        flat_cfg.trial_mode = True\n        flat_cfg.wandb.mode = \"disabled\"\n    flat_cfg.results_dir = cfg.results_dir\n\n    # results directory ----------------------------------------------------\n    results_dir = Path(flat_cfg.results_dir).expanduser().absolute()\n    _save_cfg(flat_cfg, results_dir)\n\n    # --------------------- Hyper-parameter search -------------------------\n    if int(flat_cfg.optuna.n_trials) \u003e 0 and not flat_cfg.get(\"trial_mode\", False):\n        param_map = {\n            \"learning_rate\": [\"training\", \"learning_rate\"],\n            \"batch_size\": [\"training\", \"batch_size\"],\n            \"weight_decay\": [\"training\", \"weight_decay\"],\n            \"adapter_reduction_factor\": [\"model\", \"task_adapters\", \"reduction_factor\"],\n        }\n\n        study_path = results_dir / \"optuna_study.db\"\n        study = optuna.create_study(\n            study_name=flat_cfg.run_id,\n            direction=str(flat_cfg.optuna.direction),\n            storage=f\"sqlite:///{study_path}\",\n            load_if_exists=True,\n        )\n\n        def objective(trial: optuna.Trial):\n            trial_cfg = OmegaConf.create(OmegaConf.to_container(flat_cfg, resolve=True))\n            _apply_optuna_suggestions(trial_cfg, trial, param_map)\n            trial_cfg.wandb.mode = \"disabled\"  # disable WandB inside optimisation trials\n            trial_cfg.optuna.max_epochs = flat_cfg.optuna.get(\"max_epochs\", 3)\n            metrics = _train_eval(trial_cfg, trial=trial, verbose=False)\n            return metrics[\"best_val_acc\"]\n\n        study.optimize(objective, n_trials=int(flat_cfg.optuna.n_trials), gc_after_trial=True)\n        print(f\"Optuna best params: {study.best_params}\")\n\n        # Update flattened cfg with best params for final training ----------\n        for key, val in study.best_params.items():\n            if key == \"adapter_reduction_factor\":\n                flat_cfg.model.task_adapters.reduction_factor = val\n            elif key in {\"learning_rate\", \"batch_size\", \"weight_decay\"}:\n                flat_cfg.training[key] = val\n\n    # --------------------- Final full training ---------------------------\n    _train_eval(flat_cfg, trial=None, verbose=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
cpu
RAM：500 MB

# Current Research Method
We compare DistilBERT with task-specific adapters (proposed) against standard fine-tuning (comparative) across vision and language tasks to evaluate performance improvements.

# Experimental Design
- Summary: Comparative analysis of DistilBERT performance across vision and language tasks
- Evaluation metrics: ['accuracy', 'f1_score', 'inference_time']

# Experiment Runs

- Run ID: proposed-DistilBERT-base-66M-CIFAR-10
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  Config File: config/run/proposed-DistilBERT-base-66M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-CIFAR-10
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 1e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: proposed-DistilBERT-base-66M-alpaca-cleaned
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  Config File: config/run/proposed-DistilBERT-base-66M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-alpaca-cleaned
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 5e-5
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-CIFAR-10
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  Config File: config/run/comparative-1-DistilBERT-base-66M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-CIFAR-10
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-1-DistilBERT-base-66M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-alpaca-cleaned
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  


# External Resources (Use these for implementation)


Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "import os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Optional, Sequence\n\nimport hydra\nimport optuna\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import OmegaConf\n\nimport torch\nfrom torch import nn\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import get_linear_schedule_with_warmup\nimport wandb\n\n# ---------------------------------------------------------------------------\n# Local imports (executed with -m from repository root)\n# ---------------------------------------------------------------------------\nfrom src.preprocess import build_dataloaders\nfrom src.model import build_model\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef _set_seed(seed: int = 42):\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _save_cfg(cfg, results_dir: Path):\n    \"\"\"Persist the (flattened) Hydra config for reproduction/evaluation.\"\"\"\n    results_dir.mkdir(parents=True, exist_ok=True)\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        OmegaConf.save(config=cfg, f=str(cfg_path))\n\n\n# ---------------------------------------------------------------------------\n# Core training / evaluation routine\n# ---------------------------------------------------------------------------\n\ndef _train_eval(cfg, trial: Optional[optuna.Trial] = None, verbose: bool = True) -> Dict[str, Any]:\n    \"\"\"Complete training & validation loop. Returns best metrics recorded.\"\"\"\n\n    trial_mode: bool = bool(cfg.get(\"trial_mode\", False))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------------- Data & Model -----------------------------------\n    train_loader, val_loader, _, tokenizer = build_dataloaders(cfg, trial_mode=trial_mode)\n    model = build_model(cfg, tokenizer).to(device)\n\n    # --------------------- Optimizer & LR schedule ------------------------\n    opt_map = {\"adamw\": torch.optim.AdamW, \"sgd\": torch.optim.SGD}\n    optim_cls = opt_map.get(str(cfg.training.optimizer).lower())\n    if optim_cls is None:\n        raise ValueError(f\"Unsupported optimizer {cfg.training.optimizer}\")\n\n    optimizer = optim_cls(model.parameters(), lr=cfg.training.learning_rate, weight_decay=cfg.training.weight_decay)\n    total_steps = len(train_loader) * (1 if trial_mode else cfg.training.epochs)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.training.warmup_steps,\n        num_training_steps=max(total_steps, 1),\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    # --------------------- WandB ------------------------------------------\n    use_wandb = (trial is None) and (cfg.wandb.mode != \"disabled\") and (not trial_mode)\n    if use_wandb:\n        run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run_id,\n            resume=\"allow\",\n            config=OmegaConf.to_container(cfg, resolve=True),\n            mode=cfg.wandb.mode,\n        )\n        if verbose:\n            print(f\"WandB URL: {wandb.run.get_url()}\")\n    else:\n        run = None\n\n    # --------------------- Training loop ----------------------------------\n    epochs = 1 if trial_mode else (cfg.optuna.get(\"max_epochs\", cfg.training.epochs) if trial is not None else cfg.training.epochs)\n    best_val_acc, best_val_f1 = 0.0, 0.0\n\n    global_step = 0\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_losses: List[float] = []\n        epoch_true: List[int] = []\n        epoch_pred: List[int] = []\n\n        for step, batch in enumerate(train_loader):\n            if trial_mode and step > 1:\n                break\n            labels = batch.pop(\"labels\").to(device)\n            for k in batch:\n                batch[k] = batch[k].to(device)\n            outputs = model(batch)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n            epoch_losses.append(loss.item())\n            epoch_pred.extend(outputs.argmax(dim=-1).detach().cpu().tolist())\n            epoch_true.extend(labels.detach().cpu().tolist())\n            global_step += 1\n\n        train_loss = float(sum(epoch_losses) / max(1, len(epoch_losses)))\n        train_acc = accuracy_score(epoch_true, epoch_pred)\n        train_f1 = f1_score(epoch_true, epoch_pred, average=\"weighted\")\n\n        # --------------------- Validation ---------------------------------\n        model.eval()\n        val_losses: List[float] = []\n        val_true: List[int] = []\n        val_pred: List[int] = []\n        start_inf = time.time()\n        with torch.no_grad():\n            for step, batch in enumerate(val_loader):\n                if trial_mode and step > 1:\n                    break\n                labels = batch.pop(\"labels\").to(device)\n                for k in batch:\n                    batch[k] = batch[k].to(device)\n                outputs = model(batch)\n                val_losses.append(criterion(outputs, labels).item())\n                val_pred.extend(outputs.argmax(dim=-1).detach().cpu().tolist())\n                val_true.extend(labels.detach().cpu().tolist())\n        inference_time = (time.time() - start_inf) / max(1, len(val_loader))\n        val_loss = float(sum(val_losses) / max(1, len(val_losses)))\n        val_acc = accuracy_score(val_true, val_pred)\n        val_f1 = f1_score(val_true, val_pred, average=\"weighted\")\n\n        best_val_acc = max(best_val_acc, val_acc)\n        best_val_f1 = max(best_val_f1, val_f1)\n\n        log_dict = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"train_acc\": train_acc,\n            \"val_acc\": val_acc,\n            \"train_f1\": train_f1,\n            \"val_f1\": val_f1,\n            \"best_val_acc\": best_val_acc,\n            \"best_val_f1\": best_val_f1,\n            \"inference_time\": inference_time,\n        }\n        if use_wandb:\n            wandb.log(log_dict, step=epoch)\n        if verbose:\n            print(f\"Epoch {epoch}: {log_dict}\")\n\n    if run is not None:\n        wandb.finish()\n\n    return {\"best_val_acc\": best_val_acc, \"best_val_f1\": best_val_f1}\n\n\n# ---------------------------------------------------------------------------\n# Optuna helpers\n# ---------------------------------------------------------------------------\n\ndef _apply_optuna_suggestions(cfg, trial: optuna.Trial, param_map: Dict[str, Sequence[str]]):\n    \"\"\"Mutate `cfg` in-place with values suggested by Optuna.\"\"\"\n    for param_name, space in cfg.optuna.search_space.items():\n        stype = str(space.type)\n        if stype == \"loguniform\":\n            val = trial.suggest_float(param_name, float(space.low), float(space.high), log=True)\n        elif stype == \"uniform\":\n            val = trial.suggest_float(param_name, float(space.low), float(space.high))\n        elif stype == \"categorical\":\n            val = trial.suggest_categorical(param_name, list(space.choices))\n        elif stype == \"int\":\n            val = trial.suggest_int(param_name, int(space.low), int(space.high), step=int(space.get(\"step\", 1)))\n        else:\n            raise ValueError(f\"Unsupported search space type: {space.type}\")\n\n        # propagate into cfg along mapped path (create intermediates if missing)\n        ptr = cfg\n        for key in param_map[param_name][:-1]:\n            if key not in ptr:\n                ptr[key] = {}\n            ptr = ptr[key]\n        ptr[param_map[param_name][-1]] = val\n\n\n# ---------------------------------------------------------------------------\n# Hydra entrypoint ----------------------------------------------------------\n# ---------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):  # noqa: C901\n    \"\"\"Hydra-managed entrypoint. Handles Optuna & final training.\"\"\"\n\n    _set_seed()\n\n    # ---------------------------------------------------------------------\n    # Flatten configuration: merge run-specific subtree into root namespace\n    # ---------------------------------------------------------------------\n    flat_cfg = OmegaConf.merge(cfg, cfg.run)  # later keys (run) overwrite root duplicates\n    if cfg.get(\"trial_mode\", False):\n        flat_cfg.trial_mode = True\n        flat_cfg.wandb.mode = \"disabled\"\n    flat_cfg.results_dir = cfg.results_dir\n\n    # results directory ----------------------------------------------------\n    results_dir = Path(flat_cfg.results_dir).expanduser().absolute()\n    _save_cfg(flat_cfg, results_dir)\n\n    # --------------------- Hyper-parameter search -------------------------\n    if int(flat_cfg.optuna.n_trials) > 0 and not flat_cfg.get(\"trial_mode\", False):\n        param_map = {\n            \"learning_rate\": [\"training\", \"learning_rate\"],\n            \"batch_size\": [\"training\", \"batch_size\"],\n            \"weight_decay\": [\"training\", \"weight_decay\"],\n            \"adapter_reduction_factor\": [\"model\", \"task_adapters\", \"reduction_factor\"],\n        }\n\n        study_path = results_dir / \"optuna_study.db\"\n        study = optuna.create_study(\n            study_name=str(flat_cfg.run_id),\n            direction=str(flat_cfg.optuna.direction),\n            storage=f\"sqlite:///{study_path}\",\n            load_if_exists=True,\n        )\n\n        def objective(trial: optuna.Trial):\n            trial_cfg = OmegaConf.create(OmegaConf.to_container(flat_cfg, resolve=True))\n            _apply_optuna_suggestions(trial_cfg, trial, param_map)\n            trial_cfg.wandb.mode = \"disabled\"  # disable WandB inside optimisation trials\n            trial_cfg.optuna.max_epochs = flat_cfg.optuna.get(\"max_epochs\", 3)\n            metrics = _train_eval(trial_cfg, trial=trial, verbose=False)\n            return metrics[\"best_val_acc\"]\n\n        study.optimize(objective, n_trials=int(flat_cfg.optuna.n_trials), gc_after_trial=True)\n        print(f\"Optuna best params: {study.best_params}\")\n\n        # Update flattened cfg with best params for final training ----------\n        for key, val in study.best_params.items():\n            if key == \"adapter_reduction_factor\":\n                flat_cfg.model.task_adapters.reduction_factor = val\n            elif key in {\"learning_rate\", \"batch_size\", \"weight_decay\"}:\n                flat_cfg.training[key] = val\n\n    # --------------------- Final full training ---------------------------\n    _train_eval(flat_cfg, trial=None, verbose=True)\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluate_py": "\"\"\"Independent evaluation & figure generation for the collected WandB runs.\n\nFixes: ensure all metrics referenced in plotting exist. When a run does not\ncontain ``train_acc`` or other expected columns they are skipped gracefully.\n\"\"\"\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\n\nFIGURE_DIR = \"images\"\nDATA_DIR = \"wandb_data\"\n\n# ---------------------------------------------------------------------------\n# CLI helper\n# ---------------------------------------------------------------------------\n\ndef _parse_results_dir(argv) -> Path:\n    \"\"\"Extract results_dir either from key=value or first positional argument.\"\"\"\n    for arg in argv[1:]:\n        if arg.startswith(\"results_dir=\"):\n            return Path(arg.split(\"=\", 1)[1]).expanduser().absolute()\n    if len(argv) > 1 and not argv[1].startswith(\"-\"):\n        return Path(argv[1]).expanduser().absolute()\n    raise ValueError(\"results_dir argument is required (e.g., results_dir=outputs/)\")\n\n\n# ---------------------------------------------------------------------------\n# WandB helpers\n# ---------------------------------------------------------------------------\n\ndef _fetch_runs(entity: str, project: str):\n    api = wandb.Api()\n    return api.runs(f\"{entity}/{project}\")\n\n\ndef _export_metrics(runs, out_dir: Path):\n    \"\"\"Save per-run history CSV & summary JSON. Returns summary dict.\"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    summaries: Dict[str, Dict] = {}\n\n    needed_cols: List[str] = [\n        \"epoch\",\n        \"best_val_acc\",\n        \"best_val_f1\",\n        \"train_loss\",\n        \"val_loss\",\n        \"train_acc\",\n        \"val_acc\",\n        \"train_f1\",\n        \"val_f1\",\n        \"inference_time\",\n    ]\n    for run in runs:\n        df = run.history(keys=needed_cols, pandas=True)\n        df.to_csv(out_dir / f\"run_{run.name}_metrics.csv\", index=False)\n\n        # Summary per run ---------------------------------------------------\n        summaries[run.name] = {\n            \"best_val_acc\": float(df[\"best_val_acc\"].dropna().max()) if \"best_val_acc\" in df else None,\n            \"best_val_f1\": float(df[\"best_val_f1\"].dropna().max()) if \"best_val_f1\" in df else None,\n        }\n    with open(out_dir / \"summary.json\", \"w\") as f:\n        json.dump(summaries, f, indent=2)\n    return summaries\n\n\n# ---------------------------------------------------------------------------\n# Plotting helpers\n# ---------------------------------------------------------------------------\n\ndef _plot_learning_curves(runs, dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    for run in runs:\n        df = run.history(keys=[\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"], pandas=True)\n        if df.empty or \"epoch\" not in df:\n            continue\n\n        # Loss curve -------------------------------------------------------\n        fig, ax = plt.subplots(figsize=(6, 4))\n        if \"train_loss\" in df and not df[\"train_loss\"].isnull().all():\n            ax.plot(df[\"epoch\"], df[\"train_loss\"], label=\"Train Loss\")\n        if \"val_loss\" in df and not df[\"val_loss\"].isnull().all():\n            ax.plot(df[\"epoch\"], df[\"val_loss\"], label=\"Val Loss\")\n        ax.set_title(f\"Loss Curve – {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"loss_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n        # Accuracy curve ---------------------------------------------------\n        if (\"train_acc\" in df and not df[\"train_acc\"].isnull().all()) or (\n            \"val_acc\" in df and not df[\"val_acc\"].isnull().all()\n        ):\n            fig, ax = plt.subplots(figsize=(6, 4))\n            if \"train_acc\" in df and not df[\"train_acc\"].isnull().all():\n                ax.plot(df[\"epoch\"], df[\"train_acc\"], label=\"Train Acc\")\n            if \"val_acc\" in df and not df[\"val_acc\"].isnull().all():\n                ax.plot(df[\"epoch\"], df[\"val_acc\"], label=\"Val Acc\")\n            ax.set_title(f\"Accuracy Curve – {run.name}\")\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"Accuracy\")\n            ax.legend()\n            fig.tight_layout()\n            fname = dest / f\"acc_curve_{run.name}.pdf\"\n            fig.savefig(fname)\n            print(fname.name)\n            plt.close(fig)\n\n\ndef _plot_comparison(summary: Dict[str, Dict], dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    df = pd.DataFrame.from_dict(summary, orient=\"index\").reset_index().rename(columns={\"index\": \"run_id\"})\n    if df[\"best_val_acc\"].isnull().all():\n        return\n    fig, ax = plt.subplots(figsize=(max(6, len(df) * 0.8), 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"best_val_acc\", ax=ax, palette=\"viridis\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_title(\"Best Validation Accuracy Across Runs\")\n    for p in ax.patches:\n        ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                    ha=\"center\", va=\"center\", xytext=(0, 5), textcoords=\"offset points\")\n    fig.tight_layout()\n    fname = dest / \"comparison_best_val_acc.pdf\"\n    fig.savefig(fname)\n    print(fname.name)\n    plt.close(fig)\n\n\n# ---------------------------------------------------------------------------\n# Improvement computation\n# ---------------------------------------------------------------------------\n\ndef _compute_improvement(summary: Dict[str, Dict]):\n    records: Dict[str, Dict[str, float]] = {}\n    for run_id, metrics in summary.items():\n        # heuristic: dataset name is last token after last '-'\n        dataset_name = run_id.split(\"-\")[-1]\n        method = \"proposed\" if run_id.startswith(\"proposed\") else \"baseline\"\n        records.setdefault(dataset_name, {})[method] = metrics[\"best_val_acc\"]\n\n    improvement = {\n        ds: {\n            **vals,\n            \"improvement_rate\": ((vals.get(\"proposed\") - vals.get(\"baseline\")) / vals.get(\"baseline\")) if vals.get(\"baseline\") else None,\n        }\n        for ds, vals in records.items()\n    }\n    return improvement\n\n\n# ---------------------------------------------------------------------------\n# Entrypoint\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    results_dir = _parse_results_dir(sys.argv)\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"Config file not found at {cfg_path}\")\n\n    import yaml\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    entity = cfg[\"wandb\"][\"entity\"]\n    project = cfg[\"wandb\"][\"project\"]\n\n    runs = _fetch_runs(entity, project)\n    summary = _export_metrics(runs, results_dir / DATA_DIR)\n\n    _plot_learning_curves(runs, results_dir / FIGURE_DIR)\n    _plot_comparison(summary, results_dir / FIGURE_DIR)\n\n    improvement = _compute_improvement(summary)\n    with open(results_dir / DATA_DIR / \"improvement.json\", \"w\") as f:\n        json.dump(improvement, f, indent=2)\n\n    print(\"Figures and data exported successfully.\")\n",
    "preprocess_py": "\"\"\"Dataset preprocessing and DataLoader construction.\"\"\"\nfrom typing import Tuple, List\nimport random\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import datasets as tv_datasets, transforms\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n# ----------------------------------------------------------------------------\n# CIFAR-10 helpers\n# ----------------------------------------------------------------------------\n\nclass _CIFARWrapper(torch.utils.data.Dataset):\n    \"\"\"Wrap torchvision dataset to return dicts compatible with model forward.\"\"\"\n\n    def __init__(self, base, tfm):\n        self.base = base\n        self.tfm = tfm\n\n    def __len__(self):\n        return len(self.base)\n\n    def __getitem__(self, idx):\n        img, label = self.base[idx]\n        img = self.tfm(img)\n        return {\"pixel_values\": img, \"labels\": torch.tensor(label, dtype=torch.long)}\n\n\ndef _split(dataset: torch.utils.data.Dataset, splits: Tuple[float, float, float]):\n    n_total = len(dataset)\n    lengths = [int(p * n_total) for p in splits]\n    lengths[-1] = n_total - sum(lengths[:-1])  # ensure all data used\n    return random_split(dataset, lengths, generator=torch.Generator().manual_seed(42))\n\n\ndef _build_cifar10(cfg, trial_mode: bool):\n    size = int(cfg.dataset.preprocessing.resize)\n    aug: List[str] = list(cfg.dataset.preprocessing.augmentations)\n\n    tfm_train = [\n        transforms.RandomCrop(size, padding=4) if \"random_crop\" in aug else transforms.Resize(size),\n        transforms.RandomHorizontalFlip() if \"horizontal_flip\" in aug else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ]\n    tfm_eval = [transforms.Resize(size), transforms.ToTensor()]\n\n    train_ds = tv_datasets.CIFAR10(root=\".cache/\", train=True, download=True)\n    test_ds = tv_datasets.CIFAR10(root=\".cache/\", train=False, download=True)\n    full_ds = ConcatDataset([train_ds, test_ds])\n    train_set, val_set, test_set = _split(full_ds, (\n        cfg.dataset.split.train,\n        cfg.dataset.split.val,\n        cfg.dataset.split.test,\n    ))\n\n    train_set = _CIFARWrapper(train_set, transforms.Compose(tfm_train))\n    val_set = _CIFARWrapper(val_set, transforms.Compose(tfm_eval))\n    test_set = _CIFARWrapper(test_set, transforms.Compose(tfm_eval))\n\n    bs = 2 if trial_mode else int(cfg.training.batch_size)\n    dl_kwargs = dict(batch_size=bs, num_workers=2, pin_memory=True)\n    return (\n        DataLoader(train_set, shuffle=True, **dl_kwargs),\n        DataLoader(val_set, shuffle=False, **dl_kwargs),\n        DataLoader(test_set, shuffle=False, **dl_kwargs),\n        None,\n    )\n\n# ----------------------------------------------------------------------------\n# Alpaca-cleaned helpers (binary classification: translation vs others)\n# ----------------------------------------------------------------------------\n\ndef _label_from_instruction(instr: str) -> int:\n    lowered = instr.lower()\n    keywords = [\"translate\", \"translation\", \"translating\", \"translator\"]\n    return int(any(k in lowered for k in keywords))\n\n\ndef _encode_text(batch, tokenizer, max_len):\n    instructions = batch[\"instruction\"]\n    inputs = batch.get(\"input\", [\"\" for _ in range(len(instructions))])\n    texts = [f\"Instruction: {ins} Input: {inp}\" for ins, inp in zip(instructions, inputs)]\n    enc = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n    enc[\"labels\"] = [_label_from_instruction(ins) for ins in instructions]\n    return enc\n\n\ndef _build_alpaca(cfg, trial_mode: bool):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    raw = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", cache_dir=\".cache/\")\n\n    encoded = raw.map(\n        lambda b: _encode_text(b, tokenizer, cfg.dataset.max_length),\n        batched=True,\n        remove_columns=raw.column_names,\n    )\n    encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    total = len(encoded)\n    train_size = int(cfg.dataset.split.train * total)\n    val_size = int(cfg.dataset.split.val * total)\n    test_size = total - train_size - val_size\n    train_ds, val_ds, test_ds = random_split(\n        encoded,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42),\n    )\n\n    bs = 2 if trial_mode else int(cfg.training.batch_size)\n    dl_kwargs = dict(batch_size=bs, num_workers=2, pin_memory=True)\n    return (\n        DataLoader(train_ds, shuffle=True, **dl_kwargs),\n        DataLoader(val_ds, shuffle=False, **dl_kwargs),\n        DataLoader(test_ds, shuffle=False, **dl_kwargs),\n        tokenizer,\n    )\n\n# ----------------------------------------------------------------------------\n# Public factory\n# ----------------------------------------------------------------------------\n\ndef build_dataloaders(cfg, trial_mode: bool = False):\n    name = str(cfg.dataset.name).lower()\n    if name == \"cifar10\":\n        return _build_cifar10(cfg, trial_mode)\n    elif name == \"alpaca-cleaned\":\n        return _build_alpaca(cfg, trial_mode)\n    else:\n        raise ValueError(f\"Unsupported dataset '{cfg.dataset.name}'\")\n",
    "model_py": "\"\"\"Model architectures.\"\"\"\n\nimport torch\nfrom torch import nn\nfrom transformers import DistilBertModel\n\n# ---------------------------------------------------------------------------\n# Auxiliary modules\n# ---------------------------------------------------------------------------\n\nclass HoulsbyAdapter(nn.Module):\n    \"\"\"Implementation of Houlsby adapter (bottleneck).\"\"\"\n\n    def __init__(self, hidden_dim: int, reduction_factor: int = 16, non_linearity: str = \"relu\"):\n        super().__init__()\n        bottleneck = max(1, hidden_dim // reduction_factor)\n        self.down = nn.Linear(hidden_dim, bottleneck)\n        self.nonlin = getattr(nn, non_linearity.capitalize())() if hasattr(nn, non_linearity.capitalize()) else nn.ReLU()\n        self.up = nn.Linear(bottleneck, hidden_dim)\n\n    def forward(self, x):\n        return self.up(self.nonlin(self.down(x)))\n\n\nclass VisionPatchEmbedding(nn.Module):\n    \"\"\"Small ViT-style patch embedding projecting image patches to hidden dim.\"\"\"\n\n    def __init__(self, in_channels: int, patch_size: int, embed_dim: int):\n        super().__init__()\n        self.patch_size = int(patch_size)\n        self.proj = nn.Linear(in_channels * self.patch_size * self.patch_size, embed_dim)\n\n    def forward(self, imgs):  # imgs: B,C,H,W\n        B, C, H, W = imgs.shape\n        p = self.patch_size\n        assert H % p == 0 and W % p == 0, \"Image dimensions must be divisible by patch size\"\n        patches = imgs.unfold(2, p, p).unfold(3, p, p)  # B,C,nH,nW,p,p\n        patches = patches.contiguous().view(B, C, -1, p, p)\n        patches = patches.permute(0, 2, 1, 3, 4).contiguous()  # B,N,C,p,p\n        patches = patches.view(B, patches.size(1), -1)  # B,N,C*p*p\n        return self.proj(patches)  # B, N, D\n\n# ---------------------------------------------------------------------------\n# DistilBERT classifier (text or vision)\n# ---------------------------------------------------------------------------\n\nclass DistilBertClassifier(nn.Module):\n    \"\"\"Adapter-ready DistilBERT classifier supporting vision patch inputs.\"\"\"\n\n    def __init__(self, model_name: str, num_labels: int, adapter_cfg=None, vision_patch_cfg=None):\n        super().__init__()\n        self.is_vision = vision_patch_cfg is not None\n        self.bert = DistilBertModel.from_pretrained(model_name, cache_dir=\".cache/\")\n        hidden = self.bert.config.hidden_size\n\n        if self.is_vision:\n            self.vision = VisionPatchEmbedding(3, vision_patch_cfg.patch_size, hidden)\n        else:\n            self.vision = None\n\n        if adapter_cfg and adapter_cfg.get(\"enabled\", False):\n            self.adapter = HoulsbyAdapter(\n                hidden_dim=hidden,\n                reduction_factor=int(adapter_cfg.reduction_factor),\n                non_linearity=str(adapter_cfg.non_linearity),\n            )\n        else:\n            self.adapter = None\n\n        self.pre_cls = nn.Linear(hidden, hidden)\n        self.act = nn.GELU()\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(hidden, num_labels)\n\n    def forward(self, batch):\n        if self.is_vision and \"pixel_values\" in batch:\n            embeds = self.vision(batch[\"pixel_values\"])  # B,N,D\n            mask = torch.ones(embeds.shape[:2], dtype=torch.long, device=embeds.device)\n            outputs = self.bert(inputs_embeds=embeds, attention_mask=mask)\n        else:\n            outputs = self.bert(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n            )\n\n        pooled = outputs.last_hidden_state[:, 0]  # CLS-token equivalent\n        if self.adapter is not None:\n            pooled = pooled + self.adapter(pooled)\n        x = self.pre_cls(pooled)\n        x = self.act(x)\n        x = self.dropout(x)\n        logits = self.cls(x)\n        return logits\n\n# ---------------------------------------------------------------------------\n# Factory\n# ---------------------------------------------------------------------------\n\ndef build_model(cfg, tokenizer=None):\n    \"\"\"Factory that builds the appropriate model according to cfg.\"\"\"\n    vision_patch_cfg = (\n        cfg.model.vision_patch if str(cfg.model.get(\"input_representation\", \"text\")) == \"image_sequence\" else None\n    )\n    method_tag = str(cfg.method).lower()\n    adapter_cfg = cfg.model.task_adapters if method_tag.startswith(\"proposed\") else None\n    return DistilBertClassifier(\n        model_name=cfg.model.name,\n        num_labels=int(cfg.model.num_labels),\n        adapter_cfg=adapter_cfg,\n        vision_patch_cfg=vision_patch_cfg,\n    )\n",
    "main_py": "\"\"\"Experiment orchestrator: launches src.train as a subprocess with inherited Hydra overrides.\"\"\"\n\nimport subprocess\nimport sys\nfrom typing import List\n\nimport hydra\nfrom hydra.utils import get_original_cli_args\n\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):\n    \"\"\"Hydra entrypoint that forwards all CLI overrides to the training subprocess.\"\"\"\n    original_cli: List[str] = get_original_cli_args()\n\n    # Build subprocess command ------------------------------------------------\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + original_cli\n\n    # Force WandB disabled when trial_mode=true --------------------------------\n    if cfg.get(\"trial_mode\", False):\n        cmd.append(\"wandb.mode=disabled\")\n\n    print(\"Executing subprocess:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"distilbert-adapter-exp\"\nversion = \"0.1.0\"\ndescription = \"DistilBERT adapter experiments with Hydra & WandB\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntransformers = \">=4.35.0\"\ndatasets = \">=2.14.0\"\ntorchvision = \">=0.15.0\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.15.0\"\nscikit-learn = \"^1.3.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\noptuna = \"^3.3.0\"\npandas = \"^2.1.0\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
    "config_yaml": "defaults:\n  - run: proposed-DistilBERT-base-66M-CIFAR-10  # default run, override via CLI\n  - _self_\n\nwandb:\n  entity: gengaru617-personal\n  project: 251017-test\n  mode: online  # will be overridden to \"disabled\" automatically in trial_mode\n\ntrial_mode: false  # enable via CLI for CI validation\n\nresults_dir: outputs/\n\nhydra:\n  run:\n    dir: .\n  output_subdir: null\n"
}
