
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path}

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} trial_mode=true
```
- `run`: Experiment run_id (matching a run_id from config/run/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `trial_mode=true` (optional): Lightweight execution for validation (epochs=1, batches limited to 1-2, disable Optuna n_trials=0, **WandB disabled**)

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path}
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * Log ALL metrics to WandB: `wandb.log({"train_loss": 0.5, "val_acc": 0.85, "epoch": 1, ...})`
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path}`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse `results_dir` from command line arguments
  * Load WandB config from `{results_dir}/config.yaml`
  * Retrieve all experimental data from WandB API:
    ```python
    api = wandb.Api()
    runs = api.runs(f"{entity}/{project}")
    for run in runs:
        metrics_df = run.history()  # pandas DataFrame with all logged metrics
    ```
  * Export retrieved data to:
    - `{results_dir}/wandb_data/run_{run_id}_metrics.csv` (per-run metrics)
    - `{results_dir}/wandb_data/summary.json` (aggregated comparison)
  * Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
  * Generate ALL publication-quality PDF figures and save to `{results_dir}/images/`:
    - Learning curves (loss, accuracy over epochs)
    - Cross-run comparison charts (bar charts, box plots)
    - Performance metrics tables
  * Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
  * Follow naming convention: `<figure_topic>[_<condition>][_pairN].pdf`
  * Print generated figure names to stdout

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- Pass all Hydra overrides to train.py subprocess (e.g., `wandb.mode=disabled`, `trial_mode=true`)
- In trial_mode, automatically set `wandb.mode=disabled`

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 251017-test
    mode: online  # Automatically set to "disabled" in trial_mode
  ```

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Trial Mode Behavior**: trial_mode=true automatically disables WandB (sets wandb.mode=disabled)
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: The package is close, but the following mandatory requirements are not satisfied:

1. Evaluation-script CLI mismatch (Core criterion 2 & 8).
   • Specified interface:  `uv run python -m src.evaluate results_dir={path}`  (key-value arg).
   • Provided evaluate.py expects a positional argument (e.g. `python -m src.evaluate outputs/`).  The required call will pass the literal string `results_dir=/...` and the script will fail.  Evaluate.py must accept the `results_dir` key-value override exactly as defined.

2. Trial-mode WandB disabling (Core criterion 6 & 9).
   • Requirement: trial_mode=true must *set* `wandb.mode = "disabled"` automatically.
   • Current implementation only suppresses wandb.init via a boolean flag, the config value itself is left unchanged.  The code must explicitly overwrite `cfg.wandb.mode` (or equivalent) so that downstream logic that relies on the field can safely assume it is set to "disabled".

Because these mandatory interface and configuration requirements are not fully met, the code is **not ready for immediate execution** in the described experiments.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "defaults:\n  - run: proposed-DistilBERT-base-66M-CIFAR-10\n  - _self_\n\nwandb:\n  entity: gengaru617-personal\n  project: 251017-test\n  mode: online  # overridden to \"disabled\" when trial_mode=true\n\n# Trial mode flag (set via CLI)\ntrial_mode: false\n\n# Path to store results and artefacts\nresults_dir: outputs/\n\nhydra:\n  run:\n    dir: .\n  output_subdir: null\n", "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\n\nFIGURE_DIR = \"images\"\nDATA_DIR = \"wandb_data\"\n\n\n# ---------------------------------------------------------------------------\n# WandB helpers\n# ---------------------------------------------------------------------------\n\ndef _fetch_runs(entity: str, project: str):\n    api = wandb.Api()\n    return api.runs(f\"{entity}/{project}\")\n\n\ndef _export_metrics(runs, out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    summaries: Dict[str, Dict] = {}\n\n    needed_cols = [\n        \"epoch\",\n        \"best_val_acc\",\n        \"best_val_f1\",\n        \"train_loss\",\n        \"val_loss\",\n        \"val_acc\",\n        \"val_f1\",\n    ]\n\n    for run in runs:\n        df = run.history(keys=needed_cols, pandas=True)\n        df.to_csv(out_dir / f\"run_{run.name}_metrics.csv\", index=False)\n        summaries[run.name] = {\n            \"best_val_acc\": df[\"best_val_acc\"].dropna().max() if \"best_val_acc\" in df else None,\n            \"best_val_f1\": df[\"best_val_f1\"].dropna().max() if \"best_val_f1\" in df else None,\n        }\n    with open(out_dir / \"summary.json\", \"w\") as f:\n        json.dump(summaries, f, indent=2)\n    return summaries\n\n\n# ---------------------------------------------------------------------------\n# Plotting\n# ---------------------------------------------------------------------------\n\ndef _plot_learning_curves(runs, dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    for run in runs:\n        df = run.history(keys=[\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"], pandas=True)\n        if df.empty:\n            continue\n        # Loss curve\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_loss\"], label=\"Train Loss\")\n        ax.plot(df[\"epoch\"], df[\"val_loss\"], label=\"Val Loss\")\n        ax.set_title(f\"Loss Curve - {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"loss_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n        # Accuracy curve\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_acc\"], label=\"Train Acc\")\n        ax.plot(df[\"epoch\"], df[\"val_acc\"], label=\"Val Acc\")\n        ax.set_title(f\"Accuracy Curve - {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"acc_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n\ndef _plot_comparison(summary: Dict[str, Dict], dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    df = pd.DataFrame.from_dict(summary, orient=\"index\")\n    df = df.reset_index().rename(columns={\"index\": \"run_id\"})\n\n    if df[\"best_val_acc\"].isnull().all():\n        return  # Nothing to plot\n\n    fig, ax = plt.subplots(figsize=(max(6, len(df) * 0.75), 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"best_val_acc\", ax=ax, palette=\"viridis\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_title(\"Best Validation Accuracy Across Runs\")\n    for p in ax.patches:\n        ax.annotate(\n            f\"{p.get_height():.2f}\",\n            (p.get_x() + p.get_width() / 2.0, p.get_height()),\n            ha=\"center\",\n            va=\"center\",\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )\n    fig.tight_layout()\n    fname = dest / \"comparison_best_val_acc.pdf\"\n    fig.savefig(fname)\n    print(fname.name)\n    plt.close(fig)\n\n\n# ---------------------------------------------------------------------------\n# Improvement metrics (proposed vs baseline)\n# ---------------------------------------------------------------------------\n\ndef _compute_improvement(summary: Dict[str, Dict]):\n    \"\"\"Computes improvement rate (proposed - baseline) / baseline per dataset.\n    Assumes run_id pattern contains dataset name and method (proposed / comparative).\"\"\"\n    records: Dict[str, Dict[str, float]] = {}\n    for run_id, metrics in summary.items():\n        dataset_name = run_id.split(\"-\")[-1]\n        method = \"proposed\" if run_id.startswith(\"proposed\") else \"baseline\"\n        records.setdefault(dataset_name, {})[method] = metrics[\"best_val_acc\"]\n\n    improvement = {\n        ds: {\n            \"baseline\": vals.get(\"baseline\"),\n            \"proposed\": vals.get(\"proposed\"),\n            \"improvement_rate\": (vals.get(\"proposed\") - vals.get(\"baseline\")) / vals.get(\"baseline\") if vals.get(\"baseline\") else None,\n        }\n        for ds, vals in records.items()\n    }\n    return improvement\n\n\n# ---------------------------------------------------------------------------\n# entrypoint\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate WandB experiments and generate figures.\")\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory containing experiment metadata\")\n    args = parser.parse_args()\n\n    results_path = Path(args.results_dir).absolute()\n    cfg_path = results_path / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"Config file not found at {cfg_path}\")\n\n    import yaml\n\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    entity = cfg[\"wandb\"][\"entity\"]\n    project = cfg[\"wandb\"][\"project\"]\n\n    runs = _fetch_runs(entity, project)\n    data_dir = results_path / DATA_DIR\n    summary = _export_metrics(runs, data_dir)\n\n    fig_dir = results_path / FIGURE_DIR\n    _plot_learning_curves(runs, fig_dir)\n    _plot_comparison(summary, fig_dir)\n\n    improvement = _compute_improvement(summary)\n    with open(results_path / DATA_DIR / \"improvement.json\", \"w\") as f:\n        json.dump(improvement, f, indent=2)\n", "main_py": "import subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom hydra.utils import get_original_cli_args\n\n\n@hydra.main(config_path=\"../config\")\ndef main(_cfg):\n    \"\"\"Main orchestrator \u2013 simply forwards all CLI overrides to src.train in a subprocess.\"\"\"\n    # Build command for subprocess; reuse exactly the CLI overrides provided originally.\n    original_cli: List[str] = get_original_cli_args()\n    # Remove the module specification (first element is run=..., etc.)\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + original_cli\n\n    print(\"Executing subprocess:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "import torch\nfrom torch import nn\nfrom transformers import DistilBertModel, AutoTokenizer\n\n\n# ---------------------------------------------------------------------------\n# Blocks\n# ---------------------------------------------------------------------------\n\nclass HoulsbyAdapter(nn.Module):\n    \"\"\"Implementation of the Houlsby adapter block.\"\"\"\n\n    def __init__(self, hidden_dim: int, reduction_factor: int = 16, non_linearity: str = \"relu\"):\n        super().__init__()\n        bottleneck_dim = max(1, hidden_dim // reduction_factor)\n        self.down = nn.Linear(hidden_dim, bottleneck_dim)\n        self.nonlin = getattr(nn, non_linearity.capitalize())() if hasattr(nn, non_linearity.capitalize()) else nn.ReLU()\n        self.up = nn.Linear(bottleneck_dim, hidden_dim)\n\n    def forward(self, x):  # x: (B, D)\n        return self.up(self.nonlin(self.down(x)))\n\n\nclass VisionPatchEmbedding(nn.Module):\n    \"\"\"Tiny patch embedding layer turning an image into a sequence of patch vectors.\"\"\"\n\n    def __init__(self, in_channels: int, patch_size: int, embed_dim: int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Linear(in_channels * patch_size * patch_size, embed_dim)\n\n    def forward(self, images):  # images: (B, C, H, W)\n        B, C, H, W = images.shape\n        p = self.patch_size\n        assert H % p == 0 and W % p == 0, \"Image size must be divisible by patch size\"\n        # unfold =\u003e B, C, n_patches_h, n_patches_w, p, p\n        patches = images.unfold(2, p, p).unfold(3, p, p)\n        patches = patches.contiguous().view(B, C, -1, p, p)\n        patches = patches.permute(0, 2, 1, 3, 4).contiguous()  # B, N, C, p, p\n        patches = patches.view(B, patches.size(1), -1)  # B, N, C*p*p\n        return self.proj(patches)  # B, N, D\n\n\n# ---------------------------------------------------------------------------\n# DistilBERT classifier (multi-modal)\n# ---------------------------------------------------------------------------\n\nclass DistilBertClassifier(nn.Module):\n    def __init__(\n        self,\n        model_name: str,\n        num_labels: int,\n        adapter_cfg=None,\n        vision_patch_cfg=None,\n    ):\n        super().__init__()\n        self.is_vision = vision_patch_cfg is not None\n        self.bert = DistilBertModel.from_pretrained(model_name, cache_dir=\".cache/\")\n        self.hidden_dim = self.bert.config.hidden_size\n\n        # Vision embedding when images are used\n        if self.is_vision:\n            self.vision_embedding = VisionPatchEmbedding(\n                in_channels=3,\n                patch_size=vision_patch_cfg.patch_size,\n                embed_dim=self.hidden_dim,\n            )\n        else:\n            self.vision_embedding = None\n\n        # Optional adapter\n        if adapter_cfg and adapter_cfg.get(\"enabled\", False):\n            self.adapter = HoulsbyAdapter(\n                hidden_dim=self.hidden_dim,\n                reduction_factor=adapter_cfg.reduction_factor,\n                non_linearity=adapter_cfg.non_linearity,\n            )\n        else:\n            self.adapter = None\n\n        # Classification head\n        self.pre_classifier = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.activation = nn.GELU()\n        self.dropout = nn.Dropout(0.2)\n        self.classifier = nn.Linear(self.hidden_dim, num_labels)\n\n    def forward(self, batch):\n        if self.is_vision and \"pixel_values\" in batch:\n            embeddings = self.vision_embedding(batch[\"pixel_values\"])  # (B, N, D)\n            attention_mask = torch.ones(embeddings.size()[:-1], dtype=torch.long, device=embeddings.device)\n            out = self.bert(inputs_embeds=embeddings, attention_mask=attention_mask)\n        else:\n            out = self.bert(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n            )\n        pooled = out.last_hidden_state[:, 0]  # first token / CLS alike\n\n        # Adapter residual\n        if self.adapter is not None:\n            pooled = pooled + self.adapter(pooled)\n\n        x = self.pre_classifier(pooled)\n        x = self.activation(x)\n        x = self.dropout(x)\n        logits = self.classifier(x)\n        return logits\n\n\n# ---------------------------------------------------------------------------\n# Builder\n# ---------------------------------------------------------------------------\n\ndef build_model(cfg, tokenizer=None):\n    vision_patch_cfg = (\n        cfg.model.vision_patch if cfg.model.get(\"input_representation\", \"text\") == \"image_sequence\" else None\n    )\n    adapter_cfg = cfg.model.task_adapters if cfg.method == \"proposed\" else None\n    model = DistilBertClassifier(\n        model_name=cfg.model.name,\n        num_labels=cfg.model.num_labels,\n        adapter_cfg=adapter_cfg,\n        vision_patch_cfg=vision_patch_cfg,\n    )\n    return model\n", "preprocess_py": "\"\"\"Complete preprocessing utilities for all datasets used in the study.\"\"\"\n\nfrom typing import Tuple, Optional, List\nimport random\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import datasets as tv_datasets, transforms\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n\n# ---------------------------------------------------------------------------\n# CIFAR-10 (vision) helpers\n# ---------------------------------------------------------------------------\n\n\nclass _CIFARWrapper(torch.utils.data.Dataset):\n    \"\"\"Wraps torchvision CIFAR instances returning dicts the model expects.\"\"\"\n\n    def __init__(self, base, tfm):\n        self.base = base\n        self.tfm = tfm\n\n    def __len__(self):\n        return len(self.base)\n\n    def __getitem__(self, idx):\n        img, label = self.base[idx]\n        img = self.tfm(img)\n        return {\n            \"pixel_values\": img,  # Tensor C\u00d7H\u00d7W\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\ndef _split(dataset: torch.utils.data.Dataset, splits: Tuple[float, float, float]):\n    n_total = len(dataset)\n    lengths = [int(p * n_total) for p in splits]\n    lengths[-1] = n_total - sum(lengths[:-1])  # adjust rounding\n    return random_split(dataset, lengths, generator=torch.Generator().manual_seed(42))\n\n\ndef _build_cifar10(cfg, trial_mode: bool):\n    resize_size = cfg.dataset.preprocessing.resize\n    augmentations: List[str] = cfg.dataset.preprocessing.augmentations\n\n    tfm_train = [\n        transforms.RandomCrop(resize_size, padding=4) if \"random_crop\" in augmentations else transforms.Resize(resize_size),\n        transforms.RandomHorizontalFlip() if \"horizontal_flip\" in augmentations else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ]\n    tfm_eval = [transforms.Resize(resize_size), transforms.ToTensor()]\n\n    train_dataset = tv_datasets.CIFAR10(root=\".cache/\", train=True, download=True)\n    test_dataset = tv_datasets.CIFAR10(root=\".cache/\", train=False, download=True)\n    full = ConcatDataset([train_dataset, test_dataset])\n    train_set, val_set, test_set = _split(full, (\n        cfg.dataset.split.train,\n        cfg.dataset.split.val,\n        cfg.dataset.split.test,\n    ))\n\n    train_set = _CIFARWrapper(train_set, transforms.Compose(tfm_train))\n    val_set = _CIFARWrapper(val_set, transforms.Compose(tfm_eval))\n    test_set = _CIFARWrapper(test_set, transforms.Compose(tfm_eval))\n\n    batch_size = 2 if trial_mode else cfg.training.batch_size\n    dl_kwargs = dict(batch_size=batch_size, num_workers=2, pin_memory=True)\n\n    return (\n        DataLoader(train_set, shuffle=True, **dl_kwargs),\n        DataLoader(val_set, shuffle=False, **dl_kwargs),\n        DataLoader(test_set, shuffle=False, **dl_kwargs),\n        None,  # tokenizer not used\n    )\n\n\n# ---------------------------------------------------------------------------\n# Alpaca-cleaned (text) helpers\n# ---------------------------------------------------------------------------\n\n\ndef _label_from_instruction(instr: str) -\u003e int:\n    \"\"\"Binary label: 1 if instruction requests translation else 0.\"\"\"\n    lowered = instr.lower()\n    keywords = [\"translate\", \"translation\", \"translating\", \"translator\"]\n    return int(any(k in lowered for k in keywords))\n\n\ndef _encode_text(batch, tokenizer, max_len):\n    instructions = batch[\"instruction\"]\n    inputs = batch.get(\"input\", [\"\"] * len(instructions))\n    texts = [f\"Instruction: {ins} Input: {inp}\" for ins, inp in zip(instructions, inputs)]\n    encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n    encodings[\"labels\"] = [_label_from_instruction(ins) for ins in instructions]\n    return encodings\n\n\ndef _build_alpaca(cfg, trial_mode: bool):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    raw_ds = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", cache_dir=\".cache/\")\n\n    encoded = raw_ds.map(\n        lambda batch: _encode_text(batch, tokenizer, cfg.dataset.max_length),\n        batched=True,\n        remove_columns=raw_ds.column_names,\n    )\n    encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    train_size = int(cfg.dataset.split.train * len(encoded))\n    val_size = int(cfg.dataset.split.val * len(encoded))\n    test_size = len(encoded) - train_size - val_size\n    train_ds, val_ds, test_ds = random_split(\n        encoded,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42),\n    )\n\n    batch_size = 2 if trial_mode else cfg.training.batch_size\n    dl_kwargs = dict(batch_size=batch_size, num_workers=2, pin_memory=True)\n\n    return (\n        DataLoader(train_ds, shuffle=True, **dl_kwargs),\n        DataLoader(val_ds, shuffle=False, **dl_kwargs),\n        DataLoader(test_ds, shuffle=False, **dl_kwargs),\n        tokenizer,\n    )\n\n\n# ---------------------------------------------------------------------------\n# Public factory\n# ---------------------------------------------------------------------------\n\ndef build_dataloaders(cfg, trial_mode: bool = False):\n    name = cfg.dataset.name.lower()\n    if name == \"cifar10\":\n        return _build_cifar10(cfg, trial_mode)\n    elif name == \"alpaca-cleaned\":\n        return _build_alpaca(cfg, trial_mode)\n    else:\n        raise ValueError(f\"Unsupported dataset {name}\")\n", "pyproject_toml": "[project]\nname = \"distilbert-adapter-exp\"\nversion = \"0.1.0\"\ndescription = \"Experiment code for DistilBERT adapters across tasks with Hydra, WandB and Optuna\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntransformers = \"\u003e=4.35.0\"\ndatasets = \"\u003e=2.14.0\"\ntorchvision = \"\u003e=0.15.0\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.15.0\"\nscikit-learn = \"^1.3.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\noptuna = \"^3.3.0\"\npandas = \"^2.1.0\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "import os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Tuple, Optional\n\nimport hydra\nimport optuna\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import OmegaConf\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import get_linear_schedule_with_warmup\nimport wandb\n\n# Local imports (absolute because executed with -m from repo root)\nfrom src.preprocess import build_dataloaders\nfrom src.model import build_model\n\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef _set_seed(seed: int = 42):\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _save_cfg(cfg, results_dir: Path):\n    results_dir.mkdir(parents=True, exist_ok=True)\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        OmegaConf.save(config=cfg, f=str(cfg_path))\n\n\n# ---------------------------------------------------------------------------\n# Core training / evaluation routine (single pass)\n# ---------------------------------------------------------------------------\n\ndef _train_eval(cfg, trial: Optional[optuna.Trial] = None, verbose: bool = True) -\u003e Dict[str, Any]:\n    \"\"\"Train for cfg.training.epochs (or cfg.training.optuna_epochs if supplied when called from Optuna).\n\n    When called inside Optuna, trial is not None; in that case WandB is disabled, and\n    we can optionally shorten training via cfg.optuna.max_epochs.\n    Returns a dict with at least keys \"val_acc\" and \"val_f1\".\n    \"\"\"\n    trial_mode: bool = cfg.get(\"trial_mode\", False)\n\n    # ---------------------------------------------------------------------\n    # Dataloaders / Model\n    # ---------------------------------------------------------------------\n    train_loader, val_loader, _, tokenizer = build_dataloaders(cfg, trial_mode=trial_mode)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = build_model(cfg, tokenizer)\n    model.to(device)\n\n    # ---------------------------------------------------------------------\n    # Optimiser + scheduler\n    # ---------------------------------------------------------------------\n    optim_cls = {\n        \"adamw\": torch.optim.AdamW,\n        \"sgd\": torch.optim.SGD,\n    }.get(cfg.training.optimizer.lower())\n    if optim_cls is None:\n        raise ValueError(f\"Unsupported optimizer {cfg.training.optimizer}\")\n\n    optimizer = optim_cls(\n        model.parameters(),\n        lr=cfg.training.learning_rate,\n        weight_decay=cfg.training.weight_decay,\n    )\n    total_steps = len(train_loader) * cfg.training.epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.training.warmup_steps,\n        num_training_steps=total_steps,\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    # ---------------------------------------------------------------------\n    # WandB (only outside Optuna trials and not in trial_mode when disabled)\n    # ---------------------------------------------------------------------\n    use_wandb = (trial is None) and (cfg.wandb.mode != \"disabled\") and (not trial_mode)\n    if use_wandb:\n        run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n        )\n        if verbose:\n            print(f\"WandB URL: {wandb.run.get_url()}\")\n    else:\n        run = None\n\n    # ---------------------------------------------------------------------\n    # Training loop\n    # ---------------------------------------------------------------------\n    epochs = (\n        1 if trial_mode else cfg.training.epochs\n        if trial is None else cfg.optuna.get(\"max_epochs\", 3)\n    )\n    global_step = 0\n    best_val_acc = 0.0\n    best_val_f1 = 0.0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_losses: List[float] = []\n        epoch_true, epoch_pred = [], []\n\n        for step, batch in enumerate(train_loader):\n            # In trial_mode keep it lightweight\n            if trial_mode and step \u003e 1:\n                break\n\n            labels = batch.pop(\"labels\").to(device)\n            for k in batch:\n                batch[k] = batch[k].to(device)\n            outputs = model(batch)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n            epoch_losses.append(loss.item())\n            pred_labels = outputs.argmax(dim=-1).detach().cpu().tolist()\n            epoch_pred.extend(pred_labels)\n            epoch_true.extend(labels.detach().cpu().tolist())\n            global_step += 1\n\n        train_loss = float(sum(epoch_losses) / max(1, len(epoch_losses)))\n        train_acc = accuracy_score(epoch_true, epoch_pred)\n        train_f1 = f1_score(epoch_true, epoch_pred, average=\"weighted\")\n\n        # -------------------------- Validation ---------------------------\n        model.eval()\n        val_losses: List[float] = []\n        val_true, val_pred = [], []\n        start_inf = time.time()\n        with torch.no_grad():\n            for step, batch in enumerate(val_loader):\n                if trial_mode and step \u003e 1:\n                    break\n                labels = batch.pop(\"labels\").to(device)\n                for k in batch:\n                    batch[k] = batch[k].to(device)\n                outputs = model(batch)\n                loss = criterion(outputs, labels)\n                val_losses.append(loss.item())\n                preds = outputs.argmax(dim=-1).detach().cpu().tolist()\n                val_pred.extend(preds)\n                val_true.extend(labels.detach().cpu().tolist())\n        inference_time = (time.time() - start_inf) / max(1, len(val_loader))\n        val_loss = float(sum(val_losses) / max(1, len(val_losses)))\n        val_acc = accuracy_score(val_true, val_pred)\n        val_f1 = f1_score(val_true, val_pred, average=\"weighted\")\n\n        best_val_acc = max(best_val_acc, val_acc)\n        best_val_f1 = max(best_val_f1, val_f1)\n\n        log_dict = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"train_acc\": train_acc,\n            \"val_acc\": val_acc,\n            \"train_f1\": train_f1,\n            \"val_f1\": val_f1,\n            \"best_val_acc\": best_val_acc,\n            \"best_val_f1\": best_val_f1,\n            \"inference_time\": inference_time,\n        }\n        if use_wandb:\n            wandb.log(log_dict, step=epoch)\n        if verbose:\n            print(f\"Epoch {epoch}: {log_dict}\")\n\n    if run is not None:\n        wandb.finish()\n\n    return {\n        \"best_val_acc\": best_val_acc,\n        \"best_val_f1\": best_val_f1,\n    }\n\n\n# ---------------------------------------------------------------------------\n# Optuna integration\n# ---------------------------------------------------------------------------\n\ndef _suggest_and_apply(cfg, trial: optuna.Trial, param_map: Dict[str, List[str]]):\n    \"\"\"Apply Optuna suggestions to cfg in-place according to search_space and param_map.\"\"\"\n    for param_name, space in cfg.optuna.search_space.items():\n        # Determine value\n        if space.type == \"loguniform\":\n            val = trial.suggest_float(param_name, space.low, space.high, log=True)\n        elif space.type == \"uniform\":\n            val = trial.suggest_float(param_name, space.low, space.high, log=False)\n        elif space.type == \"categorical\":\n            val = trial.suggest_categorical(param_name, space.choices)\n        elif space.type == \"int\":\n            val = trial.suggest_int(param_name, space.low, space.high, step=space.get(\"step\", 1))\n        else:\n            raise ValueError(f\"Unsupported search space type: {space.type}\")\n\n        # Apply value into cfg following param_map\n        if param_name not in param_map:\n            raise KeyError(f\"Param {param_name} not present in param_map for config update\")\n        ptr = cfg\n        keys = param_map[param_name]\n        for k in keys[:-1]:\n            ptr = ptr[k]\n        ptr[keys[-1]] = val\n\n\n# ---------------------------------------------------------------------------\n# main entrypoint\n# ---------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):  # noqa: C901  (function is intentionally long for clarity)\n    _set_seed()\n\n    trial_mode: bool = cfg.get(\"trial_mode\", False)\n    results_dir = Path(cfg.results_dir)\n    _save_cfg(cfg, results_dir)\n\n    # If Optuna requested and not disabled via n_trials = 0 or trial_mode, run optimisation.\n    if (cfg.optuna.n_trials \u003e 0) and (not trial_mode):\n        param_map = {\n            \"learning_rate\": [\"training\", \"learning_rate\"],\n            \"batch_size\": [\"training\", \"batch_size\"],\n            \"weight_decay\": [\"training\", \"weight_decay\"],\n            \"adapter_reduction_factor\": [\"model\", \"task_adapters\", \"reduction_factor\"],\n        }\n\n        study_path = results_dir / \"optuna_study.db\"\n        storage_str = f\"sqlite:///{study_path}\"\n        study = optuna.create_study(\n            direction=cfg.optuna.direction,\n            storage=storage_str,\n            study_name=cfg.run.run_id,\n            load_if_exists=True,\n        )\n\n        def objective(trial: optuna.Trial):\n            # Work on a deep copy of cfg to avoid polluting it across trials\n            trial_cfg = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n            _suggest_and_apply(trial_cfg, trial, param_map)\n            # Disable WandB inside objective to keep it lightweight\n            trial_cfg.wandb.mode = \"disabled\"\n            # Reduce epochs if user specified optuna.max_epochs\n            trial_cfg.optuna.max_epochs = cfg.optuna.get(\"max_epochs\", 3)\n            metrics = _train_eval(trial_cfg, trial=trial, verbose=False)\n            # We optimise best_val_acc by default\n            return metrics[\"best_val_acc\"]\n\n        study.optimize(objective, n_trials=cfg.optuna.n_trials, gc_after_trial=True)\n        best_params = study.best_params\n        # Apply best params to cfg for final full training\n        for k, v in best_params.items():\n            if k == \"learning_rate\":\n                cfg.training.learning_rate = v\n            elif k == \"batch_size\":\n                cfg.training.batch_size = v\n            elif k == \"weight_decay\":\n                cfg.training.weight_decay = v\n            elif k == \"adapter_reduction_factor\":\n                cfg.model.task_adapters.reduction_factor = v\n        # Persist study for inspection\n        study_path.write_text(str(study.best_trial))\n        print(f\"Optuna best params: {best_params}\")\n\n    # ---------------- Final full training \u0026 logging -----------------------\n    _train_eval(cfg, trial=None, verbose=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
cpu
RAM：500 MB

# Current Research Method
We compare DistilBERT with task-specific adapters (proposed) against standard fine-tuning (comparative) across vision and language tasks to evaluate performance improvements.

# Experimental Design
- Summary: Comparative analysis of DistilBERT performance across vision and language tasks
- Evaluation metrics: ['accuracy', 'f1_score', 'inference_time']

# Experiment Runs

- Run ID: proposed-DistilBERT-base-66M-CIFAR-10
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  Config File: config/run/proposed-DistilBERT-base-66M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-CIFAR-10
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 1e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: proposed-DistilBERT-base-66M-alpaca-cleaned
  Method: proposed
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  Config File: config/run/proposed-DistilBERT-base-66M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-DistilBERT-base-66M-alpaca-cleaned
method: proposed
model:
  name: distilbert-base-66M
  task_adapters:
    enabled: true
    adapter_type: houlsby
    reduction_factor: 16
    non_linearity: relu
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 5e-5
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    adapter_reduction_factor:
      type: int
      low: 4
      high: 32
      step: 4
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-CIFAR-10
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  Config File: config/run/comparative-1-DistilBERT-base-66M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-CIFAR-10
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 10
  input_representation: image_sequence
  vision_patch:
    patch_size: 4
dataset:
  name: cifar10
  split:
    train: 0.8
    val: 0.1
    test: 0.1
  preprocessing:
    resize: 32
    augmentations:
      - random_crop
      - horizontal_flip
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 16
  epochs: 20
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices:
        - 8
        - 16
        - 32
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  

- Run ID: comparative-1-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-1
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-1-DistilBERT-base-66M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-DistilBERT-base-66M-alpaca-cleaned
method: comparative-1
model:
  name: distilbert-base-66M
  adapter: null
  num_labels: 2
dataset:
  name: alpaca-cleaned
  max_length: 512
  split:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  optimizer: adamw
  learning_rate: 2e-4
  batch_size: 8
  epochs: 3
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
evaluation:
  metrics:
    - accuracy
    - f1_score
    - inference_time
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 3e-4
    batch_size:
      type: categorical
      choices:
        - 4
        - 8
        - 16
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1

  ```
  


# External Resources (Use these for implementation)


Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "import os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Optional\n\nimport hydra\nimport optuna\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import OmegaConf\n\nimport torch\nfrom torch import nn\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import get_linear_schedule_with_warmup\nimport wandb\n\n# Local imports (executed with -m from repo root)\nfrom src.preprocess import build_dataloaders\nfrom src.model import build_model\n\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef _set_seed(seed: int = 42):\n    import random\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _save_cfg(cfg, results_dir: Path):\n    results_dir.mkdir(parents=True, exist_ok=True)\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        OmegaConf.save(config=cfg, f=str(cfg_path))\n\n\n# ---------------------------------------------------------------------------\n# Core train / eval\n# ---------------------------------------------------------------------------\n\ndef _train_eval(cfg, trial: Optional[optuna.Trial] = None, verbose: bool = True) -> Dict[str, Any]:\n    \"\"\"Train one full run (or shortened trial) and return best metrics.\"\"\"\n\n    trial_mode: bool = cfg.get(\"trial_mode\", False)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ---------------------------------------------------------------------\n    # Data & model\n    # ---------------------------------------------------------------------\n    train_loader, val_loader, _, tokenizer = build_dataloaders(cfg, trial_mode=trial_mode)\n    model = build_model(cfg, tokenizer).to(device)\n\n    # Optimiser & scheduler ------------------------------------------------\n    opt_map = {\"adamw\": torch.optim.AdamW, \"sgd\": torch.optim.SGD}\n    optim_cls = opt_map.get(cfg.training.optimizer.lower())\n    if optim_cls is None:\n        raise ValueError(f\"Unsupported optimizer {cfg.training.optimizer}\")\n\n    optimizer = optim_cls(model.parameters(), lr=cfg.training.learning_rate, weight_decay=cfg.training.weight_decay)\n    total_steps = len(train_loader) * cfg.training.epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.training.warmup_steps,\n        num_training_steps=total_steps,\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    # WandB ---------------------------------------------------------------\n    use_wandb = (trial is None) and (cfg.wandb.mode != \"disabled\") and (not trial_mode)\n    if use_wandb:\n        run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n        )\n        if verbose:\n            print(f\"WandB URL: {wandb.run.get_url()}\")\n    else:\n        run = None\n\n    # Epoch loop ----------------------------------------------------------\n    epochs = 1 if trial_mode else cfg.training.epochs if trial is None else cfg.optuna.get(\"max_epochs\", 3)\n    global_step = 0\n    best_val_acc, best_val_f1 = 0.0, 0.0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_losses: List[float] = []\n        epoch_true, epoch_pred = [], []\n\n        for step, batch in enumerate(train_loader):\n            if trial_mode and step > 1:\n                break\n            labels = batch.pop(\"labels\").to(device)\n            for k in batch:\n                batch[k] = batch[k].to(device)\n            outputs = model(batch)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n            epoch_losses.append(loss.item())\n            epoch_pred.extend(outputs.argmax(dim=-1).detach().cpu().tolist())\n            epoch_true.extend(labels.detach().cpu().tolist())\n            global_step += 1\n\n        train_loss = float(sum(epoch_losses) / max(1, len(epoch_losses)))\n        train_acc = accuracy_score(epoch_true, epoch_pred)\n        train_f1 = f1_score(epoch_true, epoch_pred, average=\"weighted\")\n\n        # ------------------------ Validation -----------------------------\n        model.eval()\n        val_losses: List[float] = []\n        val_true, val_pred = [], []\n        start_inf = time.time()\n        with torch.no_grad():\n            for step, batch in enumerate(val_loader):\n                if trial_mode and step > 1:\n                    break\n                labels = batch.pop(\"labels\").to(device)\n                for k in batch:\n                    batch[k] = batch[k].to(device)\n                outputs = model(batch)\n                val_losses.append(criterion(outputs, labels).item())\n                val_pred.extend(outputs.argmax(dim=-1).detach().cpu().tolist())\n                val_true.extend(labels.detach().cpu().tolist())\n        inference_time = (time.time() - start_inf) / max(1, len(val_loader))\n        val_loss = float(sum(val_losses) / max(1, len(val_losses)))\n        val_acc = accuracy_score(val_true, val_pred)\n        val_f1 = f1_score(val_true, val_pred, average=\"weighted\")\n\n        best_val_acc = max(best_val_acc, val_acc)\n        best_val_f1 = max(best_val_f1, val_f1)\n\n        log_dict = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"train_acc\": train_acc,\n            \"val_acc\": val_acc,\n            \"train_f1\": train_f1,\n            \"val_f1\": val_f1,\n            \"best_val_acc\": best_val_acc,\n            \"best_val_f1\": best_val_f1,\n            \"inference_time\": inference_time,\n        }\n        if use_wandb:\n            wandb.log(log_dict, step=epoch)\n        if verbose:\n            print(f\"Epoch {epoch}: {log_dict}\")\n\n    if run is not None:\n        wandb.finish()\n\n    return {\"best_val_acc\": best_val_acc, \"best_val_f1\": best_val_f1}\n\n\n# ---------------------------------------------------------------------------\n# Optuna helpers\n# ---------------------------------------------------------------------------\n\ndef _suggest_and_apply(cfg, trial: optuna.Trial, param_map: Dict[str, List[str]]):\n    \"\"\"Apply Optuna suggestions to cfg in-place according to defined search space.\"\"\"\n    for param_name, space in cfg.optuna.search_space.items():\n        if space.type == \"loguniform\":\n            val = trial.suggest_float(param_name, space.low, space.high, log=True)\n        elif space.type == \"uniform\":\n            val = trial.suggest_float(param_name, space.low, space.high)\n        elif space.type == \"categorical\":\n            val = trial.suggest_categorical(param_name, space.choices)\n        elif space.type == \"int\":\n            val = trial.suggest_int(param_name, space.low, space.high, step=space.get(\"step\", 1))\n        else:\n            raise ValueError(f\"Unsupported search space type: {space.type}\")\n\n        # propagate value into cfg following param_map\n        ptr = cfg\n        for k in param_map[param_name][:-1]:\n            ptr = ptr[k]\n        ptr[param_map[param_name][-1]] = val\n\n\n# ---------------------------------------------------------------------------\n# Entrypoint\n# ---------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):  # noqa: C901\n    _set_seed()\n\n    # Ensure wandb.mode is disabled for trial_mode -------------------------\n    if cfg.get(\"trial_mode\", False):\n        cfg.wandb.mode = \"disabled\"\n\n    results_dir = Path(cfg.results_dir)\n    _save_cfg(cfg, results_dir)\n\n    # ------------------ Hyper-parameter optimisation ---------------------\n    if (cfg.optuna.n_trials > 0) and (not cfg.get(\"trial_mode\", False)):\n        param_map = {\n            \"learning_rate\": [\"training\", \"learning_rate\"],\n            \"batch_size\": [\"training\", \"batch_size\"],\n            \"weight_decay\": [\"training\", \"weight_decay\"],\n            \"adapter_reduction_factor\": [\"model\", \"task_adapters\", \"reduction_factor\"],\n        }\n        study_path = results_dir / \"optuna_study.db\"\n        study = optuna.create_study(\n            direction=cfg.optuna.direction,\n            storage=f\"sqlite:///{study_path}\",\n            study_name=cfg.run.run_id,\n            load_if_exists=True,\n        )\n\n        def objective(trial: optuna.Trial):\n            trial_cfg = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n            _suggest_and_apply(trial_cfg, trial, param_map)\n            trial_cfg.wandb.mode = \"disabled\"  # always disable inside trials\n            trial_cfg.optuna.max_epochs = cfg.optuna.get(\"max_epochs\", 3)\n            metrics = _train_eval(trial_cfg, trial=trial, verbose=False)\n            return metrics[\"best_val_acc\"]\n\n        study.optimize(objective, n_trials=cfg.optuna.n_trials, gc_after_trial=True)\n        print(f\"Optuna best params: {study.best_params}\")\n        # apply best params to cfg for final training\n        for k, v in study.best_params.items():\n            if k == \"adapter_reduction_factor\":\n                cfg.model.task_adapters.reduction_factor = v\n            elif k in {\"learning_rate\", \"batch_size\", \"weight_decay\"}:\n                cfg.training[k] = v\n\n    # ---------------------- Final full training --------------------------\n    _train_eval(cfg, trial=None, verbose=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"Independent evaluation: fetch WandB runs, export metrics and generate figures.\"\"\"\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Dict\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\n\nFIGURE_DIR = \"images\"\nDATA_DIR = \"wandb_data\"\n\n\n# ---------------------------------------------------------------------------\n# CLI helper (supports key=value style)\n# ---------------------------------------------------------------------------\n\ndef _parse_results_dir(argv) -> Path:\n    \"\"\"Extract results_dir from argv supporting `results_dir=/foo/bar` style.\"\"\"\n    for arg in argv[1:]:\n        if arg.startswith(\"results_dir=\"):\n            return Path(arg.split(\"=\", 1)[1]).expanduser().absolute()\n    # fallback to positional first argument\n    if len(argv) > 1:\n        return Path(argv[1]).expanduser().absolute()\n    raise ValueError(\"results_dir argument is required (e.g., results_dir=outputs/)\")\n\n\n# ---------------------------------------------------------------------------\n# WandB helpers\n# ---------------------------------------------------------------------------\n\ndef _fetch_runs(entity: str, project: str):\n    api = wandb.Api()\n    return api.runs(f\"{entity}/{project}\")\n\n\ndef _export_metrics(runs, out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    summaries: Dict[str, Dict] = {}\n    needed_cols = [\n        \"epoch\",\n        \"best_val_acc\",\n        \"best_val_f1\",\n        \"train_loss\",\n        \"val_loss\",\n        \"val_acc\",\n        \"val_f1\",\n    ]\n    for run in runs:\n        df = run.history(keys=needed_cols, pandas=True)\n        df.to_csv(out_dir / f\"run_{run.name}_metrics.csv\", index=False)\n        summaries[run.name] = {\n            \"best_val_acc\": df[\"best_val_acc\"].dropna().max() if \"best_val_acc\" in df else None,\n            \"best_val_f1\": df[\"best_val_f1\"].dropna().max() if \"best_val_f1\" in df else None,\n        }\n    with open(out_dir / \"summary.json\", \"w\") as f:\n        json.dump(summaries, f, indent=2)\n    return summaries\n\n\n# ---------------------------------------------------------------------------\n# Plotting\n# ---------------------------------------------------------------------------\n\ndef _plot_learning_curves(runs, dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    for run in runs:\n        df = run.history(keys=[\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"], pandas=True)\n        if df.empty:\n            continue\n        # Loss curve\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_loss\"], label=\"Train Loss\")\n        ax.plot(df[\"epoch\"], df[\"val_loss\"], label=\"Val Loss\")\n        ax.set_title(f\"Loss Curve - {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"loss_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n        # Accuracy curve\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(df[\"epoch\"], df[\"train_acc\"], label=\"Train Acc\")\n        ax.plot(df[\"epoch\"], df[\"val_acc\"], label=\"Val Acc\")\n        ax.set_title(f\"Accuracy Curve - {run.name}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.legend()\n        fig.tight_layout()\n        fname = dest / f\"acc_curve_{run.name}.pdf\"\n        fig.savefig(fname)\n        print(fname.name)\n        plt.close(fig)\n\n\ndef _plot_comparison(summary: Dict[str, Dict], dest: Path):\n    dest.mkdir(parents=True, exist_ok=True)\n    df = pd.DataFrame.from_dict(summary, orient=\"index\").reset_index().rename(columns={\"index\": \"run_id\"})\n    if df[\"best_val_acc\"].isnull().all():\n        return\n    fig, ax = plt.subplots(figsize=(max(6, len(df) * 0.75), 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"best_val_acc\", ax=ax, palette=\"viridis\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_title(\"Best Validation Accuracy Across Runs\")\n    for p in ax.patches:\n        ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                    ha=\"center\", va=\"center\", xytext=(0, 5), textcoords=\"offset points\")\n    fig.tight_layout()\n    fname = dest / \"comparison_best_val_acc.pdf\"\n    fig.savefig(fname)\n    print(fname.name)\n    plt.close(fig)\n\n\n# ---------------------------------------------------------------------------\n# Improvement computation\n# ---------------------------------------------------------------------------\n\ndef _compute_improvement(summary: Dict[str, Dict]):\n    records: Dict[str, Dict[str, float]] = {}\n    for run_id, metrics in summary.items():\n        dataset_name = run_id.split(\"-\")[-1]\n        method = \"proposed\" if run_id.startswith(\"proposed\") else \"baseline\"\n        records.setdefault(dataset_name, {})[method] = metrics[\"best_val_acc\"]\n\n    improvement = {ds: {**vals, \"improvement_rate\": (vals.get(\"proposed\") - vals.get(\"baseline\")) / vals.get(\"baseline\")\n                        if vals.get(\"baseline\") else None}\n                   for ds, vals in records.items()}\n    return improvement\n\n\n# ---------------------------------------------------------------------------\n# Entrypoint\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    results_path = _parse_results_dir(sys.argv)\n\n    cfg_path = results_path / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"Config file not found at {cfg_path}\")\n\n    import yaml\n    with open(cfg_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    entity = cfg[\"wandb\"][\"entity\"]\n    project = cfg[\"wandb\"][\"project\"]\n\n    runs = _fetch_runs(entity, project)\n    summary = _export_metrics(runs, results_path / DATA_DIR)\n\n    _plot_learning_curves(runs, results_path / FIGURE_DIR)\n    _plot_comparison(summary, results_path / FIGURE_DIR)\n\n    improvement = _compute_improvement(summary)\n    with open(results_path / DATA_DIR / \"improvement.json\", \"w\") as f:\n        json.dump(improvement, f, indent=2)\n",
    "preprocess_py": "\"\"\"Dataset preprocessing utilities.\"\"\"\nfrom typing import Tuple, List\nimport random\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import datasets as tv_datasets, transforms\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n\n# ---------------------------------------------------------------------------\n# CIFAR-10 helpers\n# ---------------------------------------------------------------------------\n\nclass _CIFARWrapper(torch.utils.data.Dataset):\n    def __init__(self, base, tfm):\n        self.base = base\n        self.tfm = tfm\n\n    def __len__(self):\n        return len(self.base)\n\n    def __getitem__(self, idx):\n        img, label = self.base[idx]\n        img = self.tfm(img)\n        return {\"pixel_values\": img, \"labels\": torch.tensor(label, dtype=torch.long)}\n\n\ndef _split(dataset: torch.utils.data.Dataset, splits: Tuple[float, float, float]):\n    n_total = len(dataset)\n    lengths = [int(p * n_total) for p in splits]\n    lengths[-1] = n_total - sum(lengths[:-1])\n    return random_split(dataset, lengths, generator=torch.Generator().manual_seed(42))\n\n\ndef _build_cifar10(cfg, trial_mode: bool):\n    size = cfg.dataset.preprocessing.resize\n    aug: List[str] = cfg.dataset.preprocessing.augmentations\n    tfm_train = [transforms.RandomCrop(size, padding=4) if \"random_crop\" in aug else transforms.Resize(size),\n                 transforms.RandomHorizontalFlip() if \"horizontal_flip\" in aug else transforms.Lambda(lambda x: x),\n                 transforms.ToTensor()]\n    tfm_eval = [transforms.Resize(size), transforms.ToTensor()]\n\n    train_ds = tv_datasets.CIFAR10(root=\".cache/\", train=True, download=True)\n    test_ds = tv_datasets.CIFAR10(root=\".cache/\", train=False, download=True)\n    full = ConcatDataset([train_ds, test_ds])\n    train_set, val_set, test_set = _split(full, (cfg.dataset.split.train, cfg.dataset.split.val, cfg.dataset.split.test))\n\n    train_set = _CIFARWrapper(train_set, transforms.Compose(tfm_train))\n    val_set = _CIFARWrapper(val_set, transforms.Compose(tfm_eval))\n    test_set = _CIFARWrapper(test_set, transforms.Compose(tfm_eval))\n\n    bs = 2 if trial_mode else cfg.training.batch_size\n    dl_kwargs = dict(batch_size=bs, num_workers=2, pin_memory=True)\n    return (DataLoader(train_set, shuffle=True, **dl_kwargs),\n            DataLoader(val_set, shuffle=False, **dl_kwargs),\n            DataLoader(test_set, shuffle=False, **dl_kwargs),\n            None)\n\n\n# ---------------------------------------------------------------------------\n# Alpaca-cleaned helpers\n# ---------------------------------------------------------------------------\n\ndef _label_from_instruction(instr: str) -> int:\n    lowered = instr.lower()\n    keywords = [\"translate\", \"translation\", \"translating\", \"translator\"]\n    return int(any(k in lowered for k in keywords))\n\n\ndef _encode_text(batch, tokenizer, max_len):\n    instructions = batch[\"instruction\"]\n    inputs = batch.get(\"input\", [\"\"] * len(instructions))\n    texts = [f\"Instruction: {ins} Input: {inp}\" for ins, inp in zip(instructions, inputs)]\n    enc = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n    enc[\"labels\"] = [_label_from_instruction(ins) for ins in instructions]\n    return enc\n\n\ndef _build_alpaca(cfg, trial_mode: bool):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    raw = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", cache_dir=\".cache/\")\n\n    encoded = raw.map(lambda b: _encode_text(b, tokenizer, cfg.dataset.max_length), batched=True,\n                      remove_columns=raw.column_names)\n    encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    n = len(encoded)\n    train_size = int(cfg.dataset.split.train * n)\n    val_size = int(cfg.dataset.split.val * n)\n    test_size = n - train_size - val_size\n    train_ds, val_ds, test_ds = random_split(encoded, [train_size, val_size, test_size],\n                                             generator=torch.Generator().manual_seed(42))\n\n    bs = 2 if trial_mode else cfg.training.batch_size\n    dl_kwargs = dict(batch_size=bs, num_workers=2, pin_memory=True)\n    return (DataLoader(train_ds, shuffle=True, **dl_kwargs),\n            DataLoader(val_ds, shuffle=False, **dl_kwargs),\n            DataLoader(test_ds, shuffle=False, **dl_kwargs),\n            tokenizer)\n\n\n# ---------------------------------------------------------------------------\n# Public factory\n# ---------------------------------------------------------------------------\n\ndef build_dataloaders(cfg, trial_mode: bool = False):\n    name = cfg.dataset.name.lower()\n    if name == \"cifar10\":\n        return _build_cifar10(cfg, trial_mode)\n    elif name == \"alpaca-cleaned\":\n        return _build_alpaca(cfg, trial_mode)\n    else:\n        raise ValueError(f\"Unsupported dataset {name}\")\n",
    "model_py": "import torch\nfrom torch import nn\nfrom transformers import DistilBertModel\n\n\nclass HoulsbyAdapter(nn.Module):\n    def __init__(self, hidden_dim: int, reduction_factor: int = 16, non_linearity: str = \"relu\"):\n        super().__init__()\n        bottleneck = max(1, hidden_dim // reduction_factor)\n        self.down = nn.Linear(hidden_dim, bottleneck)\n        self.nonlin = getattr(nn, non_linearity.capitalize())() if hasattr(nn, non_linearity.capitalize()) else nn.ReLU()\n        self.up = nn.Linear(bottleneck, hidden_dim)\n\n    def forward(self, x):\n        return self.up(self.nonlin(self.down(x)))\n\n\nclass VisionPatchEmbedding(nn.Module):\n    def __init__(self, in_channels: int, patch_size: int, embed_dim: int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Linear(in_channels * patch_size * patch_size, embed_dim)\n\n    def forward(self, imgs):  # imgs: B,C,H,W\n        B, C, H, W = imgs.shape\n        p = self.patch_size\n        assert H % p == 0 and W % p == 0, \"Image dims must be divisible by patch size\"\n        patches = imgs.unfold(2, p, p).unfold(3, p, p)  # B,C,nH,nW,p,p\n        patches = patches.contiguous().view(B, C, -1, p, p).permute(0, 2, 1, 3, 4).contiguous()\n        patches = patches.view(B, patches.size(1), -1)  # B,N,C*p*p\n        return self.proj(patches)  # B,N,D\n\n\nclass DistilBertClassifier(nn.Module):\n    def __init__(self, model_name: str, num_labels: int, adapter_cfg=None, vision_patch_cfg=None):\n        super().__init__()\n        self.is_vision = vision_patch_cfg is not None\n        self.bert = DistilBertModel.from_pretrained(model_name, cache_dir=\".cache/\")\n        hidden = self.bert.config.hidden_size\n\n        if self.is_vision:\n            self.vision = VisionPatchEmbedding(3, vision_patch_cfg.patch_size, hidden)\n        else:\n            self.vision = None\n\n        if adapter_cfg and adapter_cfg.get(\"enabled\", False):\n            self.adapter = HoulsbyAdapter(hidden, adapter_cfg.reduction_factor, adapter_cfg.non_linearity)\n        else:\n            self.adapter = None\n\n        self.pre_cls = nn.Linear(hidden, hidden)\n        self.act = nn.GELU()\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(hidden, num_labels)\n\n    def forward(self, batch):\n        if self.is_vision and \"pixel_values\" in batch:\n            embeds = self.vision(batch[\"pixel_values\"])\n            mask = torch.ones(embeds.size()[:-1], dtype=torch.long, device=embeds.device)\n            out = self.bert(inputs_embeds=embeds, attention_mask=mask)\n        else:\n            out = self.bert(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n        pooled = out.last_hidden_state[:, 0]\n        if self.adapter is not None:\n            pooled = pooled + self.adapter(pooled)\n        x = self.pre_cls(pooled)\n        x = self.act(x)\n        x = self.dropout(x)\n        return self.cls(x)\n\n\ndef build_model(cfg, tokenizer=None):\n    vision_patch_cfg = cfg.model.vision_patch if cfg.model.get(\"input_representation\", \"text\") == \"image_sequence\" else None\n    adapter_cfg = cfg.model.task_adapters if cfg.method == \"proposed\" else None\n    return DistilBertClassifier(cfg.model.name, cfg.model.num_labels, adapter_cfg, vision_patch_cfg)\n",
    "main_py": "\"\"\"Orchestrator: receives run id & launches src.train as subprocess.\"\"\"\n\nimport subprocess\nimport sys\nfrom typing import List\nfrom pathlib import Path\n\nimport hydra\nfrom hydra.utils import get_original_cli_args\n\n\n@hydra.main(config_path=\"../config\")\ndef main(cfg):\n    # Build base command\n    original_cli: List[str] = get_original_cli_args()\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + original_cli\n\n    # Ensure wandb.mode is disabled when trial_mode=true -------------------\n    if cfg.get(\"trial_mode\", False):\n        cmd.append(\"wandb.mode=disabled\")\n\n    print(\"Executing subprocess:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"distilbert-adapter-exp\"\nversion = \"0.1.0\"\ndescription = \"DistilBERT adapter experiments with Hydra & WandB\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntransformers = \">=4.35.0\"\ndatasets = \">=2.14.0\"\ntorchvision = \">=0.15.0\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.15.0\"\nscikit-learn = \"^1.3.0\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\noptuna = \"^3.3.0\"\npandas = \"^2.1.0\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
    "config_yaml": "defaults:\n  - run: proposed-DistilBERT-base-66M-CIFAR-10  # default example, override via CLI\n  - _self_\n\nwandb:\n  entity: gengaru617-personal\n  project: 251017-test\n  mode: online  # will be overwritten to \"disabled\" automatically when trial_mode=true\n\ntrial_mode: false  # set via CLI for lightweight validation\n\nresults_dir: outputs/\n\nhydra:\n  run:\n    dir: .\n  output_subdir: null\n"
}
