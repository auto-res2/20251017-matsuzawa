/home/toma/t-80-8-a/_work/20251017-matsuzawa/20251017-matsuzawa/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/20251017-matsuzawa/20251017-matsuzawa/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
[I 2025-10-17 21:29:58,914] A new study created in RDB with name: proposed-DistilBERT-base-66M-CIFAR-10
[I 2025-10-17 21:38:16,644] Trial 0 finished with value: 0.489 and parameters: {'learning_rate': 1.6892377779969352e-05, 'batch_size': 8, 'adapter_reduction_factor': 20, 'weight_decay': 0.0008410245656618409}. Best is trial 0 with value: 0.489.
[I 2025-10-17 21:41:55,882] Trial 1 finished with value: 0.1018 and parameters: {'learning_rate': 0.0004943122239334548, 'batch_size': 32, 'adapter_reduction_factor': 32, 'weight_decay': 0.08345664509727209}. Best is trial 0 with value: 0.489.
[I 2025-10-17 21:45:40,896] Trial 2 finished with value: 0.1638 and parameters: {'learning_rate': 0.0004773076626742261, 'batch_size': 32, 'adapter_reduction_factor': 4, 'weight_decay': 0.06020979372263966}. Best is trial 0 with value: 0.489.
[I 2025-10-17 21:50:13,653] Trial 3 finished with value: 0.502 and parameters: {'learning_rate': 7.368976658880627e-05, 'batch_size': 16, 'adapter_reduction_factor': 32, 'weight_decay': 0.0005473798443305089}. Best is trial 3 with value: 0.502.
[I 2025-10-17 21:54:44,492] Trial 4 finished with value: 0.4588 and parameters: {'learning_rate': 1.0832827017732367e-05, 'batch_size': 16, 'adapter_reduction_factor': 4, 'weight_decay': 0.03389760419761737}. Best is trial 3 with value: 0.502.
[I 2025-10-17 22:01:21,246] Trial 5 finished with value: 0.1066 and parameters: {'learning_rate': 0.00023024880952913213, 'batch_size': 8, 'adapter_reduction_factor': 4, 'weight_decay': 0.08578193121292717}. Best is trial 3 with value: 0.502.
[I 2025-10-17 22:07:15,377] Trial 6 finished with value: 0.1018 and parameters: {'learning_rate': 0.000423110950044841, 'batch_size': 8, 'adapter_reduction_factor': 28, 'weight_decay': 0.09105973085722649}. Best is trial 3 with value: 0.502.
[I 2025-10-17 22:11:42,336] Trial 7 finished with value: 0.4934 and parameters: {'learning_rate': 4.2999750115254456e-05, 'batch_size': 16, 'adapter_reduction_factor': 12, 'weight_decay': 0.08690591701969239}. Best is trial 3 with value: 0.502.
[I 2025-10-17 22:17:46,353] Trial 8 finished with value: 0.504 and parameters: {'learning_rate': 2.310771368798121e-05, 'batch_size': 8, 'adapter_reduction_factor': 16, 'weight_decay': 0.06977936637427688}. Best is trial 8 with value: 0.504.
[I 2025-10-17 22:24:30,124] Trial 9 finished with value: 0.1018 and parameters: {'learning_rate': 0.0003783360865262738, 'batch_size': 8, 'adapter_reduction_factor': 28, 'weight_decay': 0.05095651316848048}. Best is trial 8 with value: 0.504.
[I 2025-10-17 22:31:57,644] Trial 10 finished with value: 0.498 and parameters: {'learning_rate': 3.3929684735800246e-05, 'batch_size': 8, 'adapter_reduction_factor': 16, 'weight_decay': 0.030683639427538455}. Best is trial 8 with value: 0.504.
[I 2025-10-17 22:36:29,256] Trial 11 finished with value: 0.211 and parameters: {'learning_rate': 0.0001140647961977405, 'batch_size': 16, 'adapter_reduction_factor': 20, 'weight_decay': 0.00014951350931768914}. Best is trial 8 with value: 0.504.
[I 2025-10-17 22:41:05,950] Trial 12 finished with value: 0.454 and parameters: {'learning_rate': 8.03138947176019e-05, 'batch_size': 16, 'adapter_reduction_factor': 12, 'weight_decay': 0.06769643664626693}. Best is trial 8 with value: 0.504.
[I 2025-10-17 22:45:36,996] Trial 13 finished with value: 0.4952 and parameters: {'learning_rate': 2.878051652321062e-05, 'batch_size': 16, 'adapter_reduction_factor': 24, 'weight_decay': 0.022410168124198757}. Best is trial 8 with value: 0.504.
[I 2025-10-17 22:49:21,145] Trial 14 finished with value: 0.4684 and parameters: {'learning_rate': 0.00012172086374023964, 'batch_size': 32, 'adapter_reduction_factor': 12, 'weight_decay': 0.06759132641942897}. Best is trial 8 with value: 0.504.
[I 2025-10-17 22:56:41,420] Trial 15 finished with value: 0.3888 and parameters: {'learning_rate': 6.45801178459714e-05, 'batch_size': 8, 'adapter_reduction_factor': 32, 'weight_decay': 0.017831642847821454}. Best is trial 8 with value: 0.504.
[I 2025-10-17 23:01:07,280] Trial 16 finished with value: 0.5108 and parameters: {'learning_rate': 2.143588987275279e-05, 'batch_size': 16, 'adapter_reduction_factor': 16, 'weight_decay': 0.04156041530944485}. Best is trial 16 with value: 0.5108.
[I 2025-10-17 23:05:38,294] Trial 17 finished with value: 0.4702 and parameters: {'learning_rate': 2.0384179395997975e-05, 'batch_size': 16, 'adapter_reduction_factor': 16, 'weight_decay': 0.044838428953092875}. Best is trial 16 with value: 0.5108.
[I 2025-10-17 23:12:44,347] Trial 18 finished with value: 0.4856 and parameters: {'learning_rate': 1.0136740256515751e-05, 'batch_size': 8, 'adapter_reduction_factor': 8, 'weight_decay': 0.09925029808431851}. Best is trial 16 with value: 0.5108.
[I 2025-10-17 23:16:15,209] Trial 19 finished with value: 0.4136 and parameters: {'learning_rate': 1.782874782072455e-05, 'batch_size': 32, 'adapter_reduction_factor': 24, 'weight_decay': 0.07460490833018427}. Best is trial 16 with value: 0.5108.
[I 2025-10-17 23:23:45,302] Trial 20 finished with value: 0.4646 and parameters: {'learning_rate': 4.7024897196098925e-05, 'batch_size': 8, 'adapter_reduction_factor': 16, 'weight_decay': 0.04861227826413294}. Best is trial 16 with value: 0.5108.
[I 2025-10-17 23:28:24,058] Trial 21 finished with value: 0.4888 and parameters: {'learning_rate': 2.7251619484327714e-05, 'batch_size': 16, 'adapter_reduction_factor': 20, 'weight_decay': 0.0116759913929337}. Best is trial 16 with value: 0.5108.
[I 2025-10-17 23:33:15,869] Trial 22 finished with value: 0.517 and parameters: {'learning_rate': 6.583522709794831e-05, 'batch_size': 16, 'adapter_reduction_factor': 24, 'weight_decay': 0.03991635920796699}. Best is trial 22 with value: 0.517.
[I 2025-10-17 23:38:00,571] Trial 23 finished with value: 0.4436 and parameters: {'learning_rate': 1.4628129980313755e-05, 'batch_size': 16, 'adapter_reduction_factor': 24, 'weight_decay': 0.03786795522950642}. Best is trial 22 with value: 0.517.
[I 2025-10-17 23:42:43,073] Trial 24 finished with value: 0.468 and parameters: {'learning_rate': 2.427727941428894e-05, 'batch_size': 16, 'adapter_reduction_factor': 16, 'weight_decay': 0.05803265253806514}. Best is trial 22 with value: 0.517.
wandb: Currently logged in as: gengaru617 (gengaru617-personal) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run proposed-DistilBERT-base-66M-CIFAR-10
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /home/toma/t-80-8-a/_work/20251017-matsuzawa/20251017-matsuzawa/wandb/run-20251017_234246-proposed-DistilBERT-base-66M-CIFAR-10
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proposed-DistilBERT-base-66M-CIFAR-10
wandb: ⭐️ View project at https://wandb.ai/gengaru617-personal/251017-test
wandb: 🚀 View run at https://wandb.ai/gengaru617-personal/251017-test/runs/proposed-DistilBERT-base-66M-CIFAR-10
wandb: WARNING The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:   best_val_acc ▁▂▃▄▅▅▆▆▇▇▇▇▇███████
wandb:    best_val_f1 ▁▃▃▅▆▆▆▆▇▇▇▇▇███████
wandb:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb: inference_time ▃▅▃▁▄▅▁▄▁▆▆▅▆▄▄█▂▅▅▃
wandb:      train_acc ▁▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇████
wandb:       train_f1 ▁▂▃▄▅▅▆▆▆▆▇▇▇▇▇█████
wandb:     train_loss █▇▆▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁
wandb:        val_acc ▁▂▃▄▅▅▆▆▇▇▇▇▇███████
wandb:         val_f1 ▁▃▃▅▆▅▆▆▇▇▇▇▇███████
wandb:       val_loss █▇▆▅▄▄▄▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:   best_val_acc 0.7096
wandb:    best_val_f1 0.70889
wandb:          epoch 20
wandb: inference_time 0.01561
wandb:      train_acc 0.7336
wandb:       train_f1 0.7332
wandb:     train_loss 0.74999
wandb:        val_acc 0.7096
wandb:         val_f1 0.70889
wandb:       val_loss 0.84311
wandb: 
wandb: 🚀 View run proposed-DistilBERT-base-66M-CIFAR-10 at: https://wandb.ai/gengaru617-personal/251017-test/runs/proposed-DistilBERT-base-66M-CIFAR-10
wandb: ⭐️ View project at: https://wandb.ai/gengaru617-personal/251017-test
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251017_234246-proposed-DistilBERT-base-66M-CIFAR-10/logs
